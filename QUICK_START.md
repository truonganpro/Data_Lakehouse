# ğŸš€ QUICK START GUIDE - Data Lakehouse Fresh

HÆ°á»›ng dáº«n triá»ƒn khai hoÃ n chá»‰nh tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i trÃªn mÃ¡y má»›i.

---

## âœ… YÃŠU Cáº¦U Há»† THá»NG

- **Docker** & **Docker Compose** Ä‘Ã£ cÃ i Ä‘áº·t
- **8GB RAM** trá»Ÿ lÃªn
- **20GB** dung lÆ°á»£ng trá»‘ng
- **Internet** Ä‘á»ƒ download JARs vÃ  Docker images

---

## ğŸ“‹ BÆ¯á»šC 1: CLONE REPOSITORY

```bash
# Clone project tá»« GitHub
git clone https://github.com/truonganpro/Data_Warehouse.git

# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd Data_Warehouse

# Kiá»ƒm tra cáº¥u trÃºc project
ls -la
```

---

## ğŸ“‹ BÆ¯á»šC 2: DOWNLOAD JAR DEPENDENCIES

```bash
# Cáº¥p quyá»n thá»±c thi cho script
chmod +x download_jars.sh

# Download cÃ¡c JAR files cáº§n thiáº¿t
./download_jars.sh
```

**Thá»i gian:** 1-2 phÃºt

---

## ğŸ“‹ BÆ¯á»šC 3: Táº O FILE .ENV

```bash
# Táº¡o file .env tá»« template
cp env.example .env

# Chá»‰nh sá»­a .env Ä‘á»ƒ thÃªm Google API Key (náº¿u cáº§n Chat Service)
nano .env  # hoáº·c vi, vim, code, etc.

# TÃ¬m dÃ²ng GOOGLE_API_KEY vÃ  thay YOUR_GOOGLE_API_KEY_HERE báº±ng API key tháº­t
```

**LÆ°u Ã½:** Náº¿u chÆ°a cÃ³ Google API Key, báº¡n cÃ³ thá»ƒ bá» qua bÆ°á»›c nÃ y vÃ  thÃªm sau.

---

## ğŸ“‹ BÆ¯á»šC 4: KHá»I Äá»˜NG SERVICES

### Option A: Automated Setup (Recommended)

```bash
# Cáº¥p quyá»n thá»±c thi
chmod +x setup.sh

# Cháº¡y script tá»± Ä‘á»™ng setup toÃ n bá»™
./setup.sh
```

**Thá»i gian:** 15-20 phÃºt (láº§n Ä‘áº§u tiÃªn)

Script sáº½ tá»± Ä‘á»™ng:
1. âœ… Kiá»ƒm tra Docker, disk space, RAM
2. âœ… Táº¡o .env náº¿u chÆ°a cÃ³
3. âœ… Download JARs
4. âœ… Build Docker images
5. âœ… Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
6. âœ… Cháº¡y ETL pipeline (náº¿u cÃ³ data)
7. âœ… Kiá»ƒm tra health

### Option B: Manual Setup

```bash
# Build Docker images
docker-compose build

# Khá»Ÿi Ä‘á»™ng services
docker-compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps
```

---

## ğŸ“‹ BÆ¯á»šC 5: Táº¢I DATASET (Brazilian E-commerce)

**Náº¿u chÆ°a cÃ³ dataset:**

```bash
# Táº¡o thÆ° má»¥c cho dataset
mkdir -p brazilian-ecommerce

# Báº¡n cáº§n download dataset tá»« Kaggle:
# https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
# 
# Giáº£i nÃ©n vÃ  Ä‘áº·t cÃ¡c file CSV vÃ o thÆ° má»¥c brazilian-ecommerce/

# Sau Ä‘Ã³ cháº¡y full_setup.sh Ä‘á»ƒ load data
chmod +x full_setup.sh
./full_setup.sh --fresh
```

**Náº¿u Ä‘Ã£ cÃ³ dataset trong thÆ° má»¥c brazilian-ecommerce:**

```bash
# Load data vÃ o MySQL vÃ  cháº¡y ETL
chmod +x full_setup.sh
./full_setup.sh --fresh
```

---

## ğŸ“‹ BÆ¯á»šC 6: KIá»‚M TRA SERVICES

```bash
# Kiá»ƒm tra táº¥t cáº£ services
docker-compose ps

# Xem logs cá»§a má»™t service cá»¥ thá»ƒ
docker-compose logs -f streamlit
docker-compose logs -f chat_service
docker-compose logs -f etl_pipeline

# Kiá»ƒm tra health
curl http://localhost:8501/_stcore/health  # Streamlit
curl http://localhost:8001/health          # Chat Service
curl http://localhost:3000                 # Metabase
curl http://localhost:3001                 # Dagster
```

---

## ğŸ“‹ BÆ¯á»šC 7: TRUY Cáº¬P WEB INTERFACES

Sau khi táº¥t cáº£ services Ä‘Ã£ cháº¡y (2-3 phÃºt), má»Ÿ trÃ¬nh duyá»‡t:

| Service | URL | Ghi chÃº |
|---------|-----|---------|
| **ğŸš€ Streamlit App** | http://localhost:8501 | Main dashboard |
| **ğŸ’¬ Chat Service** | http://localhost:8001 | API endpoint |
| **ğŸ“Š Metabase** | http://localhost:3000 | BI Dashboard (cáº§n setup) |
| **ğŸ¯ Dagster** | http://localhost:3001 | ETL Orchestration |
| **âš¡ Spark Master** | http://localhost:8080 | Spark UI |
| **ğŸª£ MinIO Console** | http://localhost:9001 | Object Storage |
| **ğŸ” Trino** | http://localhost:8082 | SQL Query Engine |

---

## ğŸ“‹ BÆ¯á»šC 8: SETUP METABASE (Láº§n Ä‘áº§u)

1. Má»Ÿ http://localhost:3000
2. Táº¡o admin account (email/password)
3. Add Database:
   - **Type:** Presto
   - **Host:** trino
   - **Port:** 8080
   - **Database:** lakehouse
4. Select schemas: `bronze`, `silver`, `gold`, `platinum`
5. Test connection â†’ Save

---

## ğŸ“‹ BÆ¯á»šC 9: CHáº Y FORECASTING PIPELINE (Optional)

```bash
# Cháº¡y toÃ n bá»™ forecasting pipeline
chmod +x run_forecast_pipeline.sh
./run_forecast_pipeline.sh

# Hoáº·c cháº¡y tá»«ng bÆ°á»›c vá»›i Makefile
make forecast-init      # Initialize MLflow
make forecast-features  # Build features
make forecast-train     # Train model
make forecast-predict RUN_ID=<run_id>  # Generate forecasts

# Kiá»ƒm tra káº¿t quáº£ trong Trino
docker exec trino trino --execute "SELECT * FROM lakehouse.platinum.demand_forecast LIMIT 10;"
```

---

## ğŸ“‹ CÃC Lá»†NH QUAN TRá»ŒNG

### Khá»Ÿi Ä‘á»™ng/Dá»«ng services

```bash
# Start all services
docker-compose up -d

# Stop all services
docker-compose down

# Stop vÃ  xÃ³a volumes (cáº£nh bÃ¡o: máº¥t data)
docker-compose down -v

# Restart má»™t service
docker-compose restart streamlit
```

### Xem logs

```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f streamlit
docker-compose logs -f etl_pipeline
docker-compose logs -f chat_service

# Last 100 lines
docker-compose logs --tail=100 streamlit
```

### Rebuild sau khi sá»­a code

```bash
# Rebuild specific service
docker-compose build streamlit
docker-compose up -d streamlit

# Rebuild all
docker-compose build
docker-compose up -d
```

### Cháº¡y ETL pipeline

```bash
# Cháº¡y ETL job
docker exec etl_pipeline dagster job execute -m etl_pipeline -j reload_data

# Hoáº·c dÃ¹ng Makefile
make etl_bronze
```

### Kiá»ƒm tra data

```bash
# Káº¿t ná»‘i Trino
docker exec -it trino trino

# Trong Trino CLI:
SHOW CATALOGS;
USE lakehouse.gold;
SHOW TABLES;
SELECT COUNT(*) FROM fact_order;

# Kiá»ƒm tra MinIO
docker exec mc mc ls minio/lakehouse/
```

---

## ğŸ†˜ TROUBLESHOOTING

### Services khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra Docker daemon
docker ps

# Kiá»ƒm tra disk space
df -h

# Xem logs chi tiáº¿t
docker-compose logs -f

# Clean vÃ  restart
docker-compose down -v
docker-compose up -d
```

### Port bá»‹ conflict

```bash
# Kiá»ƒm tra port Ä‘ang dÃ¹ng
lsof -i :8501
lsof -i :3000

# Dá»«ng process chiáº¿m port hoáº·c Ä‘á»•i port trong docker-compose.yaml
```

### Out of memory

```bash
# Kiá»ƒm tra RAM
free -h

# Reduce Spark memory trong docker-compose.yaml
# Giáº£m SPARK_WORKER_MEMORY tá»« 8G xuá»‘ng 4G
```

### Dataset load failed

```bash
# Kiá»ƒm tra MySQL logs
docker-compose logs de_mysql

# Manual load
docker exec de_mysql mysql -uroot -padmin123 -e "SHOW DATABASES;"
docker cp brazilian-ecommerce/ de_mysql:/tmp/dataset/
```

---

## âœ… KIá»‚M TRA HOÃ€N Táº¤T

Sau khi setup xong, kiá»ƒm tra:

1. âœ… Táº¥t cáº£ services running: `docker-compose ps`
2. âœ… Streamlit accessible: http://localhost:8501
3. âœ… Chat Service healthy: `curl http://localhost:8001/health`
4. âœ… Data loaded: `SELECT COUNT(*) FROM lakehouse.gold.fact_order;`
5. âœ… Forecast working: `SELECT * FROM lakehouse.platinum.demand_forecast LIMIT 10;`

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- `README.md` - Tá»•ng quan project
- `PROJECT_OVERVIEW.md` - Chi tiáº¿t kiáº¿n trÃºc
- `CHAT_SERVICE_FILES.txt` - Chat service docs
- `FORECAST_FILES.txt` - Forecast pipeline docs
- `QUERY_WINDOW_FILES.txt` - Query Window docs

---

## ğŸ‰ HOÃ€N Táº¤T!

Náº¿u táº¥t cáº£ bÆ°á»›c trÃªn Ä‘á»u OK, project cá»§a báº¡n Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng!

**Happy Data Engineering! ğŸš€**

