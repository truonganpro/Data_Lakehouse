# ğŸ—ï¸ Modern Data Stack - Data Lakehouse Project

> **á»¨ng dá»¥ng Modern Data Stack (MDS) Ä‘á»ƒ xÃ¢y dá»±ng Data Lakehouse há»— trá»£ phÃ¢n tÃ­ch dá»¯ liá»‡u bÃ¡n hÃ ng thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­**

[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Apache Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)](https://spark.apache.org/)
[![Delta Lake](https://img.shields.io/badge/Delta_Lake-00ADD8?style=for-the-badge&logo=delta&logoColor=white)](https://delta.io/)
[![Trino](https://img.shields.io/badge/Trino-FF6900?style=for-the-badge&logo=trino&logoColor=white)](https://trino.io/)
[![Dagster](https://img.shields.io/badge/Dagster-000000?style=for-the-badge&logo=dagster&logoColor=white)](https://dagster.io/)
[![Metabase](https://img.shields.io/badge/Metabase-509EE3?style=for-the-badge&logo=metabase&logoColor=white)](https://www.metabase.com/)

## ğŸ“‹ Má»¥c lá»¥c

- [ğŸ¯ Tá»•ng quan dá»± Ã¡n](#-tá»•ng-quan-dá»±-Ã¡n)
- [ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng](#ï¸-kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng](#ï¸-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [ğŸš€ CÃ i Ä‘áº·t vÃ  cháº¡y](#-cÃ i-Ä‘áº·t-vÃ -cháº¡y)
- [ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u](#-cáº¥u-trÃºc-dá»¯-liá»‡u)
- [ğŸ”„ Quy trÃ¬nh ETL](#-quy-trÃ¬nh-etl)
- [ğŸ“ˆ Dashboard vÃ  BI](#-dashboard-vÃ -bi)
- [ğŸ”§ Cáº¥u hÃ¬nh](#-cáº¥u-hÃ¬nh)
- [ğŸ“š TÃ i liá»‡u tham kháº£o](#-tÃ i-liá»‡u-tham-kháº£o)

## ğŸ¯ Tá»•ng quan dá»± Ã¡n

### Má»¥c tiÃªu
XÃ¢y dá»±ng má»™t há»‡ thá»‘ng Data Lakehouse hoÃ n chá»‰nh sá»­ dá»¥ng Modern Data Stack Ä‘á»ƒ:
- **Thu tháº­p vÃ  xá»­ lÃ½** dá»¯ liá»‡u thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Brazilian E-commerce
- **Tá»• chá»©c dá»¯ liá»‡u** theo mÃ´ hÃ¬nh medallion (Bronze â†’ Silver â†’ Gold â†’ Platinum)
- **PhÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a** dá»¯ liá»‡u qua BI dashboard
- **Triá»ƒn khai container hÃ³a** vá»›i Docker Compose

### Dataset
- **Brazilian E-commerce Dataset**: Dá»¯ liá»‡u bÃ¡n hÃ ng thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Brazil
- **Nguá»“n**: [Kaggle - Brazilian E-commerce Public Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- **KÃ­ch thÆ°á»›c**: ~100K Ä‘Æ¡n hÃ ng, 32K sáº£n pháº©m, 9K ngÆ°á»i bÃ¡n

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```mermaid
graph TB
    subgraph "ğŸ“Š Data Sources"
        A[Brazilian E-commerce CSV]
        B[MySQL Database]
    end
    
    subgraph "ğŸ”„ ETL Layer"
        C[Dagster Orchestration]
        D[Apache Spark]
        E[Delta Lake]
    end
    
    subgraph "ğŸ  Data Lakehouse"
        F[MinIO S3 Storage]
        G[Bronze Layer - Raw Data]
        H[Silver Layer - Cleaned Data]
        I[Gold Layer - Business Logic]
        J[Platinum Layer - Analytics]
    end
    
    subgraph "ğŸ—„ï¸ Metadata Management"
        K[Hive Metastore]
        L[MySQL Backend]
    end
    
    subgraph "ğŸ” Query Engine"
        M[Trino SQL Engine]
    end
    
    subgraph "ğŸ“ˆ Visualization"
        N[Metabase BI]
        O[Streamlit Dashboard]
    end
    
    A --> C
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    K --> L
    F --> K
    K --> M
    M --> N
    M --> O
```

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

| **ThÃ nh pháº§n** | **CÃ´ng nghá»‡** | **Vai trÃ²** |
|----------------|---------------|-------------|
| **Storage** | MinIO (S3-compatible) | LÆ°u trá»¯ dá»¯ liá»‡u theo zone |
| **Metadata** | Hive Metastore + MySQL | Quáº£n lÃ½ schema vÃ  metadata |
| **Compute** | Apache Spark 3.3.2 | Xá»­ lÃ½ dá»¯ liá»‡u (ETL) |
| **Lakehouse** | Delta Lake 2.3.0 | ACID transactions, versioning |
| **Query Engine** | Trino 414 | SQL query trÃªn Delta tables |
| **Orchestration** | Dagster | Workflow management |
| **BI/Visualization** | Metabase + Streamlit | Dashboard vÃ  bÃ¡o cÃ¡o |
| **Containerization** | Docker Compose | Triá»ƒn khai vÃ  quáº£n lÃ½ |

## ğŸš€ CÃ i Ä‘áº·t vÃ  cháº¡y

### YÃªu cáº§u há»‡ thá»‘ng
- Docker & Docker Compose
- 8GB RAM trá»Ÿ lÃªn
- 20GB dung lÆ°á»£ng trá»‘ng

### 1. Clone repository
```bash
git clone <repository-url>
cd DATALAKEHOUSE
```

### 2. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng
```bash
# Táº¡o file .env
cp .env.example .env

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Hoáº·c cháº¡y script tá»± Ä‘á»™ng
chmod +x start_and_test.sh
./start_and_test.sh
```

### 3. Kiá»ƒm tra tráº¡ng thÃ¡i
```bash
# Kiá»ƒm tra containers
docker-compose ps

# Xem logs
docker-compose logs -f [service_name]
```

### 4. Truy cáº­p cÃ¡c giao diá»‡n
- **Spark Master UI**: http://localhost:8080
- **MinIO Console**: http://localhost:9001 (minio/minio123)
- **Metabase**: http://localhost:3000
- **Trino**: http://localhost:8082
- **Dagster**: http://localhost:3001

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

### Data Lakehouse Layers

```
/lakehouse/
â”œâ”€â”€ bronze/          # Raw data (CSV â†’ Parquet)
â”‚   â”œâ”€â”€ customers/
â”‚   â”œâ”€â”€ orders/
â”‚   â”œâ”€â”€ order_items/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ silver/          # Cleaned & normalized
â”‚   â”œâ”€â”€ cleaned_customers/
â”‚   â”œâ”€â”€ cleaned_orders/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ gold/            # Business logic & facts
â”‚   â”œâ”€â”€ fact_orders/
â”‚   â”œâ”€â”€ dim_customers/
â”‚   â””â”€â”€ ...
â””â”€â”€ platinum/        # Analytics & cubes
    â”œâ”€â”€ sales_summary/
    â”œâ”€â”€ customer_segments/
    â””â”€â”€ ...
```

### Schema Design

#### Fact Tables (Gold Layer)
- **fact_orders**: ÄÆ¡n hÃ ng chÃ­nh vá»›i metrics
- **fact_order_items**: Chi tiáº¿t sáº£n pháº©m trong Ä‘Æ¡n hÃ ng
- **fact_payments**: ThÃ´ng tin thanh toÃ¡n

#### Dimension Tables (Gold Layer)
- **dim_customers**: ThÃ´ng tin khÃ¡ch hÃ ng
- **dim_products**: ThÃ´ng tin sáº£n pháº©m
- **dim_sellers**: ThÃ´ng tin ngÆ°á»i bÃ¡n
- **dim_geolocation**: ThÃ´ng tin Ä‘á»‹a lÃ½
- **dim_dates**: Báº£ng thá»i gian

## ğŸ”„ Quy trÃ¬nh ETL

### 1. Extract (Dagster Assets)
```python
@asset
def extract_customers():
    """Extract customer data from CSV"""
    return spark.read.csv("s3a://lakehouse/bronze/customers/")

@asset
def extract_orders():
    """Extract order data from CSV"""
    return spark.read.csv("s3a://lakehouse/bronze/orders/")
```

### 2. Transform (Silver Layer)
```python
@asset
def clean_customers(extract_customers):
    """Clean and normalize customer data"""
    return extract_customers \
        .filter(col("customer_id").isNotNull()) \
        .withColumn("customer_zip_code_prefix", 
                   col("customer_zip_code_prefix").cast("int"))
```

### 3. Load (Gold Layer)
```python
@asset
def create_fact_orders(clean_orders, clean_order_items):
    """Create fact table for orders"""
    return clean_orders \
        .join(clean_order_items, "order_id") \
        .groupBy("order_id") \
        .agg(
            sum("price").alias("total_price"),
            count("order_item_id").alias("item_count")
        )
```

## ğŸ“ˆ Dashboard vÃ  BI

### Metabase Dashboards
1. **Sales Overview**: Tá»•ng quan doanh thu
2. **Customer Analysis**: PhÃ¢n tÃ­ch khÃ¡ch hÃ ng
3. **Product Performance**: Hiá»‡u suáº¥t sáº£n pháº©m
4. **Geographic Analysis**: PhÃ¢n tÃ­ch theo Ä‘á»‹a lÃ½

### Streamlit Applications
- **Real-time Analytics**: PhÃ¢n tÃ­ch thá»i gian thá»±c
- **Customer Segmentation**: PhÃ¢n khÃºc khÃ¡ch hÃ ng
- **Sales Forecasting**: Dá»± bÃ¡o doanh thu

## ğŸ”§ Cáº¥u hÃ¬nh

### Environment Variables (.env)
```bash
# MySQL
MYSQL_ROOT_PASSWORD=root123
MYSQL_DATABASE=metastore
MYSQL_USER=hive
MYSQL_PASSWORD=hive

# PostgreSQL (Dagster)
POSTGRES_DB=postgres
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin123

# MinIO
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=minio123
```

### Spark Configuration
```properties
# S3/MinIO
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123

# Delta Lake
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
```

### Trino Configuration
```properties
# Hive connector
hive.metastore.uri=thrift://hive-metastore:9083
hive.s3.endpoint=http://minio:9000
hive.s3.aws-access-key=minio
hive.s3.aws-secret-key=minio123
```

## ğŸ§ª Testing

### Test káº¿t ná»‘i Spark â†” MinIO
```bash
# Cháº¡y test script
python test_spark_minio_connection.py

# Hoáº·c qua Docker
docker exec spark-master python3 /opt/bitnami/spark/test_spark_minio_connection.py
```

### Test SQL queries
```sql
-- Trino
SELECT * FROM hive.default.customers LIMIT 10;

-- Spark SQL
SELECT * FROM delta.`s3a://lakehouse/gold/fact_orders` LIMIT 10;
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

### CÃ´ng nghá»‡ chÃ­nh
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Trino Documentation](https://trino.io/docs/)
- [Dagster Documentation](https://docs.dagster.io/)
- [Metabase Documentation](https://www.metabase.com/docs/)

### Modern Data Stack
- [What is a Data Lakehouse?](https://www.databricks.com/discover/data-lakehouse)
- [Modern Data Stack Architecture](https://www.getdbt.com/what-is-analytics-engineering/)
- [Data Engineering Best Practices](https://github.com/datastacktv/data-engineer-roadmap)

### Dataset
- [Brazilian E-commerce Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Táº¡o Pull Request

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

## ğŸ‘¨â€ğŸ’» TÃ¡c giáº£

**Truong An** - *Modern Data Stack Engineer*


## ğŸ™ Acknowledgments

- [Olist](https://olist.com/) for providing the Brazilian E-commerce dataset
- [Apache Software Foundation](https://apache.org/) for open-source tools
- [Modern Data Stack community](https://github.com/modern-data-stack) for inspiration

---

â­ **Náº¿u dá»± Ã¡n nÃ y há»¯u Ã­ch, hÃ£y cho má»™t star!** â­
