# Data Lakehouse - Modern Data Stack

Há»‡ thá»‘ng Data Lakehouse hoÃ n chá»‰nh vá»›i ETL Pipeline, OLAP Query Engine, AI Chatbot, vÃ  BI Dashboards.

## ğŸ—ï¸ Kiáº¿n trÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              USER INTERFACES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Streamlitâ”‚ Metabase â”‚  Dagster â”‚   Chat Service        â”‚
â”‚  :8501   â”‚  :3000   â”‚  :3001   â”‚      :8001            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“           â†“           â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QUERY & PROCESSING LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Trino  â”‚  Spark   â”‚  MLflow  â”‚   Chat Service        â”‚
â”‚  :8082   â”‚  :8080   â”‚  :5000   â”‚      :8001            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“           â†“           â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STORAGE & METADATA LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚Delta Lakeâ”‚   MinIO   â”‚   MySQL  â”‚      Qdrant          â”‚
â”‚(Lakehouse)â”‚ (S3)    â”‚(Metadata)â”‚   (Vector DB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- **Docker**: 20.10+ vÃ  Docker Compose 2.0+
- **Disk Space**: Tá»‘i thiá»ƒu 20GB trá»‘ng
- **RAM**: Tá»‘i thiá»ƒu 8GB (khuyáº¿n nghá»‹ 16GB)
- **OS**: Linux, macOS, hoáº·c Windows vá»›i WSL2

## ğŸš€ Quick Start

### 1. Clone repository

```bash
git clone <repository-url>
cd Data_lakehouse
```

### 2. Download dataset (náº¿u chÆ°a cÃ³)

Dataset Brazilian E-commerce tá»« Kaggle:
- URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
- Äáº·t vÃ o thÆ° má»¥c `brazilian-ecommerce/`

### 3. Cháº¡y setup tá»± Ä‘á»™ng

```bash
# Quick setup (khuyáº¿n nghá»‹ cho láº§n Ä‘áº§u)
./setup.sh

# Hoáº·c full setup vá»›i tÃ¹y chá»n
./full_setup.sh --fresh
```

### 4. Truy cáº­p cÃ¡c services

Sau khi setup hoÃ n táº¥t, truy cáº­p:

- **Streamlit Dashboard**: http://localhost:8501
- **Dagster UI**: http://localhost:3001
- **Metabase BI**: http://localhost:3000
- **Trino UI**: http://localhost:8082
- **Spark Master**: http://localhost:8080
- **MinIO Console**: http://localhost:9001 (minio/minio123)
- **Chat Service API**: http://localhost:8001

## ğŸ“¦ Cáº¥u trÃºc dá»± Ã¡n

```
Data_lakehouse/
â”œâ”€â”€ app/                    # Streamlit UI application
â”‚   â”œâ”€â”€ app.py             # Main dashboard
â”‚   â””â”€â”€ pages/             # Streamlit pages
â”œâ”€â”€ chat_service/          # FastAPI Chat Service
â”‚   â”œâ”€â”€ main.py           # API endpoints
â”‚   â”œâ”€â”€ skills/           # SQL generation skills
â”‚   â””â”€â”€ llm/              # LLM integration
â”œâ”€â”€ etl_pipeline/         # Spark ETL jobs
â”‚   â””â”€â”€ etl_pipeline/     # Dagster assets & jobs
â”œâ”€â”€ dagster/              # Dagster configuration
â”œâ”€â”€ docker-compose.yaml   # Service orchestration
â”œâ”€â”€ setup.sh              # Quick setup script
â”œâ”€â”€ full_setup.sh         # Full setup script
â””â”€â”€ env.example           # Environment template
```

## ğŸ”§ Cáº¥u hÃ¬nh

### Environment Variables

Copy `env.example` thÃ nh `.env` vÃ  cáº¥u hÃ¬nh:

```bash
cp env.example .env
```

**Quan trá»ng**: Cáº­p nháº­t `GOOGLE_API_KEY` trong `.env` Ä‘á»ƒ sá»­ dá»¥ng Chat Service:

```env
GOOGLE_API_KEY=your_google_api_key_here
```

### CÃ¡c biáº¿n mÃ´i trÆ°á»ng chÃ­nh:

- `MYSQL_ROOT_PASSWORD`: MySQL root password
- `MINIO_ROOT_USER`, `MINIO_ROOT_PASSWORD`: MinIO credentials
- `GOOGLE_API_KEY`: Google Gemini API key (cho Chat Service)
- `LLM_API_KEY`: OpenAI API key (tÃ¹y chá»n)

## ğŸ“Š Data Pipeline

### Medallion Architecture

1. **Bronze Layer**: Raw data tá»« MySQL
2. **Silver Layer**: Cleaned & validated data
3. **Gold Layer**: Star schema (fact & dimension tables)
4. **Platinum Layer**: Business datamarts

### Cháº¡y ETL

```bash
# Cháº¡y ETL pipeline
./full_setup.sh --etl

# Hoáº·c qua Dagster UI
# Má»Ÿ http://localhost:3001 vÃ  cháº¡y job "reload_data"
```

### Tá»‘i Æ°u hÃ³a Lakehouse (Compaction & Vacuum)

Há»‡ thá»‘ng tá»± Ä‘á»™ng cháº¡y **OPTIMIZE** vÃ  **VACUUM** cho cÃ¡c báº£ng Delta Lake má»—i ngÃ y lÃºc 3:00 AM (sau khi ETL hoÃ n thÃ nh).

#### Cháº¡y thá»§ cÃ´ng

```bash
# Cháº¡y optimize script trá»±c tiáº¿p
docker exec -it spark-master bash /scripts/optimize_lakehouse.sh

# Hoáº·c qua Dagster UI
# Má»Ÿ http://localhost:3001 vÃ  cháº¡y job "optimize_lakehouse_job"
```

#### Tá»± Ä‘á»™ng hÃ³a

- **Schedule**: Cháº¡y tá»± Ä‘á»™ng má»—i ngÃ y lÃºc 3:00 AM (sau ETL)
- **Lá»›p Ä‘Æ°á»£c tá»‘i Æ°u**: Gold vÃ  Platinum
- **Retention**: 168 giá» (7 ngÃ y) - Ä‘Ãºng chuáº©n Delta Lake
- **Chá»©c nÄƒng**:
  - **OPTIMIZE**: Gom cÃ¡c file nhá» thÃ nh file lá»›n hÆ¡n (compaction)
  - **VACUUM**: XÃ³a cÃ¡c file cÅ© khÃ´ng cÃ²n cáº§n thiáº¿t

#### Báº£ng Ä‘Æ°á»£c tá»‘i Æ°u

**Gold Layer:**
- `fact_order`
- `fact_order_item`
- `dim_customer`
- `dim_product`
- `dim_seller`

**Platinum Layer:**
- `dm_sales_monthly_category`
- `dm_seller_kpi`
- `dm_customer_lifecycle`
- `dm_payment_mix`
- `dm_logistics_sla`
- `dm_product_bestsellers`
- `dm_category_price_bands`
- `forecast_monitoring`

## ğŸ› ï¸ CÃ¡c lá»‡nh há»¯u Ã­ch

### Docker Compose

```bash
# Xem tráº¡ng thÃ¡i services
docker compose ps

# Xem logs
docker compose logs -f <service_name>

# Restart service
docker compose restart <service_name>

# Stop táº¥t cáº£
docker compose down

# Stop vÃ  xÃ³a volumes
docker compose down -v
```

### Makefile

```bash
# Build táº¥t cáº£
make build

# Start services
make up

# Stop services
make down

# Rebuild vÃ  restart
make rebuild
```

## ğŸ› Troubleshooting

### Services khÃ´ng start

1. Kiá»ƒm tra logs: `docker compose logs <service_name>`
2. Kiá»ƒm tra disk space: `df -h`
3. Kiá»ƒm tra ports Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng: `lsof -i :PORT`

### ETL pipeline fails

1. Kiá»ƒm tra MySQL connection: `docker exec de_mysql mysql -uroot -padmin123 -e "SHOW DATABASES;"`
2. Kiá»ƒm tra Spark: http://localhost:8080
3. Xem logs ETL: `docker compose logs etl_pipeline`

### Chat Service khÃ´ng hoáº¡t Ä‘á»™ng

1. Kiá»ƒm tra GOOGLE_API_KEY trong `.env`
2. Test API: `curl http://localhost:8001/healthz`
3. Xem logs: `docker compose logs chat_service`

## ğŸ“š TÃ i liá»‡u

- [Chat Service Guide](docs/chat_service_guide.md)
- [Data Dictionary](docs/data_dictionary.md)
- [KPI Definitions](docs/kpi_definitions.md)
- [Forecast Documentation](docs/forecast/README_forecast.md)

## ğŸ”’ Báº£o máº­t

- **KhÃ´ng commit** file `.env` vÃ o git
- Äá»•i passwords máº·c Ä‘á»‹nh trong production
- Sá»­ dá»¥ng environment variables cho API keys
- Giá»›i háº¡n network access trong docker-compose

## ğŸ“ License

MIT License

## ğŸ‘¤ Author

**Truong An**

- Project Creator & Developer
- Data Lakehouse - Modern Data Stack
- License: MIT

> ThÃ´ng tin tÃ¡c giáº£ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong:
> - `AUTHORS.md` - Chi tiáº¿t vá» tÃ¡c giáº£
> - `app/app.py` - Footer cá»§a Streamlit dashboard
> - Git commit history (náº¿u cÃ³)

---

**Happy Data Engineering! ğŸš€**

