# -*- coding: utf-8 -*-
"""
About Dataset Provider
Fetches dataset metadata from Trino and formats it for chat responses
"""
import os
from typing import Dict, List, Optional
from datetime import datetime
import trino
from trino.dbapi import connect

# Trino connection config
TRINO_HOST = os.getenv("TRINO_HOST", "trino")
TRINO_PORT = int(os.getenv("TRINO_PORT", "8080"))
TRINO_USER = os.getenv("TRINO_USER", "admin")
TRINO_CATALOG = os.getenv("TRINO_CATALOG", "lakehouse")

# Cache for metadata (5 minutes TTL)
_metadata_cache = None
_cache_timestamp = None
CACHE_TTL = 300  # 5 minutes


def get_trino_connection():
    """Get Trino connection"""
    return connect(
        host=TRINO_HOST,
        port=TRINO_PORT,
        user=TRINO_USER,
        catalog=TRINO_CATALOG,
        http_scheme="http"
    )


def fetch_table_info(schema: str, table: str) -> Dict:
    """Fetch basic info about a table"""
    try:
        conn = get_trino_connection()
        cur = conn.cursor()
        
        # Get row count (approximate)
        cur.execute(f"SELECT COUNT(*) as cnt FROM {schema}.{table} LIMIT 1")
        row = cur.fetchone()
        row_count = row[0] if row else 0
        
        # Get column info
        cur.execute(f"DESCRIBE {schema}.{table}")
        columns = cur.fetchall()
        
        # Find time columns
        time_cols = []
        for col in columns:
            col_name = col[0].lower()
            col_type = col[1].lower() if len(col) > 1 else ""
            if any(t in col_type for t in ["date", "timestamp"]) or any(t in col_name for t in ["date", "time", "ts", "month"]):
                time_cols.append(col[0])
        
        cur.close()
        conn.close()
        
        return {
            "row_count": row_count,
            "columns": [col[0] for col in columns],
            "time_columns": time_cols,
            "num_columns": len(columns)
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error fetching table info for {schema}.{table}: {e}")
        return {
            "row_count": 0,
            "columns": [],
            "time_columns": [],
            "num_columns": 0
        }


def get_dataset_overview() -> Dict:
    """
    Get dataset overview from Trino
    Returns summary of tables, layers, and data coverage
    """
    try:
        conn = get_trino_connection()
        cur = conn.cursor()
        
        # Get all tables from gold and platinum schemas
        gold_tables = []
        platinum_tables = []
        
        # Gold tables
        try:
            cur.execute("SHOW TABLES FROM lakehouse.gold")
            gold_tables = [row[0] for row in cur.fetchall()]
        except:
            pass
        
        # Platinum tables
        try:
            cur.execute("SHOW TABLES FROM lakehouse.platinum")
            platinum_tables = [row[0] for row in cur.fetchall()]
        except:
            pass
        
        # Get info for key tables
        key_tables = {
            "gold": ["fact_order", "fact_order_item", "dim_product", "dim_customer", "dim_seller"],
            "platinum": ["dm_sales_monthly_category", "dm_customer_lifecycle", "dm_seller_kpi", "dm_logistics_sla", "dm_payment_mix"]
        }
        
        table_info = {}
        total_rows = 0
        
        # Sample key tables for row counts
        for schema, tables in [("gold", gold_tables), ("platinum", platinum_tables)]:
            for table in tables:
                if table in key_tables.get(schema, []):
                    info = fetch_table_info(f"lakehouse.{schema}", table)
                    table_info[f"{schema}.{table}"] = info
                    total_rows += info.get("row_count", 0)
        
        # Get time range from fact_order (if exists)
        time_range = {"min": None, "max": None}
        if "fact_order" in gold_tables:
            try:
                cur.execute("""
                    SELECT 
                        MIN(CAST(full_date AS DATE)) as min_date,
                        MAX(CAST(full_date AS DATE)) as max_date
                    FROM lakehouse.gold.fact_order
                """)
                row = cur.fetchone()
                if row and row[0]:
                    time_range["min"] = str(row[0])
                    time_range["max"] = str(row[1]) if row[1] else None
            except:
                pass
        
        cur.close()
        conn.close()
        
        return {
            "gold_tables": gold_tables,
            "platinum_tables": platinum_tables,
            "total_gold_tables": len(gold_tables),
            "total_platinum_tables": len(platinum_tables),
            "table_info": table_info,
            "total_rows_sample": total_rows,
            "time_range": time_range
        }
    except Exception as e:
        print(f"‚ö†Ô∏è  Error getting dataset overview: {e}")
        return {
            "gold_tables": [],
            "platinum_tables": [],
            "total_gold_tables": 0,
            "total_platinum_tables": 0,
            "table_info": {},
            "total_rows_sample": 0,
            "time_range": {"min": None, "max": None}
        }


def get_about_dataset_card() -> str:
    """
    Generate formatted card about dataset
    """
    global _metadata_cache, _cache_timestamp
    
    # Check cache
    if _metadata_cache and _cache_timestamp:
        age = (datetime.now().timestamp() - _cache_timestamp)
        if age < CACHE_TTL:
            overview = _metadata_cache
        else:
            overview = get_dataset_overview()
            _metadata_cache = overview
            _cache_timestamp = datetime.now().timestamp()
    else:
        overview = get_dataset_overview()
        _metadata_cache = overview
        _cache_timestamp = datetime.now().timestamp()
    
    # Format response
    parts = [
        "**üìä D·ªØ li·ªáu TMƒêT Brazil (Olist E-commerce Dataset)**\n",
        "**üìà Quy m√¥ d·ªØ li·ªáu:**",
        f"  ‚Ä¢ **Gold Layer**: {overview['total_gold_tables']} b·∫£ng (Fact & Dimension tables)",
        f"  ‚Ä¢ **Platinum Layer**: {overview['total_platinum_tables']} b·∫£ng (Pre-aggregated datamarts)",
        f"  ‚Ä¢ **T·ªïng m·∫´u**: ~{overview['total_rows_sample']:,} rows (t·ª´ c√°c b·∫£ng ch√≠nh)\n"
    ]
    
    # Time range
    if overview['time_range']['min']:
        parts.append("**üìÖ Th·ªùi gian:**")
        parts.append(f"  ‚Ä¢ **Ph·∫°m vi**: {overview['time_range']['min']} ƒë·∫øn {overview['time_range']['max'] or 'N/A'}")
        parts.append("  ‚Ä¢ **Lo·∫°i**: Batch data (kh√¥ng realtime)")
        parts.append("  ‚Ä¢ **C·∫≠p nh·∫≠t**: D·ªØ li·ªáu tƒ©nh, ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l√†m s·∫°ch\n")
    
    # Key tables
    parts.append("**üèóÔ∏è Ki·∫øn tr√∫c Medallion (Lakehouse):**")
    parts.append("  ‚Ä¢ **Bronze**: Raw data t·ª´ CSV (ch∆∞a x·ª≠ l√Ω)")
    parts.append("  ‚Ä¢ **Silver**: Data ƒë√£ l√†m s·∫°ch, chu·∫©n h√≥a (null handling, type casting)")
    parts.append("  ‚Ä¢ **Gold**: Fact & Dimension tables (star schema)")
    parts.append("    - `fact_order`, `fact_order_item` (measures)")
    parts.append("    - `dim_product`, `dim_customer`, `dim_seller`, `dim_geolocation`, `dim_date`")
    parts.append("  ‚Ä¢ **Platinum**: Datamarts t·ªïng h·ª£p (pre-aggregated)\n")
    
    # Top tables by row count
    if overview['table_info']:
        parts.append("**üì¶ B·∫£ng ch√≠nh (m·∫´u):**")
        sorted_tables = sorted(
            overview['table_info'].items(),
            key=lambda x: x[1].get('row_count', 0),
            reverse=True
        )[:5]
        
        for table_name, info in sorted_tables:
            row_count = info.get('row_count', 0)
            if row_count > 0:
                parts.append(f"  ‚Ä¢ `{table_name}`: ~{row_count:,} rows")
    
    # Datamarts
    platinum_dm = [t for t in overview['platinum_tables'] if t.startswith('dm_')]
    if platinum_dm:
        parts.append("\n**üì¶ Datamarts ch√≠nh (Platinum layer):**")
        dm_descriptions = {
            "dm_sales_monthly_category": "Doanh thu theo danh m·ª•c/th√°ng (GMV, orders, units, AOV)",
            "dm_customer_lifecycle": "Ph√¢n t√≠ch cohort & retention (customers_active, retention_pct)",
            "dm_seller_kpi": "KPI nh√† b√°n (GMV, orders, on_time_rate, cancel_rate, avg_review_score)",
            "dm_logistics_sla": "SLA giao h√†ng theo v√πng (delivery_days_avg, on_time_rate)",
            "dm_payment_mix": "T·ª∑ tr·ªçng ph∆∞∆°ng th·ª©c thanh to√°n (credit_card, boleto, voucher, debit_card)",
            "demand_forecast": "D·ª± b√°o nhu c·∫ßu (ML model v·ªõi confidence intervals)"
        }
        
        for dm in platinum_dm[:6]:
            desc = dm_descriptions.get(dm, "Datamart t·ªïng h·ª£p")
            parts.append(f"  ‚Ä¢ `{dm}`: {desc}")
    
    parts.append("\n**üí° L∆∞u √Ω quan tr·ªçng:**")
    parts.append("  ‚Ä¢ D·ªØ li·ªáu **batch** n√™n s·ªë li·ªáu ·ªïn ƒë·ªãnh, kh√¥ng realtime")
    parts.append("  ‚Ä¢ T·∫•t c·∫£ queries l√† **read-only** (ch·ªâ SELECT, kh√¥ng INSERT/UPDATE/DELETE)")
    parts.append("  ‚Ä¢ Schema whitelist: ch·ªâ truy v·∫•n `lakehouse.gold` v√† `lakehouse.platinum`")
    parts.append("  ‚Ä¢ T·ª± ƒë·ªông √°p d·ª•ng **LIMIT** v√† **timeout** ƒë·ªÉ b·∫£o v·ªá hi·ªáu su·∫•t")
    
    return "\n".join(parts)


if __name__ == "__main__":
    # Test
    print("="*60)
    print("Testing About Dataset Provider")
    print("="*60)
    card = get_about_dataset_card()
    print(card)

