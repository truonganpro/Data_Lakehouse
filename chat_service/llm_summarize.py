# -*- coding: utf-8 -*-
"""
LLM-based result summarization module using Google Gemini
"""
import os
import re
from typing import List, Dict, Optional
import google.generativeai as genai
from llm.registry import generate_with_fallback


PROMPT_SUMMARY = """B·∫°n l√† nh√† ph√¢n t√≠ch d·ªØ li·ªáu chuy√™n nghi·ªáp. T√≥m t·∫Øt k·∫øt qu·∫£ truy v·∫•n theo c·∫•u tr√∫c CH√çNH X√ÅC 3 c√¢u b·∫±ng ti·∫øng Vi·ªát.

ƒê·∫¶U V√ÄO:
- C√¢u h·ªèi: {question}
- B·∫£ng d·ªØ li·ªáu (t·ªëi ƒëa 50 h√†ng): 
{table_preview}

{citations_section}

Y√äU C·∫¶U (CH√çNH X√ÅC 3 C√ÇU):

**C√¢u 1**: Gi·∫£i th√≠ch ng·∫Øn dataset + source (schema.table) + kho·∫£ng th·ªùi gian n·∫øu c√≥.
- N√™u ngu·ªìn d·ªØ li·ªáu (v√≠ d·ª•: "D·ªØ li·ªáu t·ª´ `lakehouse.platinum.dm_sales_monthly_category`")
- N√™u ph·∫°m vi th·ªùi gian n·∫øu c√≥ (v√≠ d·ª•: "t·ª´ th√°ng 06-08/2018" ho·∫∑c "top 10 s·∫£n ph·∫©m")
- Ng·∫Øn g·ªçn, ch·ªâ 1 c√¢u

**C√¢u 2**: N√™u insight ch√≠nh: xu h∆∞·ªõng tƒÉng/gi·∫£m, nh√≥m top/bottom, so s√°nh quan tr·ªçng.
- Xu h∆∞·ªõng: tƒÉng/gi·∫£m, cao nh·∫•t/th·∫•p nh·∫•t, bi·∫øn ƒë·ªông
- So s√°nh: gi·ªØa c√°c nh√≥m, th·ªùi k·ª≥, categories
- S·ªë li·ªáu c·ª• th·ªÉ (v√≠ d·ª•: "t·ª´ 1.23M ‚Üí 987K", "cao nh·∫•t 1.12M")
- Ng·∫Øn g·ªçn, ch·ªâ 1 c√¢u

**C√¢u 3**: ƒê∆∞a ra 1 g·ª£i √Ω h√†nh ƒë·ªông (actionable) cho business.
- G·ª£i √Ω c·ª• th·ªÉ d·ª±a tr√™n insight (v√≠ d·ª•: "N√™n t·∫≠p trung marketing v√†o th√°ng cao ƒëi·ªÉm")
- Actionable: c√≥ th·ªÉ th·ª±c hi·ªán ƒë∆∞·ª£c, kh√¥ng chung chung
- Li√™n quan ƒë·∫øn k·∫øt qu·∫£ ph√¢n t√≠ch
- Ng·∫Øn g·ªçn, ch·ªâ 1 c√¢u

L∆ØU √ù:
- KH√îNG b·ªãa s·ªë: Ch·ªâ d√πng s·ªë li·ªáu c√≥ trong b·∫£ng
- CH√çNH X√ÅC 3 c√¢u, kh√¥ng nhi·ªÅu h∆°n, kh√¥ng √≠t h∆°n
- M·ªói c√¢u ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu
- D√πng ti·∫øng Vi·ªát t·ª± nhi√™n

V√≠ d·ª•:
"D·ªØ li·ªáu t·ª´ `lakehouse.platinum.dm_sales_monthly_category` t·ª´ th√°ng 06-08/2018 cho th·∫•y doanh thu gi·∫£m d·∫ßn t·ª´ 1.23M ‚Üí 987K, v·ªõi th√°ng cao nh·∫•t l√† 07/2018 (1.12M). Xu h∆∞·ªõng gi·∫£m nh·∫π nh∆∞ng ·ªïn ƒë·ªãnh, kh√¥ng c√≥ bi·∫øn ƒë·ªông ƒë·ªôt ng·ªôt. N√™n t·∫≠p trung ph√¢n t√≠ch nguy√™n nh√¢n gi·∫£m v√† tƒÉng c∆∞·ªùng marketing v√†o th√°ng cao ƒëi·ªÉm (07/2018) ƒë·ªÉ duy tr√¨ hi·ªáu qu·∫£."

Tr·∫£ l·ªùi CH√çNH X√ÅC 3 C√ÇU, NG·∫ÆN G·ªåN, CH√çNH X√ÅC, D·ªÑ HI·ªÇU, ACTIONABLE.
"""


def dedupe_citations(citations: List[dict], max_items: int = 3) -> List[dict]:
    """
    Deduplicate citations by source, keeping the first occurrence (highest score)
    
    Args:
        citations: List of citation dicts with 'source' key
        max_items: Maximum number of unique citations to return
        
    Returns:
        List of deduplicated citations
    """
    if not citations:
        return []
    
    seen = set()
    out = []
    for c in citations:
        src = (c.get("source") or "unknown").strip()
        if src in seen:
            continue
        seen.add(src)
        out.append(c)
        if len(out) >= max_items:
            break
    return out


PROMPT_DOCS_QA = """B·∫°n l√† tr·ª£ l√Ω chuy√™n gi·∫£i th√≠ch c√°c kh√°i ni·ªám v√† metrics trong h·ªá th·ªëng d·ª± b√°o nhu c·∫ßu.

D·ª±a tr√™n c√°c ƒëo·∫°n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ng·∫Øn g·ªçn, ch√≠nh x√°c v√† d·ªÖ hi·ªÉu.

ƒê·∫¶U V√ÄO:
- C√¢u h·ªèi: {question}
- T√†i li·ªáu li√™n quan:
{citations_text}

Y√äU C·∫¶U:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n v√† d·ªÖ hi·ªÉu
- Ch·ªâ d√πng th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, kh√¥ng b·ªãa th√™m
- N·∫øu t√†i li·ªáu c√≥ s·ªë li·ªáu c·ª• th·ªÉ (v√≠ d·ª•: "sMAPE < 20% l√† r·∫•t t·ªët"), h√£y n√™u r√µ
- N·∫øu c√¢u h·ªèi v·ªÅ ƒë·ªãnh nghƒ©a, gi·∫£i th√≠ch ng·∫Øn g·ªçn c√¥ng th·ª©c/kh√°i ni·ªám
- N·∫øu c√¢u h·ªèi v·ªÅ ng∆∞·ª°ng t·ªët/x·∫•u, n√™u r√µ c√°c m·ª©c ƒë·ªô

Tr·∫£ l·ªùi:
"""


def summarize_with_gemini(
    question: str,
    table_preview: List[Dict],
    citations: Optional[List[Dict]] = None
) -> Optional[str]:
    """
    Summarize query results using Gemini
    
    Args:
        question: Original user question
        table_preview: List of dictionaries (rows from SQL result)
        citations: Optional list of RAG citations
        
    Returns:
        Summary text or None if LLM_PROVIDER is not gemini
    """
    if os.getenv("LLM_PROVIDER", "none").lower() != "gemini":
        return None
    
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è  GOOGLE_API_KEY not set, cannot use Gemini for summarization")
        return None
    
    try:
        genai.configure(api_key=api_key)
        
        # Use gemini-2.0-flash for fast, cost-effective summarization
        model = genai.GenerativeModel("gemini-2.0-flash")
        
        # Format table preview
        if table_preview:
            # Limit to first 50 rows
            preview_rows = table_preview[:50]
            
            # Format as text table
            if preview_rows:
                # Get column names from first row
                columns = list(preview_rows[0].keys())
                
                table_text = "| " + " | ".join(columns) + " |\n"
                table_text += "|" + "|".join(["---"] * len(columns)) + "|\n"
                
                for row in preview_rows[:10]:  # Show first 10 rows in detail
                    values = [str(row.get(col, "")) for col in columns]
                    table_text += "| " + " | ".join(values) + " |\n"
                
                if len(preview_rows) > 10:
                    table_text += f"\n... v√† {len(preview_rows) - 10} d√≤ng kh√°c.\n"
                    table_text += f"\nT·ªïng c·ªông: {len(table_preview)} d√≤ng."
            else:
                table_text = "(Kh√¥ng c√≥ d·ªØ li·ªáu)"
        else:
            table_text = "(Kh√¥ng c√≥ d·ªØ li·ªáu)"
        
        # Format citations (dedupe tr∆∞·ªõc khi format)
        citations_section = ""
        if citations and len(citations) > 0:
            citations_unique = dedupe_citations(citations, max_items=3)
            citations_section = "- T√†i li·ªáu tham kh·∫£o:\n"
            for cite in citations_unique:
                citations_section += f"  * {cite.get('source', 'unknown')} (ƒë·ªô li√™n quan: {cite.get('score', 0):.2f})\n"
        
        prompt = PROMPT_SUMMARY.format(
            question=question,
            table_preview=table_text,
            citations_section=citations_section
        )
        
        print(f"ü§ñ Summarizing with Gemini...")
        
        response = model.generate_content(prompt)
        summary = response.text.strip()
        
        print(f"‚úÖ Generated summary: {summary[:100]}...")
        
        return summary
        
    except Exception as e:
        print(f"‚ùå Error summarizing with Gemini: {e}")
        return None


def _explain_sql_and_lineage(sql: str, source_schema: str, rows_preview: Optional[List[Dict]]) -> str:
    """
    Explain SQL calculation and data lineage
    
    Args:
        sql: SQL query string
        source_schema: Source schema (gold/platinum)
        rows_preview: Query results (to infer measures)
        
    Returns:
        Explanation text or empty string
    """
    if not sql:
        return ""
    
    sql_lower = sql.lower()
    explain_parts = []
    
    # Detect main table
    main_table = None
    if "from" in sql_lower:
        # Simple extraction: FROM table_name or FROM schema.table_name
        from_match = re.search(r'from\s+[\w.]+\.([\w]+)', sql_lower)
        if from_match:
            main_table = from_match.group(1)
        else:
            from_match = re.search(r'from\s+([\w]+)', sql_lower)
            if from_match:
                main_table = from_match.group(1)
    
    # Detect measures (SUM, COUNT, AVG, etc.)
    measures = []
    if "sum(" in sql_lower:
        measures.append("t·ªïng")
    if "count(" in sql_lower or "count(*)" in sql_lower:
        measures.append("s·ªë l∆∞·ª£ng")
    if "avg(" in sql_lower or "average(" in sql_lower:
        measures.append("trung b√¨nh")
    
    # Detect dimensions (GROUP BY)
    dimensions = []
    if "group by" in sql_lower:
        group_by_match = re.search(r'group\s+by\s+([^order\s]+)', sql_lower, re.IGNORECASE)
        if group_by_match:
            group_cols = group_by_match.group(1).strip()
            # Extract column names (simple heuristic)
            for col in group_cols.split(','):
                # ‚úÖ FIX: Safe split - handle empty strings
                col_parts = col.strip().split()
                if col_parts:
                    col = col_parts[-1]  # Get last word (column name)
                else:
                    continue  # Skip empty columns
                if col and col not in ['1', '2', '3', '4', '5']:  # Skip positional numbers
                    dimensions.append(col)
    
    # Build explanation
    if main_table:
        # Determine layer
        if source_schema == "platinum":
            layer_desc = "datamart t·ªïng h·ª£p (pre-aggregated)"
            lineage = f"Bronze ‚Üí Silver ‚Üí Gold ‚Üí Platinum (`{main_table}`)"
        else:
            layer_desc = "fact/dimension tables"
            if "fact_" in main_table:
                lineage = f"Bronze ‚Üí Silver ‚Üí Gold (`{main_table}`)"
            elif "dim_" in main_table:
                lineage = f"Bronze ‚Üí Silver ‚Üí Gold (`{main_table}` - dimension table)"
            else:
                lineage = f"Bronze ‚Üí Silver ‚Üí Gold (`{main_table}`)"
        
        explain_parts.append(f"‚Ä¢ **Ngu·ªìn d·ªØ li·ªáu**: `lakehouse.{source_schema}.{main_table}` ({layer_desc})")
        explain_parts.append(f"‚Ä¢ **Lineage**: {lineage}")
    
    # Explain measures if detected
    if measures:
        measure_desc = ", ".join(measures)
        explain_parts.append(f"‚Ä¢ **Ph√©p t√≠nh**: {measure_desc}")
    
    # Explain dimensions if detected
    if dimensions and len(dimensions) <= 3:
        dim_desc = ", ".join(dimensions[:3])
        explain_parts.append(f"‚Ä¢ **Nh√≥m theo**: {dim_desc}")
    
    # Add KPI explanations for common measures
    if rows_preview and len(rows_preview) > 0:
        columns = list(rows_preview[0].keys())
        
        # Check for common KPIs
        kpi_explanations = {
            "gmv": "GMV = t·ªïng (price √ó quantity + freight_value)",
            "revenue": "Revenue = t·ªïng (price √ó quantity + freight_value)",
            "aov": "AOV = GMV / s·ªë ƒë∆°n h√†ng",
            "orders": "Orders = s·ªë l∆∞·ª£ng ƒë∆°n h√†ng duy nh·∫•t",
            "units": "Units = t·ªïng s·ªë l∆∞·ª£ng s·∫£n ph·∫©m",
            "retention": "Retention = (kh√°ch h√†ng active / cohort size) √ó 100%",
            "on_time_rate": "On-time rate = (ƒë∆°n giao ƒë√∫ng h·∫°n / t·ªïng ƒë∆°n) √ó 100%"
        }
        
        for col in columns:
            col_lower = col.lower()
            for kpi, explanation in kpi_explanations.items():
                if kpi in col_lower:
                    explain_parts.append(f"‚Ä¢ **{col}**: {explanation}")
                    break
    
    return "\n".join(explain_parts) if explain_parts else ""


def _parse_schema_from_sql(sql: str) -> str:
    """
    Parse schema name (gold/platinum) from SQL query
    
    Args:
        sql: SQL query string
        
    Returns:
        Schema name (gold or platinum), default to 'gold'
    """
    if not sql:
        return "gold"
    
    sql_lower = sql.lower()
    
    # Check for platinum first (more specific)
    if "platinum" in sql_lower:
        return "platinum"
    elif "gold" in sql_lower:
        return "gold"
    else:
        # Default to gold
        return "gold"


def format_answer(
    question: str,
    sql_query: Optional[str],
    rows_preview: Optional[List[Dict]],
    citations: Optional[List[Dict]],
    execution_time_ms: int,
    error: Optional[str] = None,
    source_schema: Optional[str] = None,
    suggestions: Optional[List[str]] = None
) -> str:
    """
    Format complete answer with header, summary, and suggestions
    
    Args:
        question: Original question
        sql_query: Executed SQL query
        rows_preview: Query results
        citations: RAG citations
        execution_time_ms: Execution time
        error: Error message if any
        source_schema: Source schema (gold/platinum), auto-parsed if None
        suggestions: List of suggestion strings
        
    Returns:
        Formatted answer text
    """
    answer_parts = []
    
    if error:
        # Error case: don't add header, just show error with suggestions
        answer_parts.append(error)
        return "\n".join(answer_parts)
    
    # Success case: Add header with data provenance
    # ‚úÖ FIX: X·ª≠ l√Ω 0 rows nh∆∞ output h·ª£p l·ªá
    has_no_data = rows_preview is not None and len(rows_preview) == 0
    
    if sql_query:
        # Parse schema if not provided
        if not source_schema:
            source_schema = _parse_schema_from_sql(sql_query)
        
        # Data freshness message (fixed for batch Olist data)
        data_freshness = "D·ªØ li·ªáu batch (2016-2018), kh√¥ng realtime"
        
        # Header with data provenance
        header = f"üóÇÔ∏è **Ngu·ªìn:** `lakehouse.{source_schema}` ‚Ä¢ ‚è±Ô∏è **Th·ªùi gian ch·∫°y:** {execution_time_ms}ms ‚Ä¢ üì¶ {data_freshness}"
        answer_parts.append(header)
        answer_parts.append("")  # Empty line
        
        # ‚úÖ FIX: X·ª≠ l√Ω 0 rows v·ªõi th√¥ng b√°o m·ªÅm
        if has_no_data:
            # Hi·ªÉn th·ªã th√¥ng b√°o "Kh√¥ng c√≥ d·ªØ li·ªáu" nh∆∞ m·ªôt k·∫øt qu·∫£ h·ª£p l·ªá
            answer_parts.append("üì≠ **Kh√¥ng c√≥ d·ªØ li·ªáu kh·ªõp ƒëi·ªÅu ki·ªán hi·ªán t·∫°i.**")
            answer_parts.append("")
            answer_parts.append("üí° **G·ª£i √Ω:** H√£y th·ª≠:")
            answer_parts.append("  ‚Ä¢ M·ªü r·ªông kho·∫£ng th·ªùi gian ho·∫∑c horizon")
            answer_parts.append("  ‚Ä¢ B·ªè b·ªõt b·ªô l·ªçc (region, category)")
            answer_parts.append("  ‚Ä¢ Ki·ªÉm tra l·∫°i ƒëi·ªÅu ki·ªán filter")
            answer_parts.append("")
        else:
            # C√≥ d·ªØ li·ªáu - hi·ªÉn th·ªã summary b√¨nh th∆∞·ªùng
            # Try to get Gemini summary
            summary = summarize_with_gemini(question, rows_preview, citations)
            
            if summary:
                answer_parts.append("üìù **T√≥m t·∫Øt:**")
                answer_parts.append(summary)
                answer_parts.append("")  # Empty line
            else:
                # Fallback: brief info about results
                answer_parts.append(f"üìä **K·∫øt qu·∫£:** {len(rows_preview)} d√≤ng")
                answer_parts.append("")  # Empty line
        
        # Add SQL explanation and lineage (Vi·ªác D)
        # ‚úÖ FIX: Ch·ªâ explain khi c√≥ d·ªØ li·ªáu ho·∫∑c kh√¥ng ph·∫£i no_data
        if not has_no_data:
            try:
                explain_text = _explain_sql_and_lineage(sql_query, source_schema, rows_preview)
            except Exception as e:
                print(f"‚ö†Ô∏è  Explanation error (non-critical): {e}")
                explain_text = None
        else:
            explain_text = None
            
        if explain_text:
            answer_parts.append("üß† **C√°ch t√≠nh:**")
            answer_parts.append(explain_text)
            answer_parts.append("")  # Empty line
    
    # Add citations (if any) - dedupe tr∆∞·ªõc khi format
    if citations and len(citations) > 0:
        citations_unique = dedupe_citations(citations, max_items=3)
        answer_parts.append("üìö **T√†i li·ªáu tham kh·∫£o:**")
        for cite in citations_unique:
            answer_parts.append(
                f"  ‚Ä¢ {cite.get('source', 'unknown')} "
                f"(ƒë·ªô li√™n quan: {cite.get('score', 0):.2f})"
            )
        answer_parts.append("")  # Empty line
    
    # Note: suggestions are handled separately in AskResponse model
    # They will be displayed as buttons in the UI
    
    return "\n".join(answer_parts)


if __name__ == "__main__":
    # Test summarization
    test_question = "Doanh thu theo th√°ng 3 th√°ng g·∫ßn ƒë√¢y?"
    
    test_data = [
        {"month": "2018-08-01", "revenue": 1234567.89, "orders": 5432},
        {"month": "2018-07-01", "revenue": 1123456.78, "orders": 4987},
        {"month": "2018-06-01", "revenue": 987654.32, "orders": 4123},
    ]
    
    test_citations = [
        {"source": "data_dictionary.md", "score": 0.89, "text": "revenue = total_price"},
        {"source": "kpi_definitions.md", "score": 0.76, "text": "Revenue KPIs"},
    ]
    
    print("="*60)
    print("Testing Gemini Summarization")
    print("="*60)
    
    summary = summarize_with_gemini(test_question, test_data, test_citations)
    
    if summary:
        print(f"\n‚úÖ Summary:\n{summary}")
    else:
        print("\n‚ùå No summary generated (LLM_PROVIDER not set to gemini?)")
    
    print("\n" + "="*60)
    print("Testing Full Answer Formatting")
    print("="*60)
    
    answer = format_answer(
        question=test_question,
        sql_query="SELECT ... FROM ...",
        rows_preview=test_data,
        citations=test_citations,
        execution_time_ms=234
    )
    
    print(f"\n{answer}")


def summarize_docs_with_llm(question: str, citations: List[Dict]) -> Optional[str]:
    """
    Summarize documents from RAG search using LLM (for conceptual questions)
    
    Args:
        question: User's question
        citations: List of RAG citation dicts with 'text' and 'source'
        
    Returns:
        Summary text or None if LLM not available
    """
    if not citations:
        print("‚ö†Ô∏è  No citations provided for summarize_docs_with_llm")
        return None
    
    # Build citations text (dedupe tr∆∞·ªõc khi format)
    citations_unique = dedupe_citations(citations, max_items=4)
    citations_text = "\n\n".join([
        f"[{i+1}] {cite.get('text', '')}\n(Ngu·ªìn: {cite.get('source', 'unknown')})"
        for i, cite in enumerate(citations_unique)
    ])
    
    print(f"üìù Citations text length: {len(citations_text)} chars")
    
    # Use LLM to generate answer from docs
    try:
        prompt = PROMPT_DOCS_QA.format(
            question=question,
            citations_text=citations_text
        )
        
        print(f"ü§ñ Calling generate_with_fallback with kind='summary'")
        answer = generate_with_fallback(
            prompt=prompt,
            kind="summary",  # Use summary kind for conceptual questions
            system=None
        )
        
        if answer:
            print(f"‚úÖ LLM generated answer: {len(answer)} chars")
        else:
            print(f"‚ö†Ô∏è  LLM returned None or empty string")
        
        return answer
    except Exception as e:
        print(f"‚ö†Ô∏è  Error summarizing docs with LLM: {e}")
        import traceback
        traceback.print_exc()
        return None

