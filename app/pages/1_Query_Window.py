import os
import re
import streamlit as st
import pandas as pd
from datetime import date, datetime, timedelta
from trino.dbapi import connect
from trino.auth import BasicAuthentication
import sys
from pathlib import Path

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.time_utils import (
    half_open_range, dual_predicates, timegrain_expr,
    coerce_date_col, format_time_bucket_alias
)
from utils.sql_guardrails import (
    is_safe_select, maybe_add_limit, check_explain_analyze,
    check_order_by_with_limit, detect_cast_on_partition,
    auto_fix_cast_on_partition, check_multi_statement_safe,
    check_grouping_sets_fallback
)

# ====== Config ======
def get_conf(key, default=None):
    # ∆∞u ti√™n secrets.toml, sau ƒë√≥ env
    try:
        return st.secrets.get(key, os.getenv(key, default))
    except:
        return os.getenv(key, default)

TRINO_HOST = get_conf("TRINO_HOST", "trino")
TRINO_PORT = int(get_conf("TRINO_PORT", "8080"))
TRINO_CATALOG = get_conf("TRINO_CATALOG", "lakehouse")
TRINO_USER = get_conf("TRINO_USER", "admin")
TRINO_PASSWORD = get_conf("TRINO_PASSWORD", "") or None

DEFAULT_SCHEMA = get_conf("TRINO_DEFAULT_SCHEMA", "gold")

# ====== Helpers ======
@st.cache_data(ttl=600, show_spinner="ƒêang ch·∫°y truy v·∫•n...")
def run_query(sql: str, schema: str):
    """Execute SQL query on Trino and return DataFrame"""
    try:
        conn = connect(
            host=TRINO_HOST,
            port=TRINO_PORT,
            user=TRINO_USER,
            catalog=TRINO_CATALOG,
            schema=schema,
            http_scheme="http",
            auth=None if not TRINO_PASSWORD else BasicAuthentication(TRINO_USER, TRINO_PASSWORD),
            source="streamlit-query-window"
        )
        cur = conn.cursor()
        cur.execute(sql)
        rows = cur.fetchall()
        cols = [d[0] for d in cur.description]
        conn.close()
        return pd.DataFrame(rows, columns=cols)
    except Exception as e:
        st.error(f"L·ªói k·∫øt n·ªëi Trino: {e}")
        raise

# ====== Date Coverage Detection ======
@st.cache_data(ttl=3600)  # Cache 1 hour
def get_date_coverage():
    """Get actual date range from Brazilian E-commerce (Olist) data"""
    coverage_sqls = [
        ("gold", "fact_order", "SELECT CAST(MIN(full_date) AS date) AS min_d, CAST(MAX(full_date) AS date) AS max_d FROM lakehouse.gold.fact_order"),
        ("gold", "fact_order_item", "SELECT CAST(MIN(full_date) AS date) AS min_d, CAST(MAX(full_date) AS date) AS max_d FROM lakehouse.gold.fact_order_item"),
        ("platinum", "demand_forecast", "SELECT CAST(MIN(forecast_date) AS date) AS min_d, CAST(MAX(forecast_date) AS date) AS max_d FROM lakehouse.platinum.demand_forecast"),
    ]
    
    for schema, table, sql in coverage_sqls:
        try:
            df = run_query(sql, schema)
            if not df.empty and pd.notna(df.loc[0, "min_d"]) and pd.notna(df.loc[0, "max_d"]):
                min_date = df.loc[0, "min_d"]
                max_date = df.loc[0, "max_d"]
                return date.fromisoformat(str(min_date)), date.fromisoformat(str(max_date)), False  # False = not fallback
        except Exception as e:
            st.session_state['_coverage_error'] = str(e)
            continue
    
    # Fallback: Brazilian E-commerce (Olist) actual range
    return date(2016, 9, 4), date(2018, 10, 17), True  # True = is fallback

# Get date coverage globally
_coverage_result = get_date_coverage()
COVER_MIN, COVER_MAX = _coverage_result[0], _coverage_result[1]
USING_FALLBACK_DATE = _coverage_result[2] if len(_coverage_result) > 2 else False

def build_sql(catalog:str, schema:str, table:str, date_col:str, grain:str,
              dims:list, measures:list, start:date, end:date, extra_filters:str, 
              use_rollup:bool, use_grouping_sets:bool, limit:int=None):
    """
    Build SQL query with time grain, dimensions, and measures
    C·∫£i thi·ªán: dual predicates, pre-select cho GROUPING SETS/ROLLUP
    """
    # Detect if date_col is year_month (varchar) vs full_date (date)
    is_year_month = date_col in ["year_month"]
    
    # Chu·∫©n h√≥a half-open range
    start_date, end_next_date = half_open_range(start, end)
    
    # Time grain expression (d√πng time_utils)
    grain_expr = timegrain_expr(grain, date_col, is_year_month)
    time_bucket_alias = format_time_bucket_alias(grain)
    
    # Build WHERE clause - dual predicates cho partition pruning
    if is_year_month:
        # Gi·∫£ ƒë·ªãnh b·∫£ng c√≥ c·∫£ year_month (VARCHAR partition) v√† full_date (DATE)
        prune_pred, exact_pred = dual_predicates(
            date_col,  # year_month VARCHAR
            "full_date",  # full_date DATE
            start_date,
            end_next_date
        )
        where = [prune_pred, exact_pred]
    else:
        # For date columns, use standard half-open filtering
        where = [
            f"{date_col} >= DATE '{start_date}'",
            f"{date_col} < DATE '{end_next_date}'"
        ]
    
    if extra_filters.strip():
        # Ki·ªÉm tra an to√†n v·ªõi is_safe_select (ƒë√£ b·ªè string literal)
        if not is_safe_select(f"SELECT * FROM dummy WHERE {extra_filters}"):
            raise ValueError("Filter ch·ª©a t·ª´ kh√≥a kh√¥ng h·ª£p l·ªá (DDL/DML)")
        where.append(f"({extra_filters})")
    where_sql = " AND ".join(where)
    
    from_ = f"{catalog}.{schema}.{table}"
    
    # C·∫£i thi·ªán: Fallback GROUPING SETS/ROLLUP n·∫øu kh√¥ng c√≥ dims
    use_grouping_sets, use_rollup = check_grouping_sets_fallback(
        use_grouping_sets, use_rollup, dims
    )
    
    # C·∫£i thi·ªán: Pre-select time_bucket khi d√πng ROLLUP/GROUPING SETS
    # (Trino kh√¥ng cho group theo bi·ªÉu th·ª©c trong GROUPING SETS)
    if use_rollup or use_grouping_sets:
        # Pre-select trong CTE
        base_select_cols = [f"{grain_expr} AS {time_bucket_alias}"]
        base_select_cols.extend(dims)
        base_select_cols.extend(measures)
        
        base_sql = f"""
WITH base AS (
  SELECT
    {', '.join(base_select_cols)}
  FROM {from_}
  WHERE {where_sql}
)"""
        
        # GROUP BY d√πng c·ªôt (kh√¥ng ph·∫£i bi·ªÉu th·ª©c)
        if use_grouping_sets and dims:
            sets = []
            sets.append(f"({time_bucket_alias}, {', '.join(dims)})")
            for i in range(len(dims)-1, 0, -1):
                sets.append(f"({time_bucket_alias}, {', '.join(dims[:i])})")
            sets.append(f"({time_bucket_alias})")
            sets.append("()")
            group_sql = f"GROUP BY GROUPING SETS (\n  {', '.join(sets)}\n)"
        elif use_rollup and dims:
            group_sql = f"GROUP BY ROLLUP ({time_bucket_alias}, {', '.join(dims)})"
        else:
            group_sql = f"GROUP BY {time_bucket_alias}, {', '.join(dims)}"
        
        # SELECT t·ª´ base v·ªõi COALESCE cho NULL ‚Üí "ALL"
        select_cols = [f"{time_bucket_alias} AS {grain}"]
        for dim in dims:
            select_cols.append(f"COALESCE(CAST({dim} AS VARCHAR), 'ALL') AS {dim}")
        select_cols.extend(measures)
        
        order_positions = [str(i+1) for i in range(1 + len(dims))]
        order_sql = "ORDER BY " + ", ".join([f"{p} NULLS LAST" for p in order_positions])
        limit_sql = f"LIMIT {limit}" if limit else ""
        
        sql = f"""{base_sql}
SELECT
  {', '.join(select_cols)}
FROM base
{group_sql}
{order_sql}
{limit_sql}
        """.strip()
    else:
        # Kh√¥ng d√πng ROLLUP/GROUPING SETS ‚Üí SQL ƒë∆°n gi·∫£n
        select_cols = [f"{grain_expr} AS {grain}"]
        select_cols.extend(dims)
        select_cols.extend(measures)
        
        group_cols = [grain_expr]
        group_cols.extend(dims)
        group_sql = f"GROUP BY {', '.join(group_cols)}"
        
        order_positions = [str(i+1) for i in range(1 + len(dims))]
        order_sql = "ORDER BY " + ", ".join([f"{p} NULLS LAST" for p in order_positions])
        limit_sql = f"LIMIT {limit}" if limit else ""
        
        sql = f"""
SELECT
  {', '.join(select_cols)}
FROM {from_}
WHERE {where_sql}
{group_sql}
{order_sql}
{limit_sql}
        """.strip()
    
    return sql

# ====== TABLE & COLUMN METADATA ======
TABLES_META = {
    "gold": {
        "fact_order": {
            "date_col": "full_date",
            "description": "Fact table: 1 row per order",
            "dimensions": [
                "customer_id",
                "primary_payment_type",
                "is_canceled",
                "delivered_on_time"
            ],
            "measures": [
                "COUNT(*) AS order_count",
                "COUNT(DISTINCT customer_id) AS unique_customers",
                "SUM(items_count) AS total_items",
                "SUM(sum_price) AS total_price",
                "SUM(sum_freight) AS total_freight",
                "SUM(payment_total) AS total_payment",
                "AVG(delivered_days) AS avg_delivery_days",
                "SUM(CASE WHEN delivered_on_time THEN 1 ELSE 0 END) AS on_time_deliveries",
                "SUM(CASE WHEN is_canceled THEN 1 ELSE 0 END) AS canceled_orders"
            ]
        },
        "fact_order_item": {
            "date_col": "full_date",
            "description": "Fact table: 1 row per order item",
            "dimensions": [
                "product_id",
                "seller_id",
                "customer_id",
                "order_status"
            ],
            "measures": [
                "COUNT(*) AS item_count",
                "COUNT(DISTINCT order_id) AS order_count",
                "COUNT(DISTINCT product_id) AS unique_products",
                "COUNT(DISTINCT seller_id) AS unique_sellers",
                "SUM(price) AS total_revenue",
                "SUM(freight_value) AS total_freight",
                "AVG(price) AS avg_item_price"
            ]
        }
    },
    "platinum": {
        "dm_sales_monthly_category": {
            "date_col": "year_month",
            "description": "Monthly sales by product category",
            "dimensions": [
                "product_category_name_english"
            ],
            "measures": [
                "SUM(gmv) AS gmv",
                "SUM(orders) AS orders",
                "SUM(units) AS units",
                "AVG(aov) AS avg_aov"
            ]
        },
        "dm_customer_lifecycle": {
            "date_col": "year_month",
            "description": "Customer lifecycle analysis",
            "dimensions": [
                "cohort_month"
            ],
            "measures": [
                "SUM(orders) AS orders",
                "SUM(gmv) AS gmv",
                "COUNT(DISTINCT customer_id) AS unique_customers"
            ]
        },
        "dm_payment_mix": {
            "date_col": "year_month",
            "description": "Payment method mix by month",
            "dimensions": [
                "payment_type"
            ],
            "measures": [
                "SUM(orders) AS orders",
                "SUM(unique_customers) AS unique_customers",
                "SUM(payment_total) AS payment_total"
            ]
        },
        "dm_logistics_sla": {
            "date_col": "year_month",
            "description": "Logistics SLA metrics by region",
            "dimensions": [
                "geolocation_state"
            ],
            "measures": [
                "AVG(avg_delivered_days) AS avg_delivered_days",
                "AVG(on_time_rate) AS on_time_rate",
                "SUM(late_orders) AS late_orders"
            ]
        },
        "demand_forecast": {
            "date_col": "forecast_date",
            "description": "Demand forecast (ML predictions)",
            "dimensions": [
                "product_id",
                "region_id",
                "model_name"
            ],
            "measures": [
                "AVG(yhat) AS forecast",
                "AVG(yhat_lo) AS ci_lower",
                "AVG(yhat_hi) AS ci_upper",
                "COUNT(DISTINCT horizon) AS horizons"
            ]
        }
    }
}

# Enhanced CSS
st.markdown("""
<style>
:root{
  --bg:#0b1220; --bg2:#10192b; --card:#0f172a; --line:#1f2a44;
  --text:#e2e8f0; --muted:#94a3b8; --ok:#22c55e; --warn:#f59e0b; --err:#ef4444; --pri:#22d3ee;
}
.main .block-container{max-width:1280px;padding-top:1rem;padding-bottom:3rem}
h1,h2,h3{letter-spacing:.2px}
hr{border-color:var(--line);opacity:.4;margin:1rem 0}
.card{background:var(--card);border:1px solid var(--line);border-radius:16px;padding:16px;transition:border-color 0.2s}
.card:hover{border-color:#294166}
.badge{display:inline-flex;gap:6px;align-items:center;padding:4px 10px;border-radius:999px;
  background:rgba(34,211,238,.12);border:1px solid rgba(34,211,238,.25);font-size:12px}
.muted{color:var(--muted)}
.stButton>button{border-radius:14px;padding:.8rem 1.1rem;font-weight:700;font-size:15px}
.section-title{display:flex;align-items:center;gap:10px;margin:1.5rem 0 1rem}
.section-title h3{margin:0}
.section-title:after{content:"";flex:1;height:1px;background:linear-gradient(90deg,transparent, var(--line))}
</style>
""", unsafe_allow_html=True)

# ====== UI ======
st.title("üßÆ C·ª≠a s·ªï truy v·∫•n ƒëa chi·ªÅu")
st.caption("Ph√¢n t√≠ch d·ªØ li·ªáu Brazilian E-commerce v·ªõi Trino ‚Ä¢ OLAP ROLLUP/GROUPING SETS")

# ====== Mode Selection ======
mode = st.sidebar.radio("üîÄ Ch·∫ø ƒë·ªô", ["Tr√¨nh d·ª±ng (GUI)", "SQL th·ªß c√¥ng"], index=0)

# ====== Manual SQL Mode ======
if mode == "SQL th·ªß c√¥ng":
    st.subheader("üß© SQL th·ªß c√¥ng (SELECT-only)")
    st.caption("Ch·ªâ cho ph√©p SELECT/WITH tr√™n lakehouse.gold|platinum. C√≥ th·ªÉ d√πng :start, :end, :month.")
    
    # Sample SQL template
    sample_sql = """WITH base AS (
  SELECT *
  FROM lakehouse.gold.fact_order
  WHERE CAST(full_date AS date) >= DATE :start
    AND CAST(full_date AS date) <  DATE :end
)
SELECT 
  date_trunc('month', full_date) AS month,
  primary_payment_type,
  SUM(payment_total) AS total_payment,
  COUNT(*) AS order_count
FROM base
GROUP BY 1, 2
ORDER BY 1, 3 DESC
LIMIT 100
"""
    
    sql_input = st.text_area(
        "üìù SQL Query",
        sample_sql,
        height=260,
        key="custom_sql",
        help="Nh·∫≠p c√¢u l·ªánh SQL. S·ª≠ d·ª•ng :start, :end, :month l√†m placeholder cho tham s·ªë."
    )
    
    # Quick parameters
    st.markdown("**‚öôÔ∏è Tham s·ªë nhanh**")
    param_col1, param_col2, param_col3 = st.columns(3)
    
    with param_col1:
        d_start = st.date_input(
            "üìÖ Start",
            value=COVER_MIN,
            min_value=COVER_MIN,
            max_value=COVER_MAX,
            help="Ng√†y b·∫Øt ƒë·∫ßu (d√πng cho :start)"
        )
    
    with param_col2:
        d_end = st.date_input(
            "üìÖ End (exclusive)",
            value=COVER_MAX,
            min_value=COVER_MIN,
            max_value=COVER_MAX,
            help="Ng√†y k·∫øt th√∫c (exclusive, d√πng cho :end)"
        )
    
    with param_col3:
        # Default to current month or last month of data
        default_month = COVER_MAX.strftime("%Y-%m")
        month_input = st.text_input(
            "üìÜ Month (YYYY-MM)",
            value=default_month,
            help="Th√°ng (d√πng cho :month)"
        )
    
    # Options
    auto_limit = st.checkbox(
        "‚úÖ T·ª± ƒë·ªông th√™m LIMIT 10000 n·∫øu thi·∫øu",
        value=True,
        help="T·ª± ƒë·ªông th√™m LIMIT 10000 v√†o cu·ªëi c√¢u l·ªánh SQL n·∫øu ch∆∞a c√≥"
    )
    
    show_explain = st.checkbox(
        "üîç Hi·ªán k·∫ø ho·∫°ch (EXPLAIN) tr∆∞·ªõc khi ch·∫°y",
        value=False,
        help="Hi·ªÉn th·ªã execution plan tr∆∞·ªõc khi ch·∫°y truy v·∫•n"
    )
    
    # ====== Safety Checks (C·∫£i thi·ªán: b·ªè string literal, word boundary) ======
    sql = sql_input.strip()
    
    # Ki·ªÉm tra multi-statement
    is_single, multi_error = check_multi_statement_safe(sql)
    if not is_single:
        st.error(f"‚ùå {multi_error}")
        st.stop()
    
    # Ki·ªÉm tra an to√†n v·ªõi is_safe_select (ƒë√£ b·ªè string literal)
    if not is_safe_select(sql):
        st.error("‚ùå SQL kh√¥ng an to√†n. Ch·ªâ cho ph√©p SELECT/WITH, kh√¥ng c√≥ DDL/DML.")
        st.stop()
    
    # Require valid catalog/schema (warning only, not blocking)
    schema_match = re.search(r"\blakehouse\.(gold|platinum)\.", sql, re.IGNORECASE)
    if not schema_match:
        st.warning("‚ö†Ô∏è H√£y truy v·∫•n trong lakehouse.gold ho·∫∑c lakehouse.platinum (v√≠ d·ª•: lakehouse.gold.fact_order).")
        # Don't stop, just warn - user might be using subqueries or views
    else:
        # Extract schema from match for later use
        detected_schema = schema_match.group(1).lower()
    
    # Check if placeholders exist in original SQL
    has_placeholders = ":start" in sql_input or ":end" in sql_input or ":month" in sql_input
    
    # Replace placeholders safely (only if they exist in SQL)
    if ":start" in sql:
        sql = sql.replace(":start", f"'{d_start.isoformat()}'")
    if ":end" in sql:
        sql = sql.replace(":end", f"'{d_end.isoformat()}'")
    if ":month" in sql:
        sql = sql.replace(":month", f"'{month_input}'")
    
    # C·∫£i thi·ªán: Auto-LIMIT th√¥ng minh (kh√¥ng th√™m n·∫øu ƒë√£ c√≥ LIMIT/OFFSET/FETCH)
    sql_before = sql
    sql = maybe_add_limit(sql, default_limit=10000, enabled=auto_limit)
    limit_added = (sql != sql_before)
    
    # Display final SQL (always show if there were changes)
    if limit_added or has_placeholders:
        st.markdown("**üìã SQL ƒë√£ x·ª≠ l√Ω:**")
        st.code(sql, language="sql")
        if limit_added:
            st.info("‚ÑπÔ∏è ƒê√£ t·ª± ƒë·ªông th√™m LIMIT 10000 v√†o c√¢u l·ªánh SQL.")
    else:
        # Show a collapsible section for SQL preview
        with st.expander("üìã Xem SQL ƒë√£ x·ª≠ l√Ω", expanded=False):
            st.code(sql, language="sql")
    
    # C·∫£i thi·ªán: EXPLAIN ANALYZE ‚Üí EXPLAIN (an to√†n h∆°n)
    sql, explain_changed = check_explain_analyze(sql)
    if explain_changed:
        st.warning("‚ö†Ô∏è ƒê√£ chuy·ªÉn EXPLAIN ANALYZE ‚Üí EXPLAIN ƒë·ªÉ an to√†n (kh√¥ng th·ª±c thi query).")
    
    # EXPLAIN button (optional)
    if show_explain:
        if st.button("üîç EXPLAIN", use_container_width=True):
            try:
                explain_sql = f"EXPLAIN {sql}"
                with st.spinner("‚è≥ ƒêang ph√¢n t√≠ch execution plan..."):
                    df_explain = run_query(explain_sql, DEFAULT_SCHEMA)
                
                if not df_explain.empty:
                    st.success("‚úÖ Execution Plan:")
                    st.dataframe(df_explain, use_container_width=True, height=280)
                else:
                    st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y execution plan.")
            except Exception as e:
                st.error(f"‚ùå L·ªói EXPLAIN: {e}")
    
    # C·∫£i thi·ªán: C·∫£nh b√°o LIMIT kh√¥ng k√®m ORDER BY
    has_limit, has_order_by = check_order_by_with_limit(sql)
    if has_limit and not has_order_by:
        st.warning("‚ö†Ô∏è B·∫°n ƒëang gi·ªõi h·∫°n s·ªë d√≤ng nh∆∞ng kh√¥ng s·∫Øp x·∫øp; th·ª© t·ª± c√≥ th·ªÉ kh√¥ng ·ªïn ƒë·ªãnh.")
    
    # C·∫£i thi·ªán: Auto-fix CAST tr√™n partition (opt-in)
    auto_fix_cast = st.checkbox(
        "üîß T·ª± ƒë·ªông s·ª≠a CAST tr√™n partition (th√™m dual predicates)",
        value=False,
        help="T·ª± ƒë·ªông th√™m partition predicate ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng"
    )
    
    if auto_fix_cast:
        sql, was_fixed = auto_fix_cast_on_partition(
            sql, partition_col="year_month", date_col="full_date",
            start=d_start, end=d_end
        )
        if was_fixed:
            st.success("‚úÖ ƒê√£ t·ª± ƒë·ªông th√™m partition predicate ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng.")
    elif detect_cast_on_partition(sql):
        st.info("üí° G·ª£i √Ω: B·∫°n ƒëang CAST tr√™n c·ªôt partition; b·∫≠t checkbox tr√™n ƒë·ªÉ t·ª± ƒë·ªông th√™m dual predicates.")
    
    # Run SQL button
    if st.button("‚ñ∂Ô∏è Run SQL", type="primary", use_container_width=True):
        try:
            with st.spinner("‚è≥ ƒêang ch·∫°y truy v·∫•n..."):
                # Determine schema from SQL (default to gold)
                # Check for platinum first, then gold, then default to gold
                if re.search(r"\blakehouse\.platinum\.", sql, re.IGNORECASE):
                    query_schema = "platinum"
                elif re.search(r"\blakehouse\.gold\.", sql, re.IGNORECASE):
                    query_schema = "gold"
                else:
                    # If no schema detected, default to gold but show warning
                    query_schema = "gold"
                    st.warning("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán schema trong SQL. ƒêang d√πng schema m·∫∑c ƒë·ªãnh: gold")
                
                df = run_query(sql, query_schema)
            
            if df.empty:
                st.warning("üì≠ Kh√¥ng c√≥ d·ªØ li·ªáu tr·∫£ v·ªÅ.")
                st.stop()
            
            # L∆∞u SQL th√†nh c√¥ng v√†o session state (UX improvement)
            st.session_state['last_success_sql'] = sql
            st.session_state['last_success_params'] = {
                'start': d_start.isoformat(),
                'end': d_end.isoformat(),
                'month': month_input
            }
            
            # Display results
            st.success(f"‚úÖ Tr·∫£ v·ªÅ {len(df):,} d√≤ng")
            st.subheader("üìä K·∫øt qu·∫£")
            
            # Copy SQL button (UX improvement)
            if 'last_success_sql' in st.session_state:
                st.code(st.session_state['last_success_sql'], language="sql")
                if st.button("üìã Copy SQL", key="copy_sql_manual"):
                    st.success("‚úÖ SQL ƒë√£ ƒë∆∞·ª£c copy v√†o clipboard (d√°n v√†o editor ƒë·ªÉ d√πng l·∫°i)")
            
            # Display dataframe
            st.dataframe(df, use_container_width=True, height=500)
            
            # Summary statistics
            with st.expander("üìà Th·ªëng k√™ t·ªïng h·ª£p"):
                st.write("**T·ªïng s·ªë d√≤ng:**", f"{len(df):,}")
                st.write("**S·ªë c·ªôt:**", len(df.columns))
                st.write("**C√°c c·ªôt:**", ", ".join(df.columns))
                
                # Show data types
                st.write("**Ki·ªÉu d·ªØ li·ªáu:**")
                for col in df.columns:
                    dtype = str(df[col].dtype)
                    st.write(f"  - `{col}`: {dtype}")
            
            # Export options
            st.subheader("üíæ Xu·∫•t d·ªØ li·ªáu")
            export_col1, export_col2 = st.columns(2)
            
            with export_col1:
                # CSV export
                csv = df.to_csv(index=False).encode("utf-8")
                st.download_button(
                    "‚¨áÔ∏è T·∫£i CSV",
                    csv,
                    f"custom_sql_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    "text/csv",
                    use_container_width=True
                )
            
            with export_col2:
                # Excel export
                try:
                    import io
                    bio = io.BytesIO()
                    with pd.ExcelWriter(bio, engine='openpyxl') as writer:
                        df.to_excel(writer, index=False, sheet_name='Query Result')
                    st.download_button(
                        "‚¨áÔ∏è T·∫£i Excel",
                        bio.getvalue(),
                        f"custom_sql_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"‚ùå L·ªói export Excel: {e}")
        
        except Exception as e:
            st.error(f"‚ùå L·ªói truy v·∫•n: {e}")
            st.code(sql, language="sql")
            # Show helpful error message
            error_str = str(e)
            if "TABLE_NOT_FOUND" in error_str or "does not exist" in error_str:
                st.info("üí° G·ª£i √Ω: Ki·ªÉm tra t√™n b·∫£ng v√† schema. V√≠ d·ª•: `lakehouse.gold.fact_order`")
            elif "SYNTAX_ERROR" in error_str or "syntax" in error_str.lower():
                st.info("üí° G·ª£i √Ω: Ki·ªÉm tra c√∫ ph√°p SQL. ƒê·∫£m b·∫£o d√πng c√∫ ph√°p Trino/Presto SQL.")
    
    # Help section for manual SQL
    with st.expander("‚ÑπÔ∏è H∆∞·ªõng d·∫´n SQL th·ªß c√¥ng", expanded=False):
        st.markdown("""
        ### üìù C√°ch s·ª≠ d·ª•ng:
        
        1. **Nh·∫≠p SQL**: G√µ c√¢u l·ªánh SELECT/WITH trong khung text area
        2. **Tham s·ªë**: S·ª≠ d·ª•ng `:start`, `:end`, `:month` l√†m placeholder
        3. **Ch·∫°y**: Nh·∫•n n√∫t "Run SQL" ƒë·ªÉ th·ª±c thi
        
        ### üîí R√†o ch·∫Øn an to√†n:
        - ‚úÖ Ch·ªâ cho ph√©p SELECT/WITH
        - ‚úÖ Ch·∫∑n DDL/DML (DROP, INSERT, UPDATE, DELETE, etc.)
        - ‚úÖ B·∫Øt bu·ªôc tham chi·∫øu trong `lakehouse.gold` ho·∫∑c `lakehouse.platinum`
        - ‚úÖ T·ª± ƒë·ªông th√™m LIMIT 10000 n·∫øu thi·∫øu
        
        ### üí° V√≠ d·ª• SQL:
        
        **V√≠ d·ª• 1: Truy v·∫•n ƒë∆°n gi·∫£n**
        ```sql
        SELECT *
        FROM lakehouse.gold.fact_order
        WHERE CAST(full_date AS date) >= DATE :start
          AND CAST(full_date AS date) <  DATE :end
        LIMIT 100
        ```
        
        **V√≠ d·ª• 2: V·ªõi CTE (WITH)**
        ```sql
        WITH monthly_sales AS (
          SELECT 
            date_trunc('month', full_date) AS month,
            SUM(payment_total) AS total
          FROM lakehouse.gold.fact_order
          WHERE CAST(full_date AS date) >= DATE :start
            AND CAST(full_date AS date) <  DATE :end
          GROUP BY 1
        )
        SELECT * FROM monthly_sales ORDER BY month
        ```
        
        **V√≠ d·ª• 3: JOIN v·ªõi dimension tables**
        ```sql
        SELECT 
          o.full_date,
          p.product_category_name_english AS category,
          SUM(oi.price) AS revenue
        FROM lakehouse.gold.fact_order_item oi
        JOIN lakehouse.gold.fact_order o ON oi.order_id = o.order_id
        JOIN lakehouse.gold.dim_product p ON oi.product_id = p.product_id
        WHERE CAST(o.full_date AS date) >= DATE :start
          AND CAST(o.full_date AS date) <  DATE :end
        GROUP BY 1, 2
        ORDER BY 1, 3 DESC
        ```
        
        ### ‚ö†Ô∏è L∆∞u √Ω:
        - **Half-open interval**: D√πng `>= :start AND < :end` ƒë·ªÉ tr√°nh l·ªói bi√™n
        - **Year-month columns**: N·∫øu d√πng `year_month` (VARCHAR), parse sang DATE:
          ```sql
          WHERE date_parse(year_month || '-01', '%Y-%m-%d') >= DATE :start
            AND date_parse(year_month || '-01', '%Y-%m-%d') <  DATE :end
          ```
        - **Performance**: Gi·ªõi h·∫°n s·ªë d√≤ng v·ªõi LIMIT ƒë·ªÉ tr√°nh query qu√° n·∫∑ng
        - **EXPLAIN**: D√πng checkbox "Hi·ªán k·∫ø ho·∫°ch" ƒë·ªÉ xem execution plan tr∆∞·ªõc khi ch·∫°y
        """)
    
    st.stop()  # Stop here, don't run GUI mode below

# ====== GUI Mode (Original Query Builder) ======
# ====== Schema & Table Selection ======
col_meta1, col_meta2 = st.columns(2)

with col_meta1:
    schemas = list(TABLES_META.keys())
    schema = st.selectbox("üìÅ Schema", schemas, index=schemas.index(DEFAULT_SCHEMA))

with col_meta2:
    tables = list(TABLES_META[schema].keys())
    fact_table = st.selectbox("üìä Fact Table", tables)

table_meta = TABLES_META[schema][fact_table]
st.info(f"‚ÑπÔ∏è {table_meta['description']}")

# ====== Time Selection ======
st.subheader("‚è∞ Th·ªùi gian")

# Helper functions for date manipulation
def first_of_month(d: date):
    """Get first day of month"""
    return d.replace(day=1)

def subtract_months(d: date, months: int):
    """Subtract months from a date safely"""
    year = d.year
    month = d.month - months
    while month <= 0:
        month += 12
        year -= 1
    # Handle day overflow (e.g., Jan 31 - 1 month = Dec 31, not Dec 30)
    day = min(d.day, [31, 29 if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31][month - 1])
    return date(year, month, day)

# Calculate default dates based on coverage
time_grain = st.selectbox("üîç Time Grain", ["day","week","month","quarter","year"], index=2)

# Smart defaults based on grain and data coverage
if time_grain == "month":
    # For month: last 6 months
    default_end = COVER_MAX
    # Calculate start as 6 months before
    default_start = subtract_months(default_end, 6)
    default_start = first_of_month(default_start)
else:
    # For day: last 90 days
    default_start = COVER_MAX - timedelta(days=89)
    default_end = COVER_MAX

col_time2, col_time3 = st.columns(2)

with col_time2:
    start_date = st.date_input(
        "üìÖ T·ª´ ng√†y", 
        value=default_start,
        min_value=COVER_MIN,
        max_value=COVER_MAX
    )

with col_time3:
    end_date = st.date_input(
        "üìÖ ƒê·∫øn ng√†y", 
        value=default_end,
        min_value=COVER_MIN,
        max_value=COVER_MAX
    )

# Validate date range
if start_date > end_date:
    st.warning("‚ö†Ô∏è Kho·∫£ng th·ªùi gian kh√¥ng h·ª£p l·ªá. ƒê√£ t·ª± ch·ªânh v·ªÅ bi√™n h·ª£p l·ªá g·∫ßn nh·∫•t.")
    start_date = COVER_MIN
    end_date = COVER_MAX

# Display data coverage info v·ªõi badge c·∫£nh b√°o fallback (UX improvement)
if USING_FALLBACK_DATE:
    col_warn1, col_warn2 = st.columns([3, 1])
    with col_warn1:
        st.warning("‚ö†Ô∏è **ƒêang d√πng d·∫£i ng√†y m·∫∑c ƒë·ªãnh** do kh√¥ng truy c·∫≠p ƒë∆∞·ª£c metadata. H√£y ki·ªÉm tra k·∫øt n·ªëi/b·∫£ng ngu·ªìn.")
    with col_warn2:
        if st.button("üîÑ Retry v·ªõi safe defaults", key="retry_coverage"):
            st.cache_data.clear()
            st.rerun()
    if '_coverage_error' in st.session_state:
        with st.expander("üîç Chi ti·∫øt l·ªói"):
            st.code(st.session_state['_coverage_error'], language=None)
st.caption(f"üìå D·∫£i d·ªØ li·ªáu kh·∫£ d·ª•ng: {COVER_MIN} ‚Üí {COVER_MAX} (Brazilian E-commerce/Olist)")

# ====== Dimensions Selection ======
st.subheader("üìê Dimensions (Chi·ªÅu ph√¢n t√≠ch)")
available_dims = table_meta["dimensions"]
dims = st.multiselect(
    "Ch·ªçn c√°c chi·ªÅu ƒë·ªÉ ph√¢n t√≠ch (t·ªëi ƒëa 3-4 chi·ªÅu cho hi·ªáu su·∫•t t·ªët)",
    available_dims,
    default=available_dims[:2] if len(available_dims) >= 2 else available_dims
)

# ====== Measures Selection ======
st.subheader("üìè Measures (Ch·ªâ s·ªë)")
available_measures = table_meta["measures"]
measures = st.multiselect(
    "Ch·ªçn c√°c ch·ªâ s·ªë c·∫ßn t√≠nh to√°n",
    available_measures,
    default=available_measures[:3] if len(available_measures) >= 3 else available_measures
)

# ====== Filters ======
st.subheader("üîç B·ªô l·ªçc")
extra_filters = st.text_area(
    "WHERE clause b·ªï sung (v√≠ d·ª•: primary_payment_type = 'credit_card' AND delivered_on_time = true)",
    "",
    height=80
)

# ====== Advanced Options ======
with st.expander("‚öôÔ∏è T√πy ch·ªçn n√¢ng cao"):
    col_adv1, col_adv2, col_adv3 = st.columns(3)
    
    with col_adv1:
        use_rollup = st.checkbox("D√πng ROLLUP (t·ªïng theo m·ªçi c·∫•p)", value=False)
    
    with col_adv2:
        use_gsets = st.checkbox("D√πng GROUPING SETS", value=False)
    
    with col_adv3:
        limit_rows = st.number_input("Gi·ªõi h·∫°n s·ªë d√≤ng", min_value=0, max_value=100000, value=10000, step=1000)

# ====== Run Query ======
if st.button("‚ñ∂Ô∏è Ch·∫°y truy v·∫•n", type="primary", use_container_width=True):
    if not dims:
        st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 dimension")
        st.stop()
    
    if not measures:
        st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 measure")
        st.stop()
    
    # Build SQL
    sql = build_sql(
        TRINO_CATALOG, schema, fact_table, table_meta["date_col"], time_grain,
        dims, measures, start_date, end_date, extra_filters,
        use_rollup, use_gsets, limit_rows if limit_rows > 0 else None
    )
    
    # Show SQL
    with st.expander("üìù SQL Query"):
        st.code(sql, language="sql")
    
    # Execute query
    try:
        with st.spinner("‚è≥ ƒêang truy v·∫•n d·ªØ li·ªáu..."):
            df = run_query(sql, schema)
        
        if df.empty:
            st.warning("üì≠ Kh√¥ng c√≥ d·ªØ li·ªáu ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán")
            st.stop()
        
        # L∆∞u SQL th√†nh c√¥ng v√†o session state (UX improvement)
        st.session_state['last_success_sql'] = sql
        st.session_state['last_success_params'] = {
            'schema': schema,
            'table': fact_table,
            'grain': time_grain,
            'start': start_date.isoformat(),
            'end': end_date.isoformat()
        }
        
        # Display results
        st.success(f"‚úÖ Tr·∫£ v·ªÅ {len(df):,} d√≤ng")
        st.subheader("üìä K·∫øt qu·∫£")
        
        # Copy SQL button (UX improvement)
        if 'last_success_sql' in st.session_state:
            with st.expander("üìã SQL ƒë√£ ch·∫°y (click ƒë·ªÉ copy)", expanded=False):
                st.code(st.session_state['last_success_sql'], language="sql")
        
        # T·∫°o b·∫£n hi·ªÉn th·ªã ri√™ng (gi·ªØ df g·ªëc ƒë·ªÉ export chu·∫©n s·ªë)
        df_display = df.copy()
        numeric_cols = df_display.select_dtypes(include=['int64', 'float64']).columns
        for col in numeric_cols:
            df_display[col] = df_display[col].apply(lambda x: f"{x:,.2f}" if pd.notna(x) else "")
        
        st.dataframe(df_display, use_container_width=True, height=500)
        
        # Summary statistics
        with st.expander("üìà Th·ªëng k√™ t·ªïng h·ª£p"):
            st.write("T·ªïng s·ªë d√≤ng:", f"{len(df):,}")
            st.write("C√°c c·ªôt:", ", ".join(df.columns))
        
        # Export options
        st.subheader("üíæ Xu·∫•t d·ªØ li·ªáu")
        col_exp1, col_exp2 = st.columns(2)
        
        with col_exp1:
            # CSV export
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button(
                "‚¨áÔ∏è T·∫£i CSV",
                csv,
                f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv",
                use_container_width=True
            )
        
        with col_exp2:
            # Excel export
            try:
                import io
                bio = io.BytesIO()
                with pd.ExcelWriter(bio, engine='openpyxl') as writer:
                    df.to_excel(writer, index=False, sheet_name='Query Result')
                st.download_button(
                    "‚¨áÔ∏è T·∫£i Excel",
                    bio.getvalue(),
                    f"query_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
            except Exception as e:
                st.error(f"L·ªói export Excel: {e}")
        
    except Exception as e:
        st.error(f"‚ùå L·ªói truy v·∫•n: {e}")
        st.code(sql, language="sql")

# ====== Help Section ======
with st.sidebar:
    st.header("‚ÑπÔ∏è H∆∞·ªõng d·∫´n")
    
    st.markdown("""
    ### C√°ch s·ª≠ d·ª•ng:
    
    1. **Ch·ªçn Schema & Table**: Ch·ªçn l·ªõp d·ªØ li·ªáu v√† b·∫£ng fact
    2. **Ch·ªçn Time Grain**: ƒê·ªô chi ti·∫øt th·ªùi gian (ng√†y/tu·∫ßn/th√°ng...)
    3. **Ch·ªçn Dimensions**: C√°c chi·ªÅu ph√¢n t√≠ch (t·ªëi ƒëa 3-4)
    4. **Ch·ªçn Measures**: C√°c ch·ªâ s·ªë c·∫ßn t√≠nh
    5. **Th√™m Filters**: ƒêi·ªÅu ki·ªán l·ªçc b·ªï sung (t√πy ch·ªçn)
    6. **Ch·∫°y**: Nh·∫•n n√∫t "Ch·∫°y truy v·∫•n"
    
    ### V√≠ d·ª• Filters:
    ```sql
    primary_payment_type = 'credit_card'
    delivered_on_time = true
    sum_price > 100
    ```
    
    ### Tips:
    - Gi·ªõi h·∫°n kho·∫£ng th·ªùi gian ƒë·ªÉ truy v·∫•n nhanh h∆°n
    - Ch·ªçn √≠t dimensions ƒë·ªÉ tr√°nh qu√° nhi·ªÅu d√≤ng
    - D√πng ROLLUP ƒë·ªÉ xem t·ªïng theo t·ª´ng c·∫•p
    """)
    
    st.divider()
    
    st.markdown(f"""
    **Trino Connection:**
    - Host: `{TRINO_HOST}:{TRINO_PORT}`
    - Catalog: `{TRINO_CATALOG}`
    - Schema: `{schema}`
    """)

