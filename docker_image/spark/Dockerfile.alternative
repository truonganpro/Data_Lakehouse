# Alternative Dockerfile cho Spark - không phụ thuộc vào bitnami/spark:3.3.2
# Sử dụng Apache Spark official image

FROM apache/spark-py:v3.3.2

USER root

# Install prerequisites
RUN apt-get update && \
    apt-get install -y curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Tạo thư mục jars nếu chưa có
RUN mkdir -p /opt/spark/jars

# Copy danh sách JARs vào container (nếu có)
COPY requirements-jars.txt /tmp/requirements-jars.txt 2>/dev/null || echo "# No jars" > /tmp/requirements-jars.txt

# Download JARs nếu có trong requirements-jars.txt
RUN if [ -f /tmp/requirements-jars.txt ] && [ -s /tmp/requirements-jars.txt ]; then \
    set -ex && \
    while read url; do \
      if [ ! -z "$url" ] && [[ ! "$url" =~ ^# ]]; then \
        echo "Downloading $url" && \
        curl -fSL "$url" -o /opt/spark/jars/$(basename "$url") || echo "Failed to download $url"; \
      fi; \
    done < /tmp/requirements-jars.txt; \
    fi

# Copy config files
COPY conf/spark-defaults.conf /opt/spark/conf/spark-defaults.conf
COPY conf/log4j.properties /opt/spark/conf/log4j.properties
COPY conf/hive-site.xml /opt/spark/conf/hive-site.xml 2>/dev/null || true

# Set environment variables tương thích với bitnami/spark
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV SPARK_CLASSPATH=/opt/jars/*:$SPARK_CLASSPATH
ENV SPARK_JARS_DIR=/opt/jars

# Expose ports
EXPOSE 8080 7077 8081

# Default command
CMD ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
