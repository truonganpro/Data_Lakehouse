# THÃ”NG TIN BÃO CÃO CHÆ¯Æ NG 6 - DATA LAKEHOUSE PROJECT
# Chá»©c nÄƒng há»‡ thá»‘ng & kiáº¿n thá»©c ká»¹ thuáº­t cáº§n cÃ³

================================================================================
ğŸ“Œ 6.1. Tá»”NG QUAN MODULE & VAI TRÃ’ NGÆ¯á»œI DÃ™NG
================================================================================

FILE: PROJECT_OVERVIEW.md (dÃ²ng 75-254)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ Há»† THá»NG BAO Gá»’M 5 MODULES CHÃNH:

1. ETL/ELT Pipeline (Dagster + Spark)
2. BI Dashboard (Metabase)
3. OLAP Query Window (Streamlit + Trino)
4. AI-powered Chat (NL â†’ SQL + RAG)
5. Demand Forecasting / Monitoring (ML)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 6.1.1. ETL/ELT MEDALLION (DAGSTER + SPARK)

Vai trÃ²: Data Engineer
Purpose: Tá»± Ä‘á»™ng hÃ³a data pipeline tá»« source â†’ datamarts

Technology:
   â€¢ Dagster (Orchestration): Job scheduling, monitoring, lineage
   â€¢ Apache Spark 3.3.2: Distributed processing
   â€¢ Delta Lake 2.3.0: ACID storage
   â€¢ Hive Metastore: Schema catalog
   â€¢ MySQL: Source data

Features:
   âœ… Incremental loading (tÄƒng dáº§n)
   âœ… Schema evolution (linh hoáº¡t schema)
   âœ… Data quality checks
   âœ… Lineage tracking (theo dÃµi nguá»“n gá»‘c)
   âœ… Job scheduling (láº­p lá»‹ch)
   âœ… Error handling & retry

Jobs:
   â€¢ reload_data: MySQL â†’ Bronze (9 tables)
   â€¢ full_pipeline_job: Bronze â†’ Silver â†’ Gold â†’ Platinum (36 assets)

Schedule:
   â€¢ Daily 00:00 (bronze reload)
   â€¢ Optional: Daily 03:00 (forecast)

Access: http://localhost:3001 (Dagster UI)

Performance:
   â€¢ Bronze: ~2 min (9 tables)
   â€¢ Silver: ~3 min (10 tables)
   â€¢ Gold: ~2 min (10 tables)
   â€¢ Platinum: ~1 min (7 tables)
   â€¢ Total: ~8-10 min

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 6.1.2. BI DASHBOARD (METABASE)

Vai trÃ²: Business Analyst, Executive
Purpose: Self-service analytics, dashboards, reports

Technology:
   â€¢ Metabase (Latest)
   â€¢ Trino connector
   â€¢ Visual query builder

Features:
   âœ… Drag-and-drop query builder
   âœ… Pre-built dashboards
   âœ… Custom SQL queries
   âœ… Scheduled reports
   âœ… Email alerts
   âœ… Drill-down capabilities

Sample Dashboards:
   1. Executive Summary: Total GMV, Orders, AOV, Revenue trend
   2. Sales Deep Dive: Category performance, Geographic distribution
   3. Customer Analytics: Lifecycle funnel, Segmentation, Cohort analysis
   4. Operations: Logistics SLA, Delivery metrics, Payment distribution
   5. Product Intelligence: Bestsellers, Price bands, Category analysis

Supported Schemas:
   â€¢ bronze: Raw data (9 tables)
   â€¢ silver: Cleaned data (10 tables)
   â€¢ gold: Star schema (10 tables)
   â€¢ platinum: Datamarts (7 tables + forecast)

Access: http://localhost:3000

Setup:
   1. Create admin account
   2. Add Database: Trino
   3. Host: trino, Port: 8080
   4. Database: lakehouse
   5. Test connection â†’ Save

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 6.1.3. OLAP QUERY WINDOW (TRINO)

Vai trÃ²: Data Analyst, Business Analyst
Purpose: Ad-hoc multi-dimensional analysis khÃ´ng cáº§n viáº¿t SQL

Technology:
   â€¢ Streamlit (UI Framework)
   â€¢ Trino 414 (SQL Engine)
   â€¢ Delta Lake (Data Source)

Features:

1. Multi-dimensional Analysis:
   â€¢ Time grain: day/week/month/quarter/year
   â€¢ Dimensions: product, customer, region, seller, category
   â€¢ Measures: revenue, orders, units, AOV, GMV

2. Advanced Aggregations:
   â€¢ ROLLUP: Hierarchical totals
   â€¢ GROUPING SETS: Custom aggregation levels
   â€¢ NULLS LAST: Subtotals at the end

3. Date Handling:
   â€¢ Dynamic date range detection
   â€¢ Olist coverage: 2016-09-04 â†’ 2018-10-17
   â€¢ Smart defaults: 6 months (month), 90 days (day)
   â€¢ Safe filtering vá»›i half-open intervals

4. Export:
   â€¢ CSV download (numeric preserved)
   â€¢ Excel download (proper types)
   â€¢ Formatted timestamps

5. Performance:
   â€¢ Query caching (10 min TTL)
   â€¢ Result limiting (configurable)
   â€¢ Efficient SQL generation

Supported Tables:
   â€¢ gold.fact_order (96K rows)
   â€¢ gold.fact_order_item (112K rows)
   â€¢ platinum.dm_sales_monthly_category (1.3K rows)
   â€¢ platinum.dm_customer_lifecycle (96K rows)
   â€¢ platinum.dm_payment_mix (90 rows)
   â€¢ platinum.dm_logistics_sla (574 rows)
   â€¢ platinum.demand_forecast (ML predictions)

Access: http://localhost:8501/ğŸ“Š_Query_Window

Example Workflow:
   1. Select schema + table
   2. Choose time grain (month)
   3. Add dimensions (product_category_name_english)
   4. Select measures (SUM(gmv) AS revenue, COUNT(*) AS orders)
   5. Set date range (last 6 months)
   6. Add filters (optional)
   7. Choose aggregation (ROLLUP/GROUPING SETS)
   8. Click "Cháº¡y truy váº¥n"
   9. Export CSV/Excel

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 6.1.4. AI-POWERED CHAT (NL â†’ SQL + RAG)

Vai trÃ²: Business Manager, Non-technical user
Purpose: Query data báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn

Technology:
   â€¢ Streamlit (Frontend)
   â€¢ FastAPI (Backend)
   â€¢ Gemini AI (LLM)
   â€¢ Qdrant (Vector DB)
   â€¢ Trino (SQL Execution)

Features:

1. Natural Language to SQL:
   â€¢ Intent recognition (Vietnamese & English)
   â€¢ Template-based SQL generation
   â€¢ LLM SQL generation (Gemini fallback)
   â€¢ Safe query execution

2. RAG (Retrieval-Augmented Generation):
   â€¢ Document embeddings trong Qdrant
   â€¢ KPI definitions retrieval
   â€¢ Context-aware responses
   â€¢ Citations & sources

3. Session Management:
   â€¢ Chat history per session
   â€¢ Context preservation
   â€¢ Query suggestions
   â€¢ Reset chat

4. Security:
   â€¢ SQL safety validation
   â€¢ Schema whitelisting (gold, platinum)
   â€¢ Read-only enforcement
   â€¢ LIMIT auto-append
   â€¢ Timeout (45s)

Example Queries:
   â€¢ "Doanh thu thÃ¡ng 3 nÄƒm 2018?"
   â€¢ "Top 10 sáº£n pháº©m bÃ¡n cháº¡y nháº¥t?"
   â€¢ "PhÆ°Æ¡ng thá»©c thanh toÃ¡n nÃ o phá»• biáº¿n nháº¥t?"
   â€¢ "Tá»· lá»‡ giao hÃ ng Ä‘Ãºng háº¡n?"

Response Format:
   1. Answer text (summarized)
   2. SQL query (generated)
   3. Data preview (50 rows)
   4. Citations (RAG sources)
   5. Execution time

Access: http://localhost:8501/ğŸ’¬_Chat

Limits:
   â€¢ Max rows: 5,000
   â€¢ Timeout: 45 seconds
   â€¢ Preview: 50 rows

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 6.1.5. DEMAND FORECASTING / MONITORING

Vai trÃ²: Data Scientist, Supply Chain Manager
Purpose: Dá»± bÃ¡o nhu cáº§u sáº£n pháº©m 28 ngÃ y vá»›i ML

Technology:
   â€¢ LightGBM (Gradient Boosting)
   â€¢ MLflow (Experiment Tracking)
   â€¢ PySpark (Feature Engineering)
   â€¢ Delta Lake (Feature Store)

Pipeline:
   1. Feature Engineering:
      gold.fact_order_item + gold.fact_order + gold.dim_customer
      â†’ silver.forecast_features
      Features: lag 1/7/28, rolling MA 7/28, calendar, price

   2. Model Training:
      silver.forecast_features
      â†’ LightGBM model
      â†’ MLflow tracking (RMSE, MAE, sMAPE, R2)

   3. Batch Prediction:
      Recursive roll-forward prediction (28 horizons)
      â†’ platinum.demand_forecast

   4. Monitoring:
      Compare actuals vs forecasts
      â†’ platinum.forecast_monitoring

Features:
   âœ… Recursive forecasting (28 days)
   âœ… Multi-product, multi-region
   âœ… Confidence intervals (yhat_lo, yhat_hi)
   âœ… Model versioning (MLflow)
   âœ… Automated retraining (optional)
   âœ… Performance monitoring

Metrics:
   â€¢ RMSE: Root Mean Square Error
   â€¢ MAE: Mean Absolute Error
   â€¢ sMAPE: Symmetric Mean Absolute Percentage Error
   â€¢ R2: Coefficient of Determination

Access: Dagster UI, Trino queries

Execution:
   ```bash
   # Full forecast pipeline
   make forecast-full
   
   # Or manual steps
   make forecast-init     # Initialize MLflow
   make forecast-features # Build features
   make forecast-train    # Train model
   make forecast-predict RUN_ID=<id> # Generate forecasts
   ```

Performance:
   â€¢ Features: ~2 min
   â€¢ Training: ~3-5 min
   â€¢ Prediction: ~1-2 min
   â€¢ Total: ~6-9 min

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ‘¥ USER ROLES & PERSONAS:

1. Data Engineer:
   â€¢ Primary: Dagster ETL pipeline
   â€¢ Secondary: Spark monitoring, troubleshooting
   â€¢ Use: ETL orchestration, data quality

2. Data Analyst:
   â€¢ Primary: Query Window, Metabase
   â€¢ Secondary: Trino SQL queries
   â€¢ Use: Ad-hoc analysis, dashboards

3. Business Analyst:
   â€¢ Primary: Metabase dashboards
   â€¢ Secondary: Query Window exports
   â€¢ Use: Regular reporting, insights

4. Business Manager:
   â€¢ Primary: Chat interface
   â€¢ Secondary: Metabase dashboards
   â€¢ Use: Quick answers, high-level metrics

5. Data Scientist:
   â€¢ Primary: Forecast pipeline
   â€¢ Secondary: MLflow experiments
   â€¢ Use: Model training, evaluation

6. Supply Chain Manager:
   â€¢ Primary: Forecast monitoring
   â€¢ Secondary: Logistics SLA datamart
   â€¢ Use: Inventory planning, optimization

7. Executive:
   â€¢ Primary: Metabase executive summary
   â€¢ Use: KPIs, trends, strategic decisions

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - PROJECT_OVERVIEW.md (dÃ²ng 75-254): All modules
   - etl_pipeline/: ETL implementation
   - app/pages/1_ğŸ“Š_Query_Window.py: OLAP UI
   - app/pages/2_ğŸ’¬_Chat.py: Chat UI
   - chat_service/main.py: Chat backend
   - docs/chat_service_guide.md: Chat guide


================================================================================
ğŸ“Œ 6.2. LUá»’NG INPUT â†’ PROCESS â†’ OUTPUT CHO Tá»ªNG MODULE
================================================================================

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 6.2.1. ETL PIPELINE

INPUT:
   â€¢ Source: MySQL (brazillian_ecommerce database)
   â€¢ Tables: customers, orders, order_items, products, sellers, payments, reviews, geolocation, categories
   â€¢ Format: Relational tables
   â€¢ Size: ~100K orders

PROCESS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   BRONZE     â”‚ â† MySQL JDBC read (9 tables)
   â”‚  (Raw)       â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   SILVER     â”‚ â† Cleaning, deduplication, casting (10 tables)
   â”‚  (Clean)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    GOLD      â”‚ â† Star schema transformation (10 tables)
   â”‚ (Structured) â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PLATINUM    â”‚ â† Business aggregations (7 datamarts)
   â”‚  (BI-Ready)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT:
   â€¢ Location: s3a://lakehouse/{layer}/{table}/
   â€¢ Format: Delta Lake (Parquet)
   â€¢ Catalog: Hive Metastore
   â€¢ Query: Trino

Lineage Example:
   MySQL.customers
      â†“ (JDBC)
   bronze.customer
      â†“ (deduplication, NULL handling)
   silver.customer
      â†“ (enrichment, city_state)
   gold.dimcustomer
      â†“ (JOINs, aggregations)
   platinum.dmsellerkpi, platinum.dmcustomerlifecycle

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 6.2.2. BI DASHBOARD (METABASE)

INPUT:
   â€¢ Data: Trino catalog (lakehouse)
   â€¢ Tables: gold.*, platinum.*
   â€¢ Connection: Trino connector

PROCESS:
   User Request (Visual/Query)
      â†“
   Query Builder / SQL Editor
      â†“
   Trino Execution
      â†“
   Result Processing

OUTPUT:
   â€¢ Charts: Line, Bar, Table
   â€¢ Dashboards: Multiple visualizations
   â€¢ Reports: Scheduled emails
   â€¢ Exports: CSV, Excel

Example Flow:
   User: "Show monthly revenue trend"
      â†“
   Metabase: Visual query builder
      â†“
   SQL: SELECT year_month, SUM(gmv) FROM platinum.dm_sales_monthly_category GROUP BY year_month
      â†“
   Trino: Executes on Delta Lake
      â†“
   Metabase: Displays line chart

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 6.2.3. OLAP QUERY WINDOW

INPUT:
   â€¢ User selections: Schema, table, dimensions, measures
   â€¢ Date range: start_date, end_date
   â€¢ Options: Rollup, grouping sets, filters

PROCESS:
   build_sql() Function:
      1. Detect date column type (VARCHAR vs DATE)
      2. Build time grain expression
      3. Build SELECT clause
      4. Build WHERE clause (date-safe + partition pruning)
      5. Build GROUP BY (ROLLUP/GROUPING SETS)
      6. Build ORDER BY (NULLS LAST)
      7. Add LIMIT

OUTPUT:
   â€¢ DataFrame: Query results
   â€¢ Display: Preview table
   â€¢ Export: CSV, Excel
   â€¢ Cache: 10 minutes TTL

Example Flow:
   User:
      â€¢ Table: platinum.dm_sales_monthly_category
      â€¢ Time grain: month
      â€¢ Dimensions: product_category_name_english
      â€¢ Measures: SUM(gmv), COUNT(orders)
      â€¢ Date: 2018-01 â†’ 2018-06
      â€¢ Aggregation: ROLLUP
      â†“
   
   Streamlit: build_sql()
      â†“
   
   SQL Generated:
   ```sql
   SELECT 
     date_trunc('month', CAST(date_parse(year_month || '-01', '%Y-%m-%d') AS date)) AS month,
     product_category_name_english,
     SUM(gmv) AS gmv,
     COUNT(orders) AS orders
   FROM lakehouse.platinum.dmsalesmonthlycategory
   WHERE year_month >= '2018-01' 
     AND year_month <= '2018-06'
     AND CAST(date_parse(year_month || '-01', '%Y-%m-%d') AS date) >= DATE '2018-01-01'
     AND CAST(date_parse(year_month || '-01', '%Y-%m-%d') AS date) < DATE '2018-07-01'
   GROUP BY ROLLUP (month, product_category_name_english)
   ORDER BY 1 NULLS LAST, 2 NULLS LAST
   ```
      â†“
   
   Trino: Execute
      â†“
   
   Results: Pivot table with subtotals

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 6.2.4. AI-POWERED CHAT

INPUT:
   â€¢ User question: Natural language (Vietnamese/English)
   â€¢ Session ID: UUID
   â€¢ Context: Chat history

PROCESS:
   Multi-Tier SQL Generation:
      1. Check HELP MODE (intent_to_sql)
      2. Try old templates (backward compatibility)
      3. Try router + skills (NEW)
      4. Fallback Gemini LLM

   SQL Safety:
      1. Read-only check (SELECT/WITH)
      2. Dangerous keyword detection
      3. Schema whitelisting
      4. LIMIT enforcement

   RAG Search:
      1. Embed query (Gemini)
      2. Search Qdrant (k=4)
      3. Retrieve relevant docs

   Execution:
      1. Run SQL on Trino
      2. Fetch results (max 5000 rows)
      3. Generate answer summary (Gemini)

OUTPUT:
   â€¢ Answer text: Summarized response
   â€¢ SQL: Generated query
   â€¢ Preview: First 50 rows
   â€¢ Citations: RAG sources
   â€¢ Execution time: ms

Example Flow:
   User: "Doanh thu thÃ¡ng 3 nÄƒm 2018?"
      â†“
   
   Router: Intent = revenue_query
      â†“
   
   SQL Template:
   ```sql
   SELECT 
     year_month,
     SUM(gmv) AS revenue
   FROM platinum.dm_sales_monthly_category
   WHERE year_month = '2018-03'
   GROUP BY year_month
   LIMIT 200
   ```
      â†“
   
   Safety Check: âœ… Pass
      â†“
   
   Trino: Execute â†’ Results
      â†“
   
   LLM: Format answer
      â†“
   
   Response: "Doanh thu thÃ¡ng 3 nÄƒm 2018 lÃ  $1,234,567"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 6.2.5. DEMAND FORECASTING

INPUT:
   â€¢ Historical sales: gold.fact_order_item (112K rows)
   â€¢ Customer data: gold.fact_order, gold.dim_customer
   â€¢ Date range: 2016-09-04 â†’ 2018-10-17

PROCESS:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. Feature Build    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Input: fact_order_item, fact_order, dim_customer
   â”‚ Processing:
   â”‚   â€¢ Aggregate to daily level (date Ã— product Ã— region)
   â”‚   â€¢ Create features:
   â”‚     - lag_1, lag_7, lag_28
   â”‚     - roll7, roll28
   â”‚     - dow, month, is_weekend
   â”‚     - price, payment_type
   â”‚ Output: silver.forecast_features (~96K rows)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 2. Model Training   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Input: forecast_features
   â”‚ Processing:
   â”‚   â€¢ LightGBM gradient boosting
   â”‚   â€¢ 5-fold cross-validation
   â”‚   â€¢ Tune hyperparameters
   â”‚   â€¢ Target: y (revenue)
   â”‚ Output: Model artifact â†’ MLflow
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 3. Batch Prediction â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Input: Trained model + latest features
   â”‚ Processing:
   â”‚   â€¢ Recursive roll-forward (28 horizons)
   â”‚   â€¢ Update features using previous yhat
   â”‚   â€¢ Generate confidence intervals
   â”‚ Output: platinum.demand_forecast
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 4. Monitoring       â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Input: Actuals + Forecasts
   â”‚ Processing:
   â”‚   â€¢ Calculate errors (abs_error, pct_error, sMAPE)
   â”‚   â€¢ Detect anomalies
   â”‚ Output: platinum.forecast_monitoring
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OUTPUT:
   â€¢ Forecasts: 28-day horizon Ã— products Ã— regions
   â€¢ Confidence intervals: yhat_lo, yhat_hi
   â€¢ Model metrics: RMSE, MAE, sMAPE, R2
   â€¢ Monitoring: Accuracy tracking

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - etl_pipeline/etl_pipeline/assets/*.py: ETL flow
   - app/pages/1_ğŸ“Š_Query_Window.py: Query Window flow
   - app/pages/2_ğŸ’¬_Chat.py: Chat flow
   - chat_service/main.py: Chat processing
   - etl_pipeline/etl_pipeline/ml/*.py: Forecasting flow


================================================================================
ğŸ“Œ 6.3. RUNBOOK Váº¬N HÃ€NH RÃšT Gá»ŒN (KHá»I Táº O, CHáº Y JOB, KIá»‚M TRA)
================================================================================

FILES: QUICK_START.md, Makefile, full_setup.sh, run_etl.sh

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ KHá»I Táº O Há»† THá»NG

Scenario: Deploy trÃªn mÃ¡y má»›i tá»« Ä‘áº§u

Commands:
   ```bash
   # 1. Clone repository
   git clone https://github.com/truonganpro/Data_Warehouse.git
   cd Data_Warehouse

   # 2. Automated setup (recommended)
   chmod +x setup.sh
   ./setup.sh

   # Hoáº·c manual setup:
   chmod +x download_jars.sh && ./download_jars.sh
   cp env.example .env
   docker-compose up -d
   ```

Expected Output:
   âœ… All 14 services running
   âœ… Health checks passed
   âœ… Accessible via URLs

Verification:
   ```bash
   docker-compose ps  # All should be "Up"
   curl http://localhost:8501/_stcore/health
   curl http://localhost:8001/health
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š CHáº Y ETL JOB

Scenario: Load data vÃ  cháº¡y pipeline

Commands:

Option A - Using Makefile:
   ```bash
   # Bronze layer only
   make etl_bronze
   
   # Full pipeline (all layers)
   # Note: Chá»‰ bronze hiá»‡n Ä‘Æ°á»£c define trong Makefile
   ```

Option B - Using Dagster UI:
   1. Open http://localhost:3001
   2. Jobs â†’ reload_data (hoáº·c full_pipeline_job)
   3. Launch Run
   4. Monitor progress

Option C - Using Docker exec:
   ```bash
   # Bronze
   docker exec etl_pipeline dagster job execute -m etl_pipeline -j reload_data
   
   # Full (náº¿u cÃ³)
   docker exec etl_pipeline dagster job execute -m etl_pipeline -j full_pipeline_job
   ```

Expected Output:
   âœ… Bronze: 9 assets materialized
   âœ… Silver: 10 assets materialized
   âœ… Gold: 10 assets materialized
   âœ… Platinum: 7 assets materialized

Verification:
   ```bash
   # Check Trino
   docker exec trino trino --execute "SHOW SCHEMAS FROM lakehouse;"
   docker exec trino trino --execute "SHOW TABLES FROM lakehouse.bronze;"
   docker exec trino trino --execute "SELECT COUNT(*) FROM lakehouse.bronze.customer;"
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”® CHáº Y FORECAST PIPELINE

Commands:
   ```bash
   # Full automated pipeline
   ./run_forecast_pipeline.sh
   
   # Or step-by-step with Makefile
   make forecast-init      # Initialize MLflow
   make forecast-features  # Build features
   make forecast-train     # Train model (save run_id)
   make forecast-predict RUN_ID=<run_id>  # Generate forecasts
   
   # Via Dagster
   make forecast-full
   ```

Verification:
   ```bash
   # Check forecast tables
   make forecast-check
   
   # Query results
   docker exec trino trino --execute "SELECT * FROM lakehouse.platinum.demand_forecast LIMIT 10;"
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” KIá»‚M TRA Há»† THá»NG

Health Checks:
   ```bash
   # All services status
   docker-compose ps

   # Service health endpoints
   curl http://localhost:8501/_stcore/health  # Streamlit
   curl http://localhost:8001/health          # Chat Service
   curl http://localhost:3000/api/health      # Metabase
   ```

Logs Monitoring:
   ```bash
   # All services
   docker-compose logs -f
   
   # Specific service
   docker-compose logs -f streamlit
   docker-compose logs -f etl_pipeline
   docker-compose logs -f chat_service
   docker-compose logs -f spark-master
   
   # Last 100 lines
   docker-compose logs --tail=100 etl_pipeline
   ```

Data Verification:
   ```bash
   # MySQL source data
   docker exec de_mysql mysql -uroot -padmin123 -e \
     "SELECT COUNT(*) FROM brazillian_ecommerce.customers;"
   
   # Trino data
   docker exec trino trino --execute "SELECT COUNT(*) FROM lakehouse.gold.fact_order;"
   
   # MinIO storage
   docker exec mc mc ls minio/lakehouse/
   ```

Storage Check:
   ```bash
   # MinIO console
   open http://localhost:9001  # minio/minio123
   
   # Trino tables
   docker exec trino trino
   # In Trino CLI:
   USE lakehouse.gold;
   SHOW TABLES;
   DESCRIBE fact_order;
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ› ï¸ TROUBLESHOOTING COMMANDS

Services Not Starting:
   ```bash
   # Check logs
   docker-compose logs -f
   
   # Restart specific service
   docker-compose restart streamlit
   
   # Rebuild and restart
   docker-compose down
   docker-compose build --no-cache
   docker-compose up -d
   ```

ETL Job Failed:
   ```bash
   # Check Dagster logs
   docker-compose logs de_dagster_daemon
   
   # Check ETL pipeline logs
   docker-compose logs etl_pipeline
   
   # Manual retry
   docker exec etl_pipeline dagster job execute -m etl_pipeline -j reload_data
   ```

Query Errors:
   ```bash
   # Test Trino connection
   docker exec trino trino --execute "SHOW CATALOGS;"
   
   # Test query
   docker exec trino trino --execute "SELECT COUNT(*) FROM lakehouse.bronze.customer;"
   ```

MinIO Connection Issues:
   ```bash
   # Check MinIO
   docker exec minio mc ls minio/lakehouse/
   
   # Test from Spark
   docker exec spark-master python3 \
     /opt/bitnami/spark/test_spark_minio_connection.py
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - Makefile: Operational commands
   - full_setup.sh: Complete setup
   - run_etl.sh: ETL execution
   - run_forecast_pipeline.sh: Forecast execution
   - QUICK_START.md: Quick reference
   - README.md: Troubleshooting section


================================================================================
ğŸ“Œ 6.4. Báº¢O Äáº¢M CHáº¤T LÆ¯á»¢NG & AN TOÃ€N TRUY Váº¤N
================================================================================

FILES:
   - chat_service/main.py: SQL safety implementation
   - chat_service/ast_guard.py: AST validation
   - app/pages/1_ğŸ“Š_Query_Window.py: Filter validation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”’ SQL SAFETY MECHANISMS

1. READ-ONLY ENFORCEMENT:
   Pattern: `^\s*(SELECT|WITH)\b` (regex)
   Action: Reject náº¿u khÃ´ng pháº£i SELECT/WITH
   Implementation:
   ```python
   if not READONLY_PATTERN.match(sql):
       raise ValueError("Only SELECT queries allowed")
   ```

2. DANGEROUS KEYWORD BLACKLIST:
   Banned: DROP, TRUNCATE, INSERT, UPDATE, DELETE, CALL, CREATE, ALTER
   Detection: AST parsing (sqlglot) + regex fallback
   Implementation:
   ```python
   def _check_dangerous_keywords_with_ast(sql: str):
       # Use sqlglot to parse
       # Check expression types
       # Report violations
   ```

3. SCHEMA WHITELISTING:
   Allowed: gold, platinum
   Validation: Parse AST, extract schemas, verify
   Implementation:
   ```python
   def _parse_sql_schemas(sql: str) -> Set[str]:
       # Parse with sqlglot
       # Extract from table.db
       # Return set of schemas
   
   if not schemas.issubset(SQL_WHITELIST_SCHEMAS):
       raise ValueError("Schema not whitelisted")
   ```

4. LIMIT ENFORCEMENT:
   Default: 200 rows
   Maximum: 5000 rows
   Auto-append: Náº¿u LIMIT khÃ´ng cÃ³
   Implementation:
   ```python
   if "LIMIT" not in sql.upper():
       sql = f"{sql.rstrip(';')} LIMIT {SQL_DEFAULT_LIMIT}"
   ```

5. TIMEOUT:
   Default: 45 seconds
   Kill: Long-running queries
   Implementation: Trino query_max_run_time

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” DATA QUALITY CHECKS

Silver Layer Quality:
   1. Deduplication: dropDuplicates() on primary keys
   2. NULL Handling: na.drop(), na.fill(), isNotNull()
   3. Type Casting: String â†’ INT, DOUBLE, TIMESTAMP
   4. Precision Control: Round(2) for monetary values
   5. Data Validation: Geographic bounds, timestamp conversion

Gold Layer Quality:
   1. Referential Integrity: JOIN ensures FK relationships
   2. Aggregate Validation: SUM, COUNT, AVG correctness
   3. Business Logic: Boolean flags, computed fields

Platinum Layer Quality:
   1. Aggregation Accuracy: GROUP BY correctness, DISTINCT counts
   2. Measure Calculations: AOV = GMV/Orders, rates, rankings

Logging & Monitoring:
   ```python
   # Row counts logged
   context.log.info(f"{name} rows={df.count()} cols={len(df.columns)}")
   
   # Metadata output
   context.add_output_metadata({
       "rows": row_count,
       "columns": obj.columns,
       "path_data": path_s3a,
   })
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ AUDIT LOGGING

MySQL Schema (chatlogs database):
   â€¢ conversations: Chat history, questions, answers
   â€¢ sql_audit: SQL queries, execution metrics, errors

Features:
   â€¢ Session tracking per user
   â€¢ SQL masking (literals â†’ ?)
   â€¢ Execution metrics (time, rows, success)
   â€¢ Error tracking
   â€¢ Timestamps for audit trail

Privacy Protection:
   ```python
   def _mask_sql_literals(sql: str) -> str:
       # 'data' â†’ '?'
       masked = re.sub(r"'([^']*)'", "'?'", sql)
       # 123 â†’ ?
       masked = re.sub(r"\b\d+(\.\d+)?\b", "?", masked)
       return masked
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - chat_service/main.py: SQL safety implementation
   - chat_service/ast_guard.py: AST validation
   - load_dataset_into_mysql/02_init_chatlogs.sql: Audit schema
   - etl_pipeline/etl_pipeline/assets/silver.py: Quality checks


================================================================================
ğŸ“Œ 6.5. GIÃ TRá»Š THá»°C TIá»„N CHO LÃƒNH Äáº O & PHÃ’NG BAN
================================================================================

FILE: PROJECT_OVERVIEW.md (dÃ²ng 501-595)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¼ BUSINESS VALUE

1. Tá»° Äá»˜NG HÃ“A BÃO CÃO:
   âŒ Before: Manual Excel, 2-3 ngÃ y má»—i tuáº§n
   âœ… After: Automated ETL daily, real-time dashboards
   Impact: 80% giáº£m thá»i gian bÃ¡o cÃ¡o, 0 lá»—i nháº­p liá»‡u

2. Dá»° BÃO CHÃNH XÃC:
   âŒ Before: Rule-based forecasting, 20-30% error
   âœ… After: ML forecasts vá»›i 10-15% error, confidence intervals
   Impact: Giáº£m inventory cost 15%, tÄƒng fill rate 10%

3. DECISION-MAKING NHANH HÆ N:
   âŒ Before: IT pháº£i viáº¿t SQL, 1-2 ngÃ y
   âœ… After: Self-service analytics, < 1 phÃºt
   Impact: Business users Ä‘á»™c láº­p, IT táº­p trung strategic work

4. DATA QUALITY Tá»T HÆ N:
   âŒ Before: Inconsistent data, nhiá»u NULL
   âœ… After: Automated validation, clean data
   Impact: Tin cáº­y vÃ o insights, giáº£m rework

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 1: EXECUTIVE DASHBOARD

Scenario: CEO muá»‘n xem tá»•ng quan monthly

User: Executive
Tool: Metabase Dashboard

Input:
   â€¢ Executive Summary Dashboard
   â€¢ Auto-refresh daily

Output:
   â€¢ Total GMV: $XXM (â†‘ 15% YoY)
   â€¢ Total Orders: XXK
   â€¢ AOV: $XX
   â€¢ Revenue trend (chart)
   â€¢ Top 5 categories (chart)

Value:
   âœ… Quick decision: Invest more vÃ o top categories
   âœ… Strategic planning: Budget allocation
   âœ… Performance monitoring: Growth tracking

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 2: DEMAND FORECASTING

Scenario: Supply Chain Manager plan inventory

User: Supply Chain Manager
Tool: Forecast Pipeline + Dashboards

Input:
   â€¢ Historical sales (2 years)
   â€¢ Product catalog (32K products)
   â€¢ Regional data

Output:
   â€¢ 28-day forecasts per product Ã— region
   â€¢ Confidence intervals (yhat_lo, yhat_hi)
   â€¢ Monitoring accuracy (sMAPE, RMSE)

Value:
   âœ… Reduce stockouts: 25% improvement
   âœ… Optimize inventory: 15% cost reduction
   âœ… Improve fill rate: 10% increase
   âœ… Better cash flow: Lower carrying cost

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 3: SALES ANALYSIS

Scenario: Sales Manager phÃ¢n tÃ­ch trend theo category

User: Sales Manager
Tool: Query Window (OLAP)

Input:
   â€¢ Table: platinum.dm_sales_monthly_category
   â€¢ Time: Last 12 months
   â€¢ Dimensions: product_category_name_english
   â€¢ Measures: GMV, Orders, AOV

Output:
   â€¢ Pivot table vá»›i ROLLUP totals
   â€¢ Excel export
   â€¢ Trend analysis

Value:
   âœ… Identify growth categories: Focus marketing
   âœ… AOV optimization: Bundle strategies
   âœ… Budget allocation: Data-driven decisions

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 4: CUSTOMER INSIGHTS

Scenario: Marketing Manager phÃ¢n tÃ­ch retention

User: Marketing Manager
Tool: Chat Interface

Input:
   â€¢ Question: "Customer retention rate by cohort?"

Output:
   â€¢ Cohort table
   â€¢ Retention rates
   â€¢ Recommendations

Value:
   âœ… Target campaigns: Specific cohorts
   âœ… Reduce churn: 20% improvement
   âœ… Increase LTV: Better segmentation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 5: OPERATIONS OPTIMIZATION

Scenario: Operations Manager tá»‘i Æ°u logistics

User: Operations Manager
Tool: Logistics SLA Datamart

Input:
   â€¢ Table: platinum.dm_logistics_sla
   â€¢ Metrics: on_time_rate, avg_delivered_days

Output:
   â€¢ Regional performance
   â€¢ Bottleneck identification
   â€¢ Delivery trends

Value:
   âœ… Improve on-time rate: 15% increase
   âœ… Cost reduction: Route optimization
   âœ… Customer satisfaction: Faster delivery

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š USE CASE 6: PAYMENT ANALYSIS

Scenario: Finance Manager phÃ¢n tÃ­ch payment methods

User: Finance Manager
Tool: Payment Mix Datamart

Input:
   â€¢ Table: platinum.dm_payment_mix
   â€¢ Focus: credit_card usage

Output:
   â€¢ Payment distribution
   â€¢ Trends over time
   â€¢ Installment patterns

Value:
   âœ… Optimize payment fees: Negotiate rates
   âœ… Risk management: Fraud detection
   âœ… Cash flow planning: Predict timing

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ ROI ESTIMATION

Time Savings:
   â€¢ IT team: 10 hours/week â†’ 2 hours/week (80% reduction)
   â€¢ Business users: 5 hours/week â†’ 30 min/week (90% reduction)
   â€¢ Total: 1,300 hours/year saved

Cost Savings:
   â€¢ Reduced inventory waste: $50K/year
   â€¢ Optimized logistics: $30K/year
   â€¢ Decreased manual errors: $20K/year
   â€¢ Total: ~$100K/year

Revenue Impact:
   â€¢ Better forecasting: +5% fill rate â†’ +$200K revenue
   â€¢ Data-driven decisions: +10% AOV â†’ +$150K revenue
   â€¢ Reduced churn: +$100K revenue
   â€¢ Total: ~$450K/year

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - PROJECT_OVERVIEW.md (dÃ²ng 501-595): Use cases
   - README.md: Business value section


================================================================================
ğŸ“Œ 6.6. TIá»‚U Káº¾T CHÆ¯Æ NG
================================================================================

ğŸ“ ÄIá»‚M CHÃNH Cáº¦N TÃ“M Táº®T:

âœ… Tá»”NG QUAN MODULES:
   â€¢ 5 modules: ETL, BI, OLAP, Chat, Forecasting
   â€¢ 7 user personas: Engineer, Analyst, Manager, Scientist, etc.
   â€¢ Purpose: Tá»« raw data â†’ business insights

âœ… INPUT â†’ OUTPUT FLOWS:
   â€¢ ETL: MySQL â†’ Bronze â†’ Silver â†’ Gold â†’ Platinum
   â€¢ BI: Trino queries â†’ Dashboards
   â€¢ OLAP: UI selections â†’ Pivot tables
   â€¢ Chat: NL â†’ SQL â†’ Answers
   â€¢ Forecast: Historical â†’ ML â†’ Predictions

âœ… RUNBOOK:
   â€¢ 1 command deployment (setup.sh)
   â€¢ Automated ETL daily
   â€¢ Manual override khi cáº§n
   â€¢ Comprehensive health checks

âœ… QUALITY & SECURITY:
   â€¢ SQL safety (read-only, whitelist, LIMIT)
   â€¢ Data quality (dedupe, validation, casting)
   â€¢ Audit logging (privacy, compliance)
   â€¢ Performance optimization

âœ… BUSINESS VALUE:
   â€¢ ROI: ~$550K/year (cost savings + revenue)
   â€¢ Time savings: 1,300 hours/year
   â€¢ Decision speed: Real-time vs 1-2 days
   â€¢ Accuracy: 15-20% improvement

ğŸ¯ THÃ€NH QUáº¢:
   Production-ready Data Lakehouse vá»›i:
   â€¢ 5 functional modules
   â€¢ 7 datamarts
   â€¢ 36 ETL assets
   â€¢ AI-powered analytics
   â€¢ ML forecasting

ğŸ“Š Káº¾T QUáº¢:
   Business users tá»± do, IT táº­p trung strategic work
   Data quality tá»‘t, insights Ä‘Ã¡ng tin cáº­y
   Forecasting chÃ­nh xÃ¡c, inventory tá»‘i Æ°u

================================================================================
ğŸ“ DANH SÃCH FILE Cáº¦N Äá»ŒC Äá»‚ VIáº¾T BÃO CÃO
================================================================================

MUST READ:
   1. PROJECT_OVERVIEW.md (full)
   2. app/pages/1_ğŸ“Š_Query_Window.py: OLAP interface
   3. app/pages/2_ğŸ’¬_Chat.py: Chat interface
   4. chat_service/main.py: Chat backend
   5. chat_service/ast_guard.py: Security
   6. docs/chat_service_guide.md: Chat guide
   7. etl_pipeline/etl_pipeline/ml/*.py: Forecasting
   8. Makefile: Operations

NICE TO HAVE:
   9. QUICK_START.md: Quick reference
   10. README.md: Use cases
   11. full_setup.sh: Setup automation
   12. run_forecast_pipeline.sh: Forecast automation

================================================================================
EOF


