# THÃ”NG TIN BÃO CÃO CHÆ¯Æ NG 3 - DATA LAKEHOUSE PROJECT
# Thiáº¿t káº¿ kiáº¿n trÃºc vÃ  triá»ƒn khai há»‡ thá»‘ng

================================================================================
ğŸ“Œ 3.1. YÃŠU Cáº¦U Há»† THá»NG (CHá»¨C NÄ‚NG & PHI CHá»¨C NÄ‚NG)
================================================================================

FILE: README.md (dÃ²ng 85-160), setup.sh (dÃ²ng 51-102)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ YÃŠU Cáº¦U Há»† THá»NG Váº¬T LÃ:

Hardware:
   â€¢ Docker & Docker Compose (containerization platform)
   â€¢ 8GB RAM trá»Ÿ lÃªn (recommended: 16GB+)
   â€¢ 20GB dung lÆ°á»£ng trá»‘ng (recommended: 50GB+)
   â€¢ CPU: Multi-core (Ä‘á»ƒ cháº¡y Spark cluster)

Software:
   â€¢ Operating System: Linux/macOS/Windows vá»›i Docker Desktop
   â€¢ Python 3.9+ (cho development)
   â€¢ Git (Ä‘á»ƒ clone repository)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… CHá»¨C NÄ‚NG (Functional Requirements):

1. ETL Pipeline Tá»± Äá»™ng:
   â€¢ Load dá»¯ liá»‡u tá»« MySQL (source) â†’ Bronze layer
   â€¢ Transform: Bronze â†’ Silver â†’ Gold â†’ Platinum
   â€¢ Medallion Architecture vá»›i 4 layers
   â€¢ Incremental loading (khÃ´ng load láº¡i toÃ n bá»™)
   â€¢ Schema evolution support

2. Data Storage & Management:
   â€¢ Delta Lake vá»›i ACID transactions
   â€¢ Versioning & time travel
   â€¢ Partitioning tá»‘i Æ°u
   â€¢ Metadata cataloging vá»›i Hive Metastore

3. Query & Analytics:
   â€¢ SQL interface vá»›i Trino (distributed query engine)
   â€¢ OLAP queries vá»›i ROLLUP/GROUPING SETS
   â€¢ BI dashboards (Metabase)
   â€¢ Export CSV/Excel
   â€¢ Natural language query (AI chatbot)

4. ML & Forecasting:
   â€¢ Feature engineering tá»± Ä‘á»™ng
   â€¢ Model training vá»›i LightGBM
   â€¢ MLflow experiment tracking
   â€¢ Batch prediction pipeline
   â€¢ 28-day forecast horizon

5. Orchestration:
   â€¢ Dagster workflow management
   â€¢ Scheduled jobs (daily 3:00 AM)
   â€¢ Error handling & retry
   â€¢ Lineage tracking
   â€¢ Monitoring & alerting

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš¡ PHI CHá»¨C NÄ‚NG (Non-Functional Requirements):

Performance:
   â€¢ Query latency: < 10s cho 95% queries
   â€¢ ETL throughput: xá»­ lÃ½ Ä‘Æ°á»£c 100K records/minute
   â€¢ Concurrent users: 10+ simultaneous queries
   â€¢ Cache: TTL 10 phÃºt cho query results

Scalability:
   â€¢ Horizontal scaling vá»›i Spark workers
   â€¢ Auto-scaling Delta Lake tables
   â€¢ Stateless services (Docker containers)

Availability:
   â€¢ Uptime target: 99.9%
   â€¢ Health checks cho táº¥t cáº£ services
   â€¢ Graceful degradation náº¿u service down
   â€¢ Data replication vá»›i Delta Lake

Security:
   â€¢ Read-only SQL queries (cháº·n DELETE/DROP)
   â€¢ Schema whitelisting (chá»‰ gold, platinum)
   â€¢ SQL injection prevention
   â€¢ Audit logging cho táº¥t cáº£ queries
   â€¢ Masked literals trong logs

Maintainability:
   â€¢ Containerized deployment (Docker)
   â€¢ Infrastructure as Code (docker-compose)
   â€¢ Automated setup script
   â€¢ Comprehensive logging
   â€¢ Documentation Ä‘áº§y Ä‘á»§

Compatibility:
   â€¢ S3-compatible storage (MinIO)
   â€¢ Hive Metastore standards
   â€¢ Delta Lake open format
   â€¢ SQL standards (ANSI SQL)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - README.md (dÃ²ng 85-160): System requirements, setup
   - setup.sh (dÃ²ng 51-102): Pre-flight checks
   - docker-compose.yaml: Infrastructure as Code
   - PROJECT_OVERVIEW.md: Feature overview


================================================================================
ğŸ“Œ 3.2. KIáº¾N TRÃšC Tá»”NG THá»‚ Äá»€ XUáº¤T
================================================================================

FILE: PROJECT_OVERVIEW.md (dÃ²ng 24-71), README.md (dÃ²ng 41-82)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ MODERN DATA STACK ARCHITECTURE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER (USER INTERFACES)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Streamlit  â”‚  Metabase   â”‚   Dagster   â”‚   Jupyter                     â”‚
â”‚  (Port 8501)â”‚ (Port 3000) â”‚ (Port 3001) â”‚  (Port 8888)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“              â†“              â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPUTE LAYER (Query & Processing)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Trino     â”‚    Spark    â”‚   MLflow    â”‚   Chat Service                â”‚
â”‚ (Port 8082) â”‚ (Port 8080) â”‚ (Port 5000) â”‚  (Port 8001)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“              â†“              â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE & METADATA LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Delta Lake  â”‚    MinIO    â”‚    MySQL    â”‚   Qdrant (Vector DB)          â”‚
â”‚ (Lakehouse) â”‚(S3 Object)  â”‚ (Metadata)  â”‚  (RAG Embeddings)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š MEDALLION ARCHITECTURE (DATA LAYERS):

MySQL (Source Data)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BRONZE LAYER          â”‚ â†’ Raw data tá»« MySQL (9 tables)
â”‚ Location: s3a://lakehouse/bronze/â”‚
â”‚ Format: Delta Lake       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SILVER LAYER          â”‚ â†’ Cleaned & normalized (10 tables + features)
â”‚ Location: s3a://lakehouse/silver/â”‚
â”‚ Format: Delta Lake       â”‚
â”‚ Cleaning: dropDuplicates, na.drop, casting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    GOLD LAYER            â”‚ â†’ Star schema: 6 Dimensions + 4 Facts
â”‚ Location: s3a://lakehouse/gold/â”‚
â”‚ Format: Delta Lake       â”‚
â”‚ Dimensions: customer, seller, product, category, date, geo
â”‚ Facts: order, order_item, review, payment
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PLATINUM LAYER        â”‚ â†’ Business datamarts (7 + forecast)
â”‚ Location: s3a://lakehouse/platinum/â”‚
â”‚ Format: Delta Lake       â”‚
â”‚ Datamarts: sales, seller_kpi, customer_lifecycle, payment_mix, logistics, demand_forecast
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Trino â†’ BI Dashboards â†’ Business Users

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ TECHNOLOGY STACK:

Storage Layer:
   â€¢ MinIO: S3-compatible object storage
   â€¢ Delta Lake: ACID transactions, versioning
   â€¢ MySQL: Metadata (Hive Metastore, Dagster storage)

Metadata Layer:
   â€¢ Hive Metastore: Table catalog, schema management
   â€¢ Trino Catalog: lakehouse (Delta Lake connector)

Compute Layer:
   â€¢ Apache Spark 3.3.2: ETL processing
   â€¢ Trino 414: Distributed SQL queries
   â€¢ MLflow: Experiment tracking, model registry

Orchestration:
   â€¢ Dagster: Workflow management, scheduling
   â€¢ Docker Compose: Service orchestration

Presentation Layer:
   â€¢ Streamlit: Interactive dashboards, Query Window
   â€¢ Metabase: BI dashboards, reports
   â€¢ Dagster UI: ETL monitoring
   â€¢ Chat Service: Natural language interface

AI/ML:
   â€¢ LightGBM: Gradient boosting for forecasting
   â€¢ Qdrant: Vector database for RAG
   â€¢ Gemini AI: SQL generation, summarization

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š SERVICE DEPENDENCIES:

Dependency Graph:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ de_mysqlâ”‚ (foundation: metadata, source data)
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â†’ hive-metastore
        â”œâ”€â”€â†’ de_dagster (storage)
        â””â”€â”€â†’ chat_service (logs)
        
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    minio    â”‚ (foundation: object storage)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â†’ spark-master
          â”œâ”€â”€â†’ etl_pipeline
          â””â”€â”€â†’ trino
          
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ hive-       â”‚
   â”‚ metastore   â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â†’ trino
          â””â”€â”€â†’ etl_pipeline
          
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ spark-master  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â†’ spark-worker-1
          â””â”€â”€â†’ etl_pipeline
          
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   trino     â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â†’ metabase
          â”œâ”€â”€â†’ streamlit
          â””â”€â”€â†’ chat_service
          
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  qdrant     â”‚ (independent)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â””â”€â”€â†’ chat_service

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - PROJECT_OVERVIEW.md (dÃ²ng 24-71): Architecture overview
   - README.md (dÃ²ng 41-82): Technology stack, Medallion architecture
   - docker-compose.yaml: Complete service definitions
   - etl_pipeline/etl_pipeline/assets/*.py: Layer implementations


================================================================================
ğŸ“Œ 3.3. MÃ”I TRÆ¯á»œNG THá»°C NGHIá»†M (DOCKER COMPOSE, TÃ€I NGUYÃŠN)
================================================================================

FILE: docker-compose.yaml, setup.sh, README.md (dÃ²ng 85-116)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ³ DOCKER COMPOSE ARCHITECTURE:

Total Services: 14 containers
   â€¢ 1 MySQL (source data + metadata)
   â€¢ 1 Hive Metastore
   â€¢ 1 MinIO + 1 MC (bucket setup)
   â€¢ 1 Trino coordinator
   â€¢ 1 Spark Master + 1 Spark Worker
   â€¢ 1 Metabase
   â€¢ 1 Dagster (webserver + daemon)
   â€¢ 1 ETL Pipeline (Dagster)
   â€¢ 1 Streamlit
   â€¢ 1 Chat Service
   â€¢ 1 Qdrant (vector DB)

Network: de_network (bridge)

Volumes:
   â€¢ mysql/ â†’ Persistent MySQL data
   â€¢ minio/ â†’ Object storage data
   â€¢ dagster_home/ â†’ Dagster storage, logs
   â€¢ metabase_data/ â†’ Metabase DB
   â€¢ qdrant_data/ â†’ Vector embeddings

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š TÃ€I NGUYÃŠN Má»–I SERVICE:

MySQL (de_mysql):
   â€¢ Image: mysql:8.0
   â€¢ Port: 3306
   â€¢ Volumes: ./mysql, ./load_dataset_into_mysql
   â€¢ Healthcheck: mysqladmin ping

Hive Metastore:
   â€¢ Build: ./docker_image/hive-metastore
   â€¢ Port: 9083 (Thrift)
   â€¢ Dependency: de_mysql (healthy)
   â€¢ Config: metastore-site.xml

MinIO:
   â€¢ Image: minio/minio
   â€¢ Ports: 9000 (API), 9001 (Console)
   â€¢ Volume: ./minio
   â€¢ Buckets: lakehouse (auto-created by mc)

Trino:
   â€¢ Image: trinodb/trino:414
   â€¢ Port: 8082 (mapped from 8080)
   â€¢ Volume: ./trino/etc
   â€¢ Catalog: lakehouse (Delta Lake)

Spark:
   â€¢ Master: bitnami/spark:3.3.2
   â€¢ Worker: bitnami/spark:3.3.2
   â€¢ Ports: 7077 (Master), 8080 (Web UI), 8081 (Worker Web UI)
   â€¢ Volume: ./jars (Delta, S3 connectors)

Metabase:
   â€¢ Image: metabase/metabase:latest
   â€¢ Port: 3000
   â€¢ Volume: metabase_data (H2 database)

Dagster:
   â€¢ Webserver: de_dagster_dagit (Port 3001)
   â€¢ Daemon: de_dagster_daemon
   â€¢ Storage: MySQL (de_mysql)
   â€¢ Volume: ./dagster_home

ETL Pipeline:
   â€¢ Build: ./etl_pipeline
   â€¢ Port: 4000 (gRPC for Dagster)
   â€¢ Depends: spark-master, minio, de_mysql

Streamlit:
   â€¢ Build: ./docker_image/streamlit
   â€¢ Port: 8501
   â€¢ Volume: ./app

Chat Service:
   â€¢ Build: ./chat_service
   â€¢ Port: 8001
   â€¢ Depends: qdrant, trino, de_mysql

Qdrant:
   â€¢ Image: qdrant/qdrant:latest
   â€¢ Port: 6333
   â€¢ Volume: ./qdrant_data

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸš€ AUTOMATED SETUP:

Script: setup.sh
Functionality:
   1. Pre-flight checks:
      â€¢ Docker & Docker Compose installed
      â€¢ Disk space >= 20GB
      â€¢ RAM >= 8GB
      
   2. Environment setup:
      â€¢ Create .env from env.example
      â€¢ Download JAR dependencies (download_jars.sh)
      â€¢ Validate configuration
      
   3. Infrastructure deployment:
      â€¢ Build Docker images
      â€¢ docker-compose up -d
      â€¢ Wait for health checks
      
   4. Verification:
      â€¢ Check all services running
      â€¢ Test endpoints
      â€¢ Verify connectivity

Execution: ./setup.sh (single command)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š RESOURCE ESTIMATION:

Minimum Requirements:
   â€¢ CPU: 4 cores
   â€¢ RAM: 8GB
   â€¢ Disk: 20GB
   â€¢ Network: Internal Docker network

Recommended Requirements:
   â€¢ CPU: 8+ cores
   â€¢ RAM: 16GB+
   â€¢ Disk: 50GB+ SSD
   â€¢ Network: Docker bridge network

Peak Resource Usage:
   â€¢ Spark processing: 6GB RAM, 2 cores
   â€¢ Trino queries: 2GB RAM, 1 core
   â€¢ Dagster ETL: 4GB RAM, 2 cores
   â€¢ Other services: 2GB RAM, 1 core
   Total: ~14GB RAM, 6 cores

Storage Breakdown:
   â€¢ MySQL: 1GB
   â€¢ MinIO (Delta Lake): 15GB+
   â€¢ Dagster storage: 1GB
   â€¢ Other: 3GB

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - docker-compose.yaml: Complete service definitions
   - setup.sh: Automated deployment
   - download_jars.sh: JAR dependencies
   - README.md (dÃ²ng 85-116): Setup instructions
   - QUICK_START.md: Quick deployment guide


================================================================================
ğŸ“Œ 3.4. THIáº¾T Láº¬P & Cáº¤U HÃŒNH CÃC THÃ€NH PHáº¦N
================================================================================

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“¦ 3.4.1. LÆ¯U TRá»®: MINIO & Bá» Cá»¤C BUCKETS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILES: docker-compose.yaml (dÃ²ng 53-86), env.example (dÃ²ng 28-36)

Configuration:
   â€¢ Image: minio/minio
   â€¢ Command: server /data --console-address :9001
   â€¢ Ports: 9000 (API), 9001 (Web Console)
   â€¢ Access: minio/minio123
   â€¢ Endpoint: http://minio:9000 (internal), http://localhost:9001 (console)

Bucket Setup (mc service):
   ```bash
   # Auto-create lakehouse bucket
   mc mb minio/lakehouse
   mc policy set public minio/lakehouse
   ```

Bucket Structure:
   ```
   lakehouse/
   â”œâ”€â”€ bronze/
   â”‚   â”œâ”€â”€ customer/
   â”‚   â”œâ”€â”€ seller/
   â”‚   â”œâ”€â”€ product/
   â”‚   â”œâ”€â”€ order/
   â”‚   â”œâ”€â”€ orderitem/
   â”‚   â”œâ”€â”€ payment/
   â”‚   â”œâ”€â”€ orderreview/
   â”‚   â”œâ”€â”€ productcategory/
   â”‚   â””â”€â”€ geolocation/
   â”œâ”€â”€ silver/
   â”‚   â”œâ”€â”€ customer/
   â”‚   â”œâ”€â”€ seller/
   â”‚   â”œâ”€â”€ product/
   â”‚   â”œâ”€â”€ order/
   â”‚   â”œâ”€â”€ orderitem/
   â”‚   â”œâ”€â”€ payment/
   â”‚   â”œâ”€â”€ orderreview/
   â”‚   â”œâ”€â”€ productcategory/
   â”‚   â”œâ”€â”€ date/
   â”‚   â””â”€â”€ forecast_features/
   â”œâ”€â”€ gold/
   â”‚   â”œâ”€â”€ dimcustomer/
   â”‚   â”œâ”€â”€ dimseller/
   â”‚   â”œâ”€â”€ dimproduct/
   â”‚   â”œâ”€â”€ dimproductcategory/
   â”‚   â”œâ”€â”€ dimdate/
   â”‚   â”œâ”€â”€ dimgeolocation/
   â”‚   â”œâ”€â”€ factorder/
   â”‚   â”œâ”€â”€ factorderitem/
   â”‚   â”œâ”€â”€ factreview/
   â”‚   â””â”€â”€ factpayment/
   â””â”€â”€ platinum/
       â”œâ”€â”€ dmsalesmonthlycategory/
       â”œâ”€â”€ dmsellerkpi/
       â”œâ”€â”€ dmcustomerlifecycle/
       â”œâ”€â”€ dmpaymentmix/
       â”œâ”€â”€ dmlogisticssla/
       â””â”€â”€ demand_forecast/
   ```

Access Pattern:
   â€¢ Spark/Trino: s3a://lakehouse/ (read/write)
   â€¢ Hive Metastore: s3://lakehouse/ (catalog registration)
   â€¢ Console: http://localhost:9001 (manual management)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 3.4.2. METADATA: HIVE METASTORE (MYSQL) & CATALOG TRINO
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILES: 
   - docker-compose.yaml (dÃ²ng 7-50)
   - docker_image/hive-metastore/metastore-site.xml
   - trino/etc/catalog/lakehouse.properties

Hive Metastore Configuration (metastore-site.xml):
   ```xml
   <property>
     <name>javax.jdo.option.ConnectionURL</name>
     <value>jdbc:mysql://de_mysql:3306/metastore</value>
   </property>
   <property>
     <name>javax.jdo.option.ConnectionDriverName</name>
     <value>com.mysql.cj.jdbc.Driver</value>
   </property>
   <property>
     <name>hive.metastore.warehouse.dir</name>
     <value>s3a://lakehouse/</value>
   </property>
   <property>
     <name>fs.s3a.endpoint</name>
     <value>http://minio:9000</value>
   </property>
   ```

Setup:
   â€¢ Image: Custom build (docker_image/hive-metastore)
   â€¢ Port: 9083 (Thrift)
   â€¢ Dependency: de_mysql must be healthy
   â€¢ Database: metastore (auto-created)
   â€¢ Credentials: hive/hive

Trino Catalog (lakehouse.properties):
   ```
   connector.name=delta_lake
   
   # MinIO credentials
   hive.s3.aws-access-key=minio
   hive.s3.aws-secret-key=minio123
   hive.s3.endpoint=http://minio:9000
   hive.s3.path-style-access=true
   hive.s3.ssl.enabled=false
   
   # Hive Metastore connection
   hive.metastore.uri=thrift://hive-metastore:9083
   
   # Delta Lake configuration
   delta.register-table-procedure.enabled=true
   ```

Usage:
   â€¢ Spark creates tables via Hive Metastore
   â€¢ Trino reads catalog from Hive Metastore
   â€¢ Single source of truth for schema

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš¡ 3.4.3. Xá»¬ LÃ: SPARK + DELTA (Cáº¤U HÃŒNH S3A, EXTENSIONS)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILES:
   - docker_image/spark/conf/spark-defaults.conf
   - etl_pipeline/etl_pipeline/resources/spark_io_manager.py
   - jars/ (JAR dependencies)

Spark Configuration (spark-defaults.conf):
   ```
   # JARs
   spark.jars=/opt/jars/mysql-connector-j-8.0.33.jar,
            /opt/jars/delta-core_2.12-2.3.0.jar,
            /opt/jars/delta-storage-2.3.0.jar,
            /opt/jars/hadoop-aws-3.3.2.jar,
            /opt/jars/aws-java-sdk-bundle-1.11.1026.jar

   # Delta Lake
   spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
   spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

   # S3 / MinIO configs
   spark.hadoop.fs.s3a.endpoint=http://minio:9000
   spark.hadoop.fs.s3a.access.key=minio
   spark.hadoop.fs.s3a.secret.key=minio123
   spark.hadoop.fs.s3a.path.style.access=true
   spark.hadoop.fs.s3a.connection.ssl.enabled=false
   spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
   spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

   # Warehouse path
   spark.sql.warehouse.dir=s3a://lakehouse/

   # Memory / Parallelism
   spark.driver.memory=4g
   spark.executor.memory=4g
   spark.executor.cores=2
   spark.default.parallelism=4
   ```

SparkIOManager Configuration (Python):
   ```python
   builder = (
       SparkSession.builder
       .master("spark://spark-master:7077")
       .config("spark.jars", spark_jars)
       .config("spark.sql.extensions", 
               "io.delta.sql.DeltaSparkSessionExtension")
       .config("spark.sql.catalog.spark_catalog", 
               "org.apache.spark.sql.delta.catalog.DeltaCatalog")
       .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")
       .config("spark.hadoop.fs.s3a.access.key", "minio")
       .config("spark.hadoop.fs.s3a.secret.key", "minio123")
       .config("spark.sql.warehouse.dir", "s3a://lakehouse/")
       .config("hive.metastore.uris", "thrift://hive-metastore:9083")
       .enableHiveSupport()
   )
   ```

JAR Dependencies (jars/):
   â€¢ mysql-connector-j-8.0.33.jar (JDBC)
   â€¢ delta-core_2.12-2.3.0.jar (Delta Lake)
   â€¢ delta-storage-2.3.0.jar (Delta storage)
   â€¢ hadoop-aws-3.3.2.jar (S3A filesystem)
   â€¢ aws-java-sdk-bundle-1.11.1026.jar (AWS SDK)

Workflow:
   1. Spark reads from JDBC (MySQL)
   2. Transform data with PySpark
   3. Write to Delta Lake on S3A (MinIO)
   4. Register schema to Hive Metastore
   5. Trino queries via Delta Lake connector

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”„ 3.4.4. ORCHESTRATION: DAGSTER (REPO, SCHEDULES, IO MANAGERS)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILES:
   - dagster_home/dagster.yaml
   - dagster_home/workspace.yaml
   - etl_pipeline/etl_pipeline/__init__.py
   - etl_pipeline/etl_pipeline/job/__init__.py
   - etl_pipeline/etl_pipeline/resources/spark_io_manager.py

Dagster Configuration (dagster.yaml):
   ```yaml
   storage:
     mysql:
       mysql_db:
         username:
           env: DAGSTER_MYSQL_USERNAME
         password:
           env: DAGSTER_MYSQL_PASSWORD
         hostname:
           env: DAGSTER_MYSQL_HOSTNAME
         db_name:
           env: DAGSTER_MYSQL_DB
         port: 3306

   scheduler:
     module: dagster.core.scheduler
     class: DagsterDaemonScheduler
     config:
       max_catchup_runs: 5

   run_coordinator:
     module: dagster.core.run_coordinator
     class: QueuedRunCoordinator
     config:
       max_concurrent_runs: 3
   ```

Workspace (workspace.yaml):
   ```yaml
   load_from:
     - grpc_server:
         host: etl_pipeline
         port: 4000
         location_name: "etl_pipeline"
   ```

Resource Configuration (Python):
   ```python
   SPARK_CONFIG = {
       "endpoint_url": "http://minio:9000",
       "minio_access_key": "minio",
       "minio_secret_key": "minio123",
       "jdbc_jar_path": "/opt/jars/mysql-connector-j-8.0.33.jar",
       "jars_dir": "/opt/jars",
       "warehouse": "s3a://lakehouse/",
       "master": "spark://spark-master:7077",
   }

   resources = {
       "spark_io_manager": SparkIOManager(SPARK_CONFIG),
   }
   ```

Jobs:
   1. reload_data: Bronze layer only
      â€¢ Selection: bronze_selection
      â€¢ Purpose: Initial data load
      
   2. full_pipeline_job: All layers
      â€¢ Selection: AssetSelection.all()
      â€¢ Purpose: Complete ETL pipeline

Schedules:
   â€¢ reload_data_schedule: Daily 3:00 AM Asia/Ho_Chi_Minh
   â€¢ daily_forecast_schedule: Daily 4:00 AM (if forecast enabled)

Assets Organization:
   â€¢ Bronze: 9 assets (raw data)
   â€¢ Silver: 10 assets (cleaned data)
   â€¢ Gold: 10 assets (star schema)
   â€¢ Platinum: 7 assets (datamarts)
   
Total: 36 assets in pipeline

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š 3.4.5. BI: METABASE (Káº¾T Ná»I, PHÃ‚N QUYá»€N)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FILES: docker-compose.yaml (dÃ²ng 106-131), README.md (dÃ²ng 154-160)

Configuration:
   â€¢ Image: metabase/metabase:latest
   â€¢ Port: 3000
   â€¢ Platform: linux/amd64
   â€¢ Volume: metabase_data (H2 database)
   â€¢ Healthcheck: curl http://localhost:3000/api/health

Initial Setup (First Access):
   1. Open http://localhost:3000
   2. Create admin account
   3. Add Database:
      â€¢ Type: Presto (compatible with Trino)
      â€¢ Host: trino
      â€¢ Port: 8080
      â€¢ Database: lakehouse
      â€¢ Username: admin
      â€¢ Password: (empty)
   4. Test connection â†’ Save

Available Schemas:
   â€¢ bronze: Raw data tables
   â€¢ silver: Cleaned tables
   â€¢ gold: Star schema (facts + dimensions)
   â€¢ platinum: Business datamarts

Default Sample Queries:
   â€¢ Monthly Revenue Trend
   â€¢ Top Categories by Revenue
   â€¢ Payment Mix Analysis
   â€¢ Logistics SLA Performance
   â€¢ Customer Lifecycle Analysis

Permissions (Default):
   â€¢ Admin: Full access
   â€¢ Users: Can query all schemas
   â€¢ Groups: Configure as needed

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - docker-compose.yaml: All service configs
   - env.example: Environment variables
   - dagster_home/*.yaml: Dagster config
   - docker_image/spark/conf/spark-defaults.conf: Spark config
   - trino/etc/catalog/*.properties: Trino catalogs
   - etl_pipeline/etl_pipeline/resources/*.py: IO managers
   - metabase_queries.sql: Sample queries


================================================================================
ğŸ“Œ 3.5. Báº¢O Máº¬T, LOGGING VÃ€ QUAN SÃT Há»† THá»NG
================================================================================

FILES: chat_service/main.py, chat_service/ast_guard.py, load_dataset_into_mysql/02_init_chatlogs.sql

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”’ SQL SAFETY MECHANISMS:

1. Read-Only Enforcement:
   Pattern: `^\s*(SELECT|WITH)\b`
   Action: Reject if not SELECT/WITH statement
   
2. Keyword Blacklist:
   Banned: DROP, TRUNCATE, INSERT, UPDATE, DELETE, CALL, CREATE, ALTER
   Detection: Regex + AST parsing (if sqlglot available)
   
3. Schema Whitelisting:
   Allowed: gold, platinum
   Validation: Parse SQL AST, extract schemas, verify
   
4. LIMIT Enforcement:
   Default: 200 rows
   Maximum: 5000 rows
   Auto-append: If LIMIT not present
   
5. Timeout:
   Default: 45 seconds
   Kill: Long-running queries

Implementation:
   ```python
   def enforce_sql_safety(sql: str) -> str:
       # 1. Check read-only
       if not READONLY_PATTERN.match(sql):
           raise ValueError("Only SELECT queries allowed")
       
       # 2. Check dangerous keywords
       if dangerous_keywords_found:
           raise ValueError("Dangerous keywords detected")
       
       # 3. Check schema whitelist
       schemas = _parse_sql_schemas(sql)
       if not schemas.issubset(SQL_WHITELIST_SCHEMAS):
           raise ValueError("Schema not whitelisted")
       
       # 4. Enforce LIMIT
       if "LIMIT" not in sql.upper():
           sql = f"{sql.rstrip(';')} LIMIT {SQL_DEFAULT_LIMIT}"
       
       return sql
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ AUDIT LOGGING:

MySQL Schema (chatlogs database):
   ```sql
   CREATE TABLE conversations (
       id INT AUTO_INCREMENT PRIMARY KEY,
       session_id VARCHAR(64) NOT NULL,
       question TEXT NOT NULL,
       answer TEXT,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       INDEX idx_session (session_id),
       INDEX idx_created (created_at)
   );

   CREATE TABLE sql_audit (
       id INT AUTO_INCREMENT PRIMARY KEY,
       session_id VARCHAR(64) NOT NULL,
       generated_sql TEXT,
       executed_sql TEXT,
       success TINYINT(1) DEFAULT 0,
       rows_returned INT DEFAULT 0,
       execution_time_ms INT,
       error_message TEXT,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       INDEX idx_session (session_id),
       INDEX idx_created (created_at)
   );
   ```

Logging Features:
   â€¢ Conversation history per session
   â€¢ SQL queries (masked literals for privacy)
   â€¢ Execution metrics (time, rows)
   â€¢ Error tracking
   â€¢ Timestamps for audit trail

Data Masking (Privacy):
   ```python
   def _mask_sql_literals(sql: str) -> str:
       # Replace string literals: 'data' â†’ '?'
       masked = re.sub(r"'([^']*)'", "'?'", sql)
       # Replace numeric literals: 123 â†’ ?
       masked = re.sub(r"\b\d+(\.\d+)?\b", "?", masked)
       return masked
   ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š HEALTH CHECKS & MONITORING:

Service Health Checks (docker-compose.yaml):
   â€¢ de_mysql: mysqladmin ping
   â€¢ metabase: curl /api/health
   â€¢ chat_service: curl /health
   â€¢ qdrant: curl /health

Health Endpoints:
   ```python
   @app.get("/health")
   def health():
       return {
           "status": "healthy",
           "trino": test_trino_connection(),
           "qdrant": test_qdrant_connection(),
           "db": test_db_connection()
       }
   ```

Dagster Monitoring:
   â€¢ Web UI: http://localhost:3001
   â€¢ Asset lineage graphs
   â€¢ Run logs & errors
   â€¢ Scheduled job status
   â€¢ Resource usage

Spark Monitoring:
   â€¢ Web UI: http://localhost:8080 (Master)
   â€¢ Worker UI: http://localhost:8081
   â€¢ Job execution history
   â€¢ Task progress & errors

Log Aggregation:
   â€¢ Docker logs: docker-compose logs -f [service]
   â€¢ Dagster logs: dagster_home/logs/
   â€¢ Spark logs: Container stdout/stderr
   â€¢ Chat service: MySQL audit tables

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ›¡ï¸ NETWORK SECURITY:

Internal Network:
   â€¢ Type: bridge
   â€¢ Isolation: All services on de_network
   â€¢ Communication: Service names as hostnames
   â€¢ External: Only exposed ports

Exposed Ports:
   â€¢ 8501: Streamlit (public)
   â€¢ 3000: Metabase (public)
   â€¢ 3001: Dagster (public)
   â€¢ 8080: Spark (public)
   â€¢ 8082: Trino (public)
   â€¢ 9001: MinIO Console (public)
   â€¢ 8001: Chat Service (public)
   
Internal Only:
   â€¢ 3306: MySQL (internal)
   â€¢ 9083: Hive Metastore (internal)
   â€¢ 9000: MinIO API (internal)
   â€¢ 7077: Spark Master (internal)
   â€¢ 6333: Qdrant (internal)

Recommendation (Production):
   â€¢ Add nginx reverse proxy
   â€¢ Enable TLS/SSL
   â€¢ Restrict ports with firewall
   â€¢ Add authentication middleware

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ FILES LIÃŠN QUAN:
   - chat_service/main.py: SQL safety, logging logic
   - chat_service/ast_guard.py: AST-based validation
   - load_dataset_into_mysql/02_init_chatlogs.sql: Audit schema
   - docker-compose.yaml: Health check configs
   - dagster_home/dagster.yaml: Dagster monitoring
   - PROJECT_OVERVIEW.md: Security features overview


================================================================================
ğŸ“Œ 3.6. TIá»‚U Káº¾T CHÆ¯Æ NG
================================================================================

ğŸ“ ÄIá»‚M CHÃNH Cáº¦N TÃ“M Táº®T:

âœ… YÃŠU Cáº¦U Há»† THá»NG:
   â€¢ Hardware: Docker, 8GB+ RAM, 20GB+ disk
   â€¢ Functional: ETL, storage, query, ML, orchestration
   â€¢ Non-functional: Performance, scalability, security, maintainability

âœ… KIáº¾N TRÃšC Tá»”NG THá»‚:
   â€¢ Modern Data Stack: 3 layers (Presentation, Compute, Storage)
   â€¢ Medallion Architecture: Bronze â†’ Silver â†’ Gold â†’ Platinum
   â€¢ 14 Docker services hoáº¡t Ä‘á»™ng trÃªn de_network

âœ… MÃ”I TRÆ¯á»œNG THá»°C NGHIá»†M:
   â€¢ Containerized vá»›i Docker Compose
   â€¢ Automated setup vá»›i setup.sh
   â€¢ Resource: 8GB RAM, 20GB disk (minimum)

âœ… THIáº¾T Láº¬P THÃ€NH PHáº¦N:
   â€¢ MinIO: S3-compatible storage, 4-layer bucket structure
   â€¢ Hive Metastore + Trino: Metadata catalog & query engine
   â€¢ Spark + Delta: ETL processing vá»›i S3A connector
   â€¢ Dagster: Orchestration vá»›i MySQL storage
   â€¢ Metabase: BI tool vá»›i Trino connector

âœ… Báº¢O Máº¬T & QUAN SÃT:
   â€¢ SQL safety: read-only, whitelist, LIMIT, timeout
   â€¢ Audit logging: conversations, SQL, metrics
   â€¢ Health checks cho táº¥t cáº£ services
   â€¢ Network isolation vá»›i Docker bridge

ğŸ¯ THÃ€NH QUáº¢:
   â€¢ Production-ready Data Lakehouse
   â€¢ Automated deployment (one command)
   â€¢ Comprehensive monitoring & security
   â€¢ Scalable & maintainable architecture

ğŸ“Š Káº¾T QUáº¢:
   14 services deployed vÃ  hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh
   â€¢ Storage: MinIO vá»›i 4 layers
   â€¢ Compute: Spark + Trino distributed
   â€¢ Orchestration: Dagster vá»›i scheduling
   â€¢ BI: Metabase & Streamlit dashboards
   â€¢ AI: Chat service vá»›i RAG

================================================================================
ğŸ“ DANH SÃCH FILE Cáº¦N Äá»ŒC Äá»‚ VIáº¾T BÃO CÃO
================================================================================

MUST READ:
   1. docker-compose.yaml (all 14 services)
   2. env.example (environment variables)
   3. dagster_home/dagster.yaml (orchestration config)
   4. dagster_home/workspace.yaml (workspace definition)
   5. docker_image/spark/conf/spark-defaults.conf (Spark config)
   6. trino/etc/catalog/lakehouse.properties (Trino catalog)
   7. docker_image/hive-metastore/metastore-site.xml (Metastore config)
   8. etl_pipeline/etl_pipeline/resources/spark_io_manager.py (IO manager)

NICE TO HAVE:
   9. setup.sh (automated deployment)
   10. download_jars.sh (dependencies)
   11. chat_service/main.py (security & logging)
   12. load_dataset_into_mysql/02_init_chatlogs.sql (audit schema)

================================================================================
EOF


