# ğŸ“¦ DATA LAKEHOUSE - PROJECT FILES DUMP
# Generated: Fri Oct 24 00:02:45 +07 2025
# Complete configuration and code files

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: .env
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•



# ========================
# ğŸ§­ DAGSTER
# ========================
DAGSTER_PG_HOSTNAME=de_psql
DAGSTER_PG_USERNAME=admin
DAGSTER_PG_PASSWORD=admin123
DAGSTER_PG_DB=postgres

# ========================
# ğŸ¬ MYSQL
# ========================
MYSQL_HOST=de_mysql
MYSQL_PORT=3306
MYSQL_DATABASE=brazillian_ecommerce
MYSQL_ROOT_PASSWORD=admin123
MYSQL_USER=admin
MYSQL_PASSWORD=admin123
MB_DB_EXTRA_OPTS="?useSSL=false&allowPublicKeyRetrieval=true"

# ========================
# ğŸ“¦ MINIO (S3 Compatible)
# ========================
MINIO_ENDPOINT=minio:9000
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=minio123
MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=minio123
DATALAKE_BUCKET=warehouse
AWS_ACCESS_KEY_ID=minio
AWS_SECRET_ACCESS_KEY=minio123
AWS_REGION=us-east-1

# ========================
# âš¡ SPARK WORKER
# ========================
SPARK_MODE=worker
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WORKER_MEMORY=8G
SPARK_WORKER_CORES=4
SPARK_RPC_AUTHENTICATION_ENABLED=no
SPARK_RPC_ENCRYPTION_ENABLED=no
SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
SPARK_SSL_ENABLED=no

# ========================
# ğŸ§ª MLFLOW + S3
# ========================
MLFLOW_S3_ENDPOINT_URL=http://minio:9000
AWS_ACCESS_KEY_ID=minio
AWS_SECRET_ACCESS_KEY=minio123

# ========================
# ğŸ HIVE METASTORE (Postgres)
# ========================
HIVE_METASTORE_DB_TYPE=mysql
HIVE_METASTORE_DB_HOST=de_mysql
HIVE_METASTORE_DB_PORT=3306
HIVE_METASTORE_DB_NAME=metastore
HIVE_METASTORE_DB_USER=hive
HIVE_METASTORE_DB_PASS=hive

HIVE_METASTORE_JDBC_URL=jdbc:mysql://de_mysql:3306/metastore
# URI cho Hive Metastore Thrift service
HIVE_METASTORE_URI=thrift://hive-metastore:9083


# ========================
# âš¡ TRINO (Delta Lake + MinIO)
# ========================
# Trino sáº½ dÃ¹ng MinIO nhÆ° S3 backend, nÃªn cáº§n thÃ´ng tin nÃ y Ä‘á»ƒ Ä‘á»c Delta Table
TRINO_S3_ENDPOINT=http://minio:9000
TRINO_S3_REGION=us-east-1
TRINO_S3_ACCESS_KEY=minio
TRINO_S3_SECRET_KEY=minio123
TRINO_HIVE_METASTORE_URI=thrift://hive-metastore:9083
# ğŸ“¦ DAGSTER MYSQL STORAGE
DAGSTER_MYSQL_USERNAME=root
DAGSTER_MYSQL_PASSWORD=admin123
DAGSTER_MYSQL_HOSTNAME=de_mysql
DAGSTER_MYSQL_DB=dagster


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: docker-compose.yaml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

version: "3.9"

services:
  # ========================
  # ğŸ¬ MYSQL (Metastore)
  # ========================
  de_mysql:
    image: mysql:8.0
    container_name: de_mysql
    command: --default-authentication-plugin=mysql_native_password
    volumes:
      - ./mysql:/var/lib/mysql
      - ./load_dataset_into_mysql:/docker-entrypoint-initdb.d:ro
    ports:
      - "3306:3306"
    env_file:
      - .env
    networks:
      - de_network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "de_mysql", "-u$$MYSQL_USER", "-p$$MYSQL_PASSWORD"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ========================
  # ğŸ HIVE METASTORE
  # ========================
  hive-metastore:
    container_name: hive-metastore
    build:
      context: ./docker_image/hive-metastore
      dockerfile: Dockerfile
    depends_on:
      de_mysql:
        condition: service_healthy
    environment:
      - DB_TYPE=mysql
      - DB_HOST=de_mysql
      - DB_PORT=3306
      - DB_NAME=metastore
      - DB_USER=hive
      - DB_PASS=hive
      - HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
    volumes:
      - ./docker_image/hive-metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    ports:
      - "9083:9083"
    networks:
      - de_network

  # ========================
  # ğŸª£ MINIO (S3)
  # ========================
  minio:
    image: minio/minio
    container_name: minio
    command: ["server", "/data", "--console-address", ":9001"]
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio:/data
    env_file:
      - .env
    networks:
      - de_network

  # MinIO Client (setup buckets)
  mc:
    image: minio/mc
    container_name: mc
    depends_on:
      - minio
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
        until (/usr/bin/mc config host add minio http://minio:9000 minio minio123)
        do echo '...waiting...' && sleep 1; done;
        /usr/bin/mc mb minio/warehouse || true;
        /usr/bin/mc mb minio/lakehouse || true;
        /usr/bin/mc policy set public minio/warehouse;
        /usr/bin/mc policy set public minio/lakehouse;
        exit 0;
      "
    networks:
      - de_network

  # ========================
  # âš¡ TRINO (Delta Lake)
  # ========================
  trino:
    image: trinodb/trino:414
    container_name: trino
    ports:
      - "8082:8080"   # trÃ¡nh xung Ä‘á»™t vá»›i Spark Master
    volumes:
      - ./trino/etc:/etc/trino
    depends_on:
      - hive-metastore
      - minio
    networks:
      - de_network

  # ========================
  # ğŸ“Š METABASE
  # ========================
  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    platform: linux/amd64
    hostname: metabase
    ports:
      - "3000:3000"
    environment:
      # Metabase application database (SQLite for simplicity)
      MB_DB_TYPE: h2
      MB_DB_FILE: /metabase-data/metabase.db
      # Trino connection will be configured via UI
    depends_on:
      - trino
      - hive-metastore
    volumes:
      - metabase_data:/metabase-data
    networks:
      - de_network
    healthcheck:
      test: ["CMD", "curl", "--fail", "-I", "http://localhost:3000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 5

  # ========================
  # ğŸ”¥ SPARK CLUSTER
  # ========================
  spark-master:
    image: bitnami/spark:3.3.2
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_CLASSPATH=/opt/jars/*
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./jars:/opt/jars
      - ./docker_image/spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - de_network

  spark-worker-1:
    image: bitnami/spark:3.3.2
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_CLASSPATH=/opt/jars/*
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - ./jars:/opt/jars
      - ./docker_image/spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - de_network
  # Streamlit
  streamlit:
    container_name: streamlit
    build:
      context: ./docker_image/streamlit
      dockerfile: Dockerfile
    image: streamlit:latest
    ports:
      - "8501:8501"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
    volumes:
      - ./app:/app
    networks:
      - de_network 

  # ========================
  # ğŸ”„ ETL PIPELINE
  # ========================
  etl_pipeline:
    build:
      context: ./etl_pipeline
      dockerfile: Dockerfile
    container_name: etl_pipeline
    ports:
      - "4000:4000"
    volumes:
      - ./jars:/opt/jars
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    depends_on:
      - spark-master
      - minio
      - de_mysql
    networks:
      - de_network

  # ========================
  # ğŸ§­ DAGSTER
  # ========================
  de_dagster:
    build:
      context: ./dagster/
    container_name: de_dagster
    ports:
      - "5001:5000"
    env_file:
      - .env
    depends_on:
      - de_mysql
    networks:
      - de_network

  de_dagster_dagit:
    build:
      context: ./dagster/
    depends_on:
      - de_dagster
      - etl_pipeline
    entrypoint: ["dagster-webserver"]
    command: ["-h", "0.0.0.0", "-p", "3001", "-w", "/opt/dagster/dagster_home/workspace.yaml"]
    container_name: de_dagster_dagit
    ports:
      - "3001:3001"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    networks:
      - de_network

  de_dagster_daemon:
    build:
      context: ./dagster/
    depends_on:
      - de_dagster
    entrypoint:
      - dagster-daemon
      - run
    container_name: de_dagster_daemon
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dagster_home:/opt/dagster/dagster_home
    env_file:
      - .env
    networks:
      - de_network

volumes:
  metabase_data:

networks:
  de_network:
    driver: bridge

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: Makefile
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Load environment variables
include .env
export $(shell sed 's/=.*//' .env)

# Build all services
build:
	docker compose --env-file .env build

# Start all services (detached)
up:
	docker compose --env-file .env up -d

# Stop and remove all services
down:
	docker compose --env-file .env down

# Remove containers, networks, volumes (use with caution)
down-clean:
	docker compose --env-file .env down -v

# Restart (stop -> up)
restart: down up

# Show logs (follow)
logs:
	docker compose --env-file .env logs -f

# Rebuild and restart everything
rebuild:
	docker compose --env-file .env build --no-cache
	docker compose --env-file .env up -d --force-recreate

# Connect to Dagster MySQL database
to_dagster_mysql:
	docker exec -it de_mysql \
		mysql --local-infile=1 -u"${DAGSTER_MYSQL_USERNAME}" -p"${DAGSTER_MYSQL_PASSWORD}" ${DAGSTER_MYSQL_DB}

# Connect to MySQL (user)
to_mysql:
	docker exec -it de_mysql \
		mysql --local-infile=1 -u"${MYSQL_USER}" -p"${MYSQL_PASSWORD}" ${MYSQL_DATABASE}

# Connect to MySQL (root)
to_mysql_root:
	docker exec -it de_mysql \
		mysql --local-infile=1 -u"root" -p"${MYSQL_ROOT_PASSWORD}" ${MYSQL_DATABASE}

# Shell into containers
to_etl:
	docker exec -it etl_pipeline bash

to_dagster:
	docker exec -it de_dagster bash

# Run ETL jobs manually (náº¿u báº¡n Ä‘Ã£ define job/commands trong container)
etl_bronze:
	docker exec -it etl_pipeline dagster job execute -m etl_pipeline.job.reload_data

# Full pipeline
etl_all: etl_bronze
	@echo "âœ… ETL pipeline executed (requested jobs)."

run_mysql_to_minio:
	docker exec -it spark-master \
		spark-submit \
		--packages com.mysql:mysql-connector-j:8.0.33 \
		--conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
		--conf spark.hadoop.fs.s3a.access.key=minio \
		--conf spark.hadoop.fs.s3a.secret.key=minio123 \
		--conf spark.hadoop.fs.s3a.path.style.access=true \
		/opt/dagster/app/etl_pipeline/job/mysql_to_minio.py

run_mysql_to_minio_all:
	docker exec -it spark-master \
		spark-submit \
		--packages com.mysql:mysql-connector-j:8.0.33 \
		--conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
		--conf spark.hadoop.fs.s3a.access.key=minio \
		--conf spark.hadoop.fs.s3a.secret.key=minio123 \
		--conf spark.hadoop.fs.s3a.path.style.access=true \
		/opt/dagster/app/etl_pipeline/job/mysql_to_minio.py

create_bucket:
	docker exec -it minio \
		mc alias set local http://127.0.0.1:9000 minio minio123 && \
		mc mb local/lakehouse || true

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: .gitignore
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Environment variables
.env
.env.local
.env.*.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual environments
venv/
env/
ENV/
env.bak/
venv.bak/
myenv/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb_checkpoints/

# Data directories
mysql/
postgresql/
minio/
dagster_home/storage/
dagster_home/logs/
dagster_home/history/

# JAR files (too large for git)
jars/*.jar
etl_pipeline/jars/*.jar

# CSV data files
brazilian-ecommerce/*.csv
*.csv

# Logs
*.log
logs/
*.out

# Docker
docker-compose.override.yml

# Dagster
.dagster/
dagster_home/storage/
dagster_home/schedules/

# Build artifacts
*.egg
*.egg-info/
dist/
build/


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: rebuild.sh
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#!/bin/bash

echo "ğŸ”§ Rebuilding Data Lakehouse after critical fixes..."
echo "=================================================="

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Step 1: Stop all services
echo -e "${YELLOW}Step 1: Stopping all services...${NC}"
docker-compose down

# Step 2: Rebuild affected images
echo -e "${YELLOW}Step 2: Rebuilding Docker images...${NC}"
docker-compose build --no-cache dagster etl_pipeline streamlit

# Step 3: Start services in order
echo -e "${YELLOW}Step 3: Starting services...${NC}"

# Start MySQL first
echo "Starting MySQL..."
docker-compose up -d de_mysql

# Wait for MySQL
echo "Waiting for MySQL to be ready..."
sleep 30

# Start MinIO
echo "Starting MinIO..."
docker-compose up -d minio mc

# Wait a bit
sleep 10

# Start Hive Metastore
echo "Starting Hive Metastore..."
docker-compose up -d hive-metastore

# Wait for Metastore
sleep 20

# Start Spark cluster
echo "Starting Spark cluster..."
docker-compose up -d spark-master spark-worker-1

# Wait for Spark
sleep 15

# Start ETL pipeline
echo "Starting ETL pipeline..."
docker-compose up -d etl_pipeline

# Wait for ETL
sleep 10

# Start Dagster
echo "Starting Dagster..."
docker-compose up -d de_dagster de_dagster_dagit de_dagster_daemon

# Wait for Dagster
sleep 15

# Start Trino and Metabase
echo "Starting Trino and Metabase..."
docker-compose up -d trino metabase

# Start Streamlit
echo "Starting Streamlit..."
docker-compose up -d streamlit

# Start Jupyter
echo "Starting Jupyter..."
docker-compose up -d spark-notebook

# Step 4: Show status
echo -e "${YELLOW}Step 4: Checking service status...${NC}"
docker-compose ps

# Step 5: Verify critical fixes
echo ""
echo -e "${GREEN}================================================${NC}"
echo -e "${GREEN}Verifying critical fixes...${NC}"
echo -e "${GREEN}================================================${NC}"

echo ""
echo "1. Checking Dagster version..."
docker exec de_dagster pip show dagster | grep Version

echo ""
echo "2. Checking MySQL connection..."
docker exec de_mysql mysql -uhive -phive -e "SHOW DATABASES;" 2>/dev/null && echo "âœ… MySQL connection OK" || echo "âŒ MySQL connection FAILED"

echo ""
echo "3. Checking ETL pipeline container..."
docker exec etl_pipeline python -c "import dagster; print('âœ… ETL pipeline Python OK')" 2>/dev/null || echo "âŒ ETL pipeline check FAILED"

echo ""
echo -e "${GREEN}================================================${NC}"
echo -e "${GREEN}Rebuild Complete!${NC}"
echo -e "${GREEN}================================================${NC}"
echo ""
echo "Access URLs:"
echo "  - Dagster UI:     http://localhost:3001"
echo "  - Metabase:       http://localhost:3000"
echo "  - Spark Master:   http://localhost:8080"
echo "  - MinIO Console:  http://localhost:9001"
echo "  - Streamlit:      http://localhost:8501"
echo "  - Trino:          http://localhost:8082"
echo ""
echo "Next steps:"
echo "  1. Open Dagster UI: http://localhost:3001"
echo "  2. Materialize Bronze layer assets"
echo "  3. Then Silver â†’ Gold â†’ Platinum"
echo "  4. Verify in Metabase: http://localhost:3000"
echo ""



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: run_etl.sh
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#!/bin/bash
set -e

echo "ğŸš€ Starting ETL Pipeline..."
echo ""

# Wait for Dagster to be ready
echo "â³ Waiting for Dagster to be ready..."
for i in {1..30}; do
    if docker exec etl_pipeline dagster instance info > /dev/null 2>&1; then
        echo "âœ… Dagster is ready!"
        break
    fi
    echo "  Waiting... ($i/30)"
    sleep 2
done

echo ""
echo "ğŸ“Š Running Bronze Layer ETL (MySQL â†’ MinIO)..."
docker exec etl_pipeline dagster job execute -m etl_pipeline -j reload_data

echo ""
echo "âœ… ETL Complete! Verifying data in MinIO..."
echo ""
echo "ğŸ“ Access Points:"
echo "  â€¢ Dagster UI:     http://localhost:3001"
echo "  â€¢ MinIO Console:  http://localhost:9001 (minio/minio123)"
echo "  â€¢ Trino:          http://localhost:8082"
echo "  â€¢ Metabase:       http://localhost:3000"
echo "  â€¢ Streamlit:      http://localhost:8501"
echo ""
echo "ğŸ” To verify data in Trino, run:"
echo "  docker exec -it trino trino"
echo "  USE lakehouse.bronze;"
echo "  SHOW TABLES;"
echo "  SELECT COUNT(*) FROM customer;"



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: full_setup.sh
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#!/bin/bash
set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘   Data Lakehouse - Full Setup & ETL Pipeline              â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""

# Step 1: Clean environment
echo -e "${YELLOW}[1/8] Cleaning environment...${NC}"
docker-compose down -v 2>/dev/null || true
echo -e "${GREEN}âœ“ Environment cleaned${NC}"
echo ""

# Step 2: Create .env if not exists
echo -e "${YELLOW}[2/8] Setting up environment variables...${NC}"
if [ ! -f .env ]; then
    cp env.example .env
    echo -e "${GREEN}âœ“ Created .env from env.example${NC}"
else
    echo -e "${GREEN}âœ“ .env already exists${NC}"
fi
echo ""

# Step 3: Build images
echo -e "${YELLOW}[3/8] Building Docker images (this may take 5-10 minutes)...${NC}"
docker-compose build
echo -e "${GREEN}âœ“ Images built successfully${NC}"
echo ""

# Step 4: Start services
echo -e "${YELLOW}[4/8] Starting services...${NC}"
docker-compose up -d
echo -e "${GREEN}âœ“ Services started${NC}"
echo ""

# Step 5: Wait for MySQL
echo -e "${YELLOW}[5/8] Waiting for MySQL to be ready...${NC}"
sleep 30
echo -e "${GREEN}âœ“ MySQL should be ready${NC}"
echo ""

# Step 6: Load data into MySQL
echo -e "${YELLOW}[6/8] Loading dataset into MySQL...${NC}"

# Check if dataset exists
if [ ! -d "brazilian-ecommerce" ]; then
    echo -e "${RED}âœ— Error: brazilian-ecommerce/ directory not found!${NC}"
    echo -e "${YELLOW}Please download the dataset from Kaggle:${NC}"
    echo -e "${YELLOW}https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce${NC}"
    exit 1
fi

# Copy dataset to MySQL container
echo "  â†’ Copying dataset files to MySQL container..."
docker cp brazilian-ecommerce/ de_mysql:/tmp/dataset/ 2>/dev/null || {
    echo -e "${YELLOW}  âš  Retrying copy dataset...${NC}"
    sleep 5
    docker cp brazilian-ecommerce/ de_mysql:/tmp/dataset/
}

# Copy SQL scripts
echo "  â†’ Copying SQL scripts..."
docker cp load_dataset_into_mysql/01_olist.sql de_mysql:/tmp/olist.sql
docker cp load_dataset_into_mysql/load_data.sql de_mysql:/tmp/load_data.sql

# Execute SQL scripts
echo "  â†’ Creating database schema..."
docker exec de_mysql mysql -uroot -proot123 -e "SET GLOBAL local_infile=1;" 2>/dev/null || true
docker exec de_mysql mysql -uroot -proot123 -e "CREATE DATABASE IF NOT EXISTS brazillian_ecommerce;" 2>/dev/null
docker exec de_mysql bash -c "mysql -uroot -proot123 brazillian_ecommerce < /tmp/olist.sql" 2>/dev/null

echo "  â†’ Loading CSV data (this may take 2-3 minutes)..."
docker exec de_mysql bash -c "mysql -uroot -proot123 --local-infile=1 brazillian_ecommerce < /tmp/load_data.sql" 2>/dev/null

echo -e "${GREEN}âœ“ Data loaded into MySQL${NC}"
echo ""

# Step 7: Verify MySQL data
echo -e "${YELLOW}[7/8] Verifying MySQL data...${NC}"
CUSTOMER_COUNT=$(docker exec de_mysql mysql -uroot -proot123 -sN -e "SELECT COUNT(*) FROM brazillian_ecommerce.customers;" 2>/dev/null)
ORDER_COUNT=$(docker exec de_mysql mysql -uroot -proot123 -sN -e "SELECT COUNT(*) FROM brazillian_ecommerce.orders;" 2>/dev/null)

if [ -z "$CUSTOMER_COUNT" ] || [ "$CUSTOMER_COUNT" -eq 0 ]; then
    echo -e "${RED}âœ— MySQL data verification failed!${NC}"
    exit 1
fi

echo -e "${GREEN}âœ“ MySQL data verified:${NC}"
echo "  â€¢ Customers: $CUSTOMER_COUNT"
echo "  â€¢ Orders: $ORDER_COUNT"
echo ""

# Step 8: Summary
echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘   Setup Complete! Next Steps:                             â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${GREEN}[8/8] System is ready!${NC}"
echo ""
echo -e "${YELLOW}Access URLs:${NC}"
echo "  â€¢ Dagster UI:     ${BLUE}http://localhost:3001${NC}"
echo "  â€¢ Metabase:       ${BLUE}http://localhost:3000${NC}"
echo "  â€¢ Spark Master:   ${BLUE}http://localhost:8080${NC}"
echo "  â€¢ MinIO Console:  ${BLUE}http://localhost:9001${NC} (minio/minio123)"
echo "  â€¢ Streamlit:      ${BLUE}http://localhost:8501${NC}"
echo "  â€¢ Trino:          ${BLUE}http://localhost:8082${NC}"
echo ""
echo -e "${YELLOW}To run ETL pipeline:${NC}"
echo ""
echo "  Option 1 - Via Dagster UI (Recommended):"
echo "    1. Open ${BLUE}http://localhost:3001${NC}"
echo "    2. Go to 'Jobs' â†’ 'full_pipeline_job'"
echo "    3. Click 'Launch Run'"
echo ""
echo "  Option 2 - Via Command Line:"
echo "    ${GREEN}make etl_bronze${NC}  # Run Bronze layer only"
echo ""
echo -e "${YELLOW}To verify data in Trino:${NC}"
echo "    ${GREEN}docker exec -it trino trino${NC}"
echo "    ${GREEN}SHOW CATALOGS;${NC}"
echo "    ${GREEN}USE lakehouse.bronze;${NC}"
echo "    ${GREEN}SHOW TABLES;${NC}"
echo ""
echo -e "${GREEN}Happy Data Engineering! ğŸ‰${NC}"



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: env.example
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ===========================================
# Modern Data Stack - Environment Variables
# ===========================================

# ===========================================
# ğŸ¬ MySQL Configuration
# ===========================================
MYSQL_ROOT_PASSWORD=root123
MYSQL_DATABASE=metastore
MYSQL_USER=hive
MYSQL_PASSWORD=hive

# ===========================================
# ğŸ—„ï¸ Dagster MySQL Configuration
# ===========================================
DAGSTER_MYSQL_HOSTNAME=de_mysql
DAGSTER_MYSQL_DB=dagster
DAGSTER_MYSQL_USERNAME=dagster
DAGSTER_MYSQL_PASSWORD=dagster123

# ===========================================
# ğŸª£ MinIO Configuration
# ===========================================
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=minio123

# ===========================================
# ğŸ”§ Optional: Custom Ports
# ===========================================
# SPARK_MASTER_PORT=7077
# SPARK_WORKER_PORT=8081
# TRINO_PORT=8082
# METABASE_PORT=3000
# DAGSTER_PORT=3001


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: dagster_home/dagster.yaml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator
  config:
    max_concurrent_runs: 3

scheduler:
  module: dagster.core.scheduler
  class: DagsterDaemonScheduler
  config:
    max_catchup_runs: 5

storage:
  mysql:
    mysql_db:
      username:
        env: DAGSTER_MYSQL_USERNAME
      password:
        env: DAGSTER_MYSQL_PASSWORD
      hostname:
        env: DAGSTER_MYSQL_HOSTNAME
      db_name:
        env: DAGSTER_MYSQL_DB
      port: 3306

run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher

compute_logs:
  module: dagster.core.storage.local_compute_log_manager
  class: LocalComputeLogManager
  config:
    base_dir: /opt/dagster/dagster_home/compute_logs

local_artifact_storage:
  module: dagster.core.storage.root
  class: LocalArtifactStorage
  config:
    base_dir: /opt/dagster/dagster_home/local_artifact_storage



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: dagster_home/workspace.yaml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

load_from:
  - grpc_server:
      host: etl_pipeline
      port: 4000
      location_name: "etl_pipeline"


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: dagster/requirements.txt
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

dagster==1.9.3
dagit==1.9.3
dagster-mysql==0.25.3

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/requirements.txt
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Main Dagster Libraries
dagster==1.9.3
dagit==1.9.3
humanfriendly==9.2

# Additional Dagster Integrations
dagster-mysql==0.25.3
dagster-aws==0.25.3
dagster-spark==0.25.3

# Hive Integration
PyHive==0.6.5

# Data Processing Libraries
numpy==1.24.3
pandas==2.0.3
polars==0.20.25
pyspark==3.3.2
pyarrow==12.0.1
findspark==2.0.0

# Database and ORM
SQLAlchemy==1.4.46
pymysql==1.0.2

# Cryptography
cryptography==38.0.3

# Filesystem and S3
fsspec==2023.3.0
minio==7.1.13
s3fs==2023.3.0
boto3==1.24.20
aiobotocore==2.4.2
botocore==1.27.59

# Delta Lake
delta-spark==2.3.0

# Testing (optional, can be installed separately for development)
# pytest==7.2.2
# pylint==2.17.1
# pytest-cov==4.0.0
# autopep8==2.0.2


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/setup.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from setuptools import find_packages, setup

setup(
    name="etl_pipeline",
    packages=find_packages(exclude=["etl_pipeline_tests"]),
    install_requires=[
        "dagster",
        "dagster-cloud"
    ],
    extras_require={"dev": ["dagster-webserver", "pytest"]},
)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/__init__.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
from dagster import Definitions, load_assets_from_modules

try:
    from dagstermill import ConfigurableLocalOutputNotebookIOManager
    NOTEBOOK_IO_MANAGER_AVAILABLE = True
except ImportError:
    ConfigurableLocalOutputNotebookIOManager = None
    NOTEBOOK_IO_MANAGER_AVAILABLE = False

from .assets import bronze, silver, gold, platinum
from .job import reload_data, full_pipeline_job
from .schedule import  reload_data_schedule
from .resources.minio_io_manager import MinioIOManager
from .resources.mysql_io_manager import MysqlIOManager
from .resources.spark_io_manager import SparkIOManager


MYSQL_CONFIG = {
    "host": os.getenv("MYSQL_HOST", "de_mysql"),
    "port": int(os.getenv("MYSQL_PORT", "3306")),
    "database": os.getenv("MYSQL_DATABASE", "brazillian_ecommerce"),
    "user": os.getenv("MYSQL_USER", "hive"),
    "password": os.getenv("MYSQL_PASSWORD", "hive"),
}


MINIO_CONFIG = {
    "endpoint_url": os.getenv("MINIO_ENDPOINT", "http://minio:9000"),
    "minio_access_key": os.getenv("MINIO_ACCESS_KEY", "minio"),
    "minio_secret_key": os.getenv("MINIO_SECRET_KEY", "minio123"),
    "bucket": os.getenv("DATALAKE_BUCKET", "warehouse"),
}

SPARK_CONFIG = {
    "spark_master": os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077"),
    "endpoint_url": os.getenv("MINIO_ENDPOINT", "http://minio:9000"),
    "minio_access_key": os.getenv("MINIO_ACCESS_KEY", "minio"),
    "minio_secret_key": os.getenv("MINIO_SECRET_KEY", "minio123"),
}

resources = {
    "mysql_io_manager": MysqlIOManager(MYSQL_CONFIG),
    "minio_io_manager": MinioIOManager(MINIO_CONFIG),
    "spark_io_manager": SparkIOManager(SPARK_CONFIG),
}

if NOTEBOOK_IO_MANAGER_AVAILABLE:
    resources["output_notebook_io_manager"] = ConfigurableLocalOutputNotebookIOManager()

# Load assets from each layer separately
bronze_layer_assets = load_assets_from_modules([bronze])
silver_layer_assets = load_assets_from_modules([silver])
gold_layer_assets = load_assets_from_modules([gold])
platinum_layer_assets = load_assets_from_modules([platinum])

defs = Definitions(
    assets=bronze_layer_assets
    + silver_layer_assets
    + gold_layer_assets
    + platinum_layer_assets,
    jobs=[reload_data, full_pipeline_job],
    schedules=[reload_data_schedule],
    resources=resources,
)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/resources/spark_io_manager.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import IOManager, InputContext, OutputContext
from pyspark.sql import SparkSession, DataFrame
import os


class SparkIOManager(IOManager):
    def __init__(self, config=None):
        # Ensure endpoint URL has proper protocol
        endpoint_url = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
        if not endpoint_url.startswith("http"):
            endpoint_url = f"http://{endpoint_url}"
            
        default_config = {
            "endpoint_url": endpoint_url,
            "minio_access_key": os.getenv("MINIO_ACCESS_KEY", "minio"),
            "minio_secret_key": os.getenv("MINIO_SECRET_KEY", "minio123"),
            "jdbc_jar_path": os.getenv("JDBC_JAR_PATH", "/opt/jars/mysql-connector-j-8.0.33.jar"),
            "jars_dir": os.getenv("JARS_DIR", "/opt/jars"),
            "warehouse": os.getenv("SPARK_WAREHOUSE", "s3a://lakehouse/"),
            "master": os.getenv("SPARK_MASTER", "spark://spark-master:7077"),
        }
        self._config = {**default_config, **(config or {})}
        self._spark = None

    def _get_spark(self) -> SparkSession:
        if self._spark is None:
            jdbc_jar = self._config["jdbc_jar_path"]
            jars_dir = self._config["jars_dir"]
            
            # Validate required JARs exist
            required_jars = [
                ("JDBC jar", jdbc_jar),
                ("Delta Core", os.path.join(jars_dir, "delta-core_2.12-2.3.0.jar")),
                ("Delta Storage", os.path.join(jars_dir, "delta-storage-2.3.0.jar")),
                ("Hadoop AWS", os.path.join(jars_dir, "hadoop-aws-3.3.2.jar")),
                ("AWS SDK Bundle", os.path.join(jars_dir, "aws-java-sdk-bundle-1.11.1026.jar")),
            ]
            
            for jar_name, jar_path in required_jars:
                if not os.path.exists(jar_path):
                    raise FileNotFoundError(f"Missing {jar_name}: {jar_path}")

            spark_jars = ",".join([
                f"file://{jdbc_jar}",
                f"file://{jars_dir}/delta-core_2.12-2.3.0.jar",
                f"file://{jars_dir}/delta-storage-2.3.0.jar",
                f"file://{jars_dir}/hadoop-aws-3.3.2.jar",
                f"file://{jars_dir}/aws-java-sdk-bundle-1.11.1026.jar",
            ])

            builder = (
                SparkSession.builder
                .master(self._config["master"])
                .appName("Dagster SparkIOManager")
                .config("spark.jars", spark_jars)
                .config("spark.driver.extraClassPath", jdbc_jar)
                .config("spark.executor.extraClassPath", jdbc_jar)
                .config("spark.driver.userClassPathFirst", "true")
                .config("spark.executor.userClassPathFirst", "true")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .config("spark.hadoop.fs.s3a.endpoint", self._config["endpoint_url"])
                .config("spark.hadoop.fs.s3a.access.key", self._config["minio_access_key"])
                .config("spark.hadoop.fs.s3a.secret.key", self._config["minio_secret_key"])
                .config("spark.hadoop.fs.s3a.path.style.access", "true")
                .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                # Map s3:// to s3a:// FileSystem (fix "No FileSystem for scheme s3")
                .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
                .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
                .config("spark.sql.warehouse.dir", self._config["warehouse"])
                # Hive Metastore configuration
                .config("hive.metastore.uris", "thrift://hive-metastore:9083")
                .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083")
                .config("spark.sql.catalogImplementation", "hive")
                .enableHiveSupport()
            )

            self._spark = builder.getOrCreate()
        return self._spark

    def handle_output(self, context: OutputContext, obj: DataFrame):
        full_path = context.asset_key.path
        if len(full_path) < 2:
            raise ValueError("Unexpected asset_key.path: " + str(full_path))

        layer = full_path[0]       # vÃ­ dá»¥: "bronze"
        table = full_path[-1]      # vÃ­ dá»¥: "customer"
        
        # Use s3a:// for data writing (Spark/Hadoop compatible)
        path_s3a = f"{self._config['warehouse']}{layer}/{table}"      # s3a://lakehouse/layer/table
        # Use s3:// for metadata registration (Trino compatible)
        path_s3 = path_s3a.replace("s3a://", "s3://", 1)             # s3://lakehouse/layer/table
        db_loc_s3 = f"{self._config['warehouse'].replace('s3a://','s3://',1)}{layer}"

        spark = self._get_spark()
        context.log.info(f"(SparkIOManager) Saving table: {layer}.{table} -> {path_s3a}")

        try:
            # 1) Ghi file Delta váº­t lÃ½ ra MinIO (s3a)
            row_count = obj.count()
            obj.write.format("delta").mode("overwrite").save(path_s3a)
            context.log.info(f"Successfully wrote {row_count} rows to {path_s3a}")

            # 2) ÄÄƒng kÃ½ metadata vÃ o Hive Metastore vá»›i LOCATION dÃ¹ng 's3://'
            try:
                # Create database with s3:// location
                spark.sql(f"CREATE DATABASE IF NOT EXISTS {layer} LOCATION '{db_loc_s3}'")
                
                # Drop existing table if any (to avoid s3a:// vs s3:// conflicts)
                spark.sql(f"DROP TABLE IF EXISTS {layer}.{table}")
                
                # Create table with s3:// location for Trino compatibility
                spark.sql(f"CREATE TABLE {layer}.{table} USING DELTA LOCATION '{path_s3}'")
                context.log.info(f"Successfully registered {layer}.{table} in Hive Metastore with s3:// location")
                
            except Exception as e:
                context.log.warning(f"Failed to register in Hive Metastore: {e}")
                # Fallback
                try:
                    spark.sql(f"CREATE DATABASE IF NOT EXISTS {layer}")
                    obj.write.format("delta").mode("overwrite").option("path", path_s3a).saveAsTable(f"{layer}.{table}")
                    context.log.info(f"Used fallback registration method for {layer}.{table}")
                except Exception as fallback_error:
                    context.log.error(f"Fallback registration also failed: {fallback_error}")
                    raise

            context.add_output_metadata({
                "path_data": path_s3a,
                "path_registered": path_s3,
                "rows": row_count,
                "columns": obj.columns,
            })
        except Exception as e:
            context.log.error(f"Failed to save table {layer}.{table}: {str(e)}")
            raise

    def load_input(self, context: InputContext) -> DataFrame:
        full_path = context.asset_key.path
        if len(full_path) < 2:
            raise ValueError("Unexpected asset_key.path: " + str(full_path))

        layer = full_path[0]
        table = full_path[-1]
        path = f"{self._config['warehouse']}{layer}/{table}"

        spark = self._get_spark()
        context.log.info(f"(SparkIOManager) Loading table: {layer}.{table} from {path}")

        try:
            df = spark.read.format("delta").load(path)
            row_count = df.count()
            context.log.info(f"Successfully loaded {row_count} rows from {path}")
            return df
        except Exception as e:
            context.log.error(f"(SparkIOManager) Failed to load {path}, error: {e}")
            raise

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/resources/minio_io_manager.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import IOManager, InputContext, OutputContext
from minio import Minio
import polars as pl
import pandas as pd
from contextlib import contextmanager
from datetime import datetime
from typing import Union
import os
import pyarrow as pa
import pyarrow.parquet as pq

@contextmanager
def connect_minio(config):
    endpoint = config.get("endpoint_url") or os.getenv("MINIO_ENDPOINT")
    access_key = config.get("minio_access_key") or config.get("access_key") or os.getenv("MINIO_ACCESS_KEY")
    secret_key = config.get("minio_secret_key") or config.get("secret_key") or os.getenv("MINIO_SECRET_KEY")

    # Minio client expects host:port (no http://). Strip if user provided full URL.
    if endpoint.startswith("http://") or endpoint.startswith("https://"):
        endpoint = endpoint.split("//", 1)[1]

    client = Minio(endpoint, access_key=access_key, secret_key=secret_key, secure=False)
    try:
        yield client
    finally:
        pass


def make_bucket(client: Minio, bucket_name: str):
    if not bucket_name:
        raise ValueError("Bucket name not configured for MinioIOManager")
    if not client.bucket_exists(bucket_name):
        client.make_bucket(bucket_name)

class MinioIOManager(IOManager):
    def __init__(self, config=None):
        # cháº¥p nháº­n cáº£ 'bucket' hoáº·c 'bucket_name' vÃ  cáº£ access key tÃªn khÃ¡c nhau
        self._config = config or {
            "endpoint_url": os.getenv("MINIO_ENDPOINT", "minio:9000"),
            "minio_access_key": os.getenv("MINIO_ACCESS_KEY", "minio"),
            "minio_secret_key": os.getenv("MINIO_SECRET_KEY", "minio123"),
            "bucket": os.getenv("DATALAKE_BUCKET", "warehouse"),
        }

    def _get_path(self, context: Union[InputContext, OutputContext]):
        layer, schema, table = context.asset_key.path
        key = "/".join([layer, schema, table.replace(f"{layer}_", "")])
        tmp_file_path = "/tmp/file_{}_{}.parquet".format("_".join(context.asset_key.path), datetime.today().strftime("%Y%m%d%H%M%S"))
        if context.has_partition_key:
            partition_str = str(table) + "_" + context.asset_partition_key
            return os.path.join(key, f"{partition_str}.parquet"), tmp_file_path
        else:
            return f"{key}.parquet", tmp_file_path

    def handle_output(self, context: OutputContext, obj):
        key_name, tmp_file_path = self._get_path(context)

        if isinstance(obj, pl.DataFrame):
            table = pa.Table.from_pandas(obj.to_pandas())
        elif isinstance(obj, pd.DataFrame):
            table = pa.Table.from_pandas(obj)
        else:
            raise ValueError("Unsupported DataFrame type for Parquet conversion.")

        pq.write_table(table, tmp_file_path)

        try:
            bucket_name = self._config.get("bucket") or self._config.get("bucket_name")
            with connect_minio(self._config) as client:
                make_bucket(client, bucket_name)
                client.fput_object(bucket_name, key_name, tmp_file_path)
                os.remove(tmp_file_path)
        except Exception as e:
            raise e

    def load_input(self, context: InputContext):
        bucket_name = self._config.get("bucket") or self._config.get("bucket_name")
        key_name, tmp_file_path = self._get_path(context)

        try:
            with connect_minio(self._config) as client:
                make_bucket(client, bucket_name)
                context.log.info(f"(MinIO load_input) from key_name: {key_name}")
                client.fget_object(bucket_name, key_name, tmp_file_path)
                df_data = pl.read_parquet(tmp_file_path)
                context.log.info(f"(MinIO load_input) Got Polars DataFrame with shape: {df_data.shape}")
                os.remove(tmp_file_path)
                return df_data
        except Exception as e:
            raise e

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/assets/bronze.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import asset, Output
from pyspark.sql import DataFrame

COMPUTE_KIND = "SparkSQL"
LAYER = "bronze"


def log_shape(context, df: DataFrame, name: str):
    context.log.info(f"{name} rows={df.count()} cols={len(df.columns)}")


def load_mysql_table(context, table_name: str) -> DataFrame:
    import os
    spark = context.resources.spark_io_manager._get_spark()

    jdbc_url = (
        "jdbc:mysql://de_mysql:3306/brazillian_ecommerce"
        "?zeroDateTimeBehavior=CONVERT_TO_NULL"
        "&serverTimezone=UTC"
        "&useSSL=false"
        "&allowPublicKeyRetrieval=true"
    )

    # Use environment variables for credentials
    mysql_user = os.getenv("MYSQL_USER", "root")
    # If using root, get ROOT_PASSWORD; otherwise get MYSQL_PASSWORD
    if mysql_user == "root":
        mysql_password = os.getenv("MYSQL_ROOT_PASSWORD", "root123")
    else:
        mysql_password = os.getenv("MYSQL_PASSWORD", "hive")

    df = spark.read.format("jdbc").options(
        url=jdbc_url,
        driver="com.mysql.cj.jdbc.Driver",
        dbtable=table_name,
        user=mysql_user,
        password=mysql_password,
    ).load()

    log_shape(context, df, f"bronze_{table_name}")
    return df


@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "customer"], group_name=LAYER)
def bronze_customer(context):
    return Output(load_mysql_table(context, "customers"))   # MySQL: customers â†’ asset/table: bronze.customer

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "seller"], group_name=LAYER)
def bronze_seller(context):
    return Output(load_mysql_table(context, "sellers"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "product"], group_name=LAYER)
def bronze_product(context):
    return Output(load_mysql_table(context, "products"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "order"], group_name=LAYER)
def bronze_order(context):
    return Output(load_mysql_table(context, "orders"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "orderitem"], group_name=LAYER)
def bronze_order_item(context):
    return Output(load_mysql_table(context, "order_items"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "payment"], group_name=LAYER)
def bronze_payment(context):
    return Output(load_mysql_table(context, "order_payments"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "orderreview"], group_name=LAYER)
def bronze_order_review(context):
    return Output(load_mysql_table(context, "order_reviews"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "geolocation"], group_name=LAYER)
def bronze_geolocation(context):
    return Output(load_mysql_table(context, "geolocation"))

@asset(io_manager_key="spark_io_manager", compute_kind=COMPUTE_KIND, key_prefix=[LAYER, "productcategory"], group_name=LAYER)
def bronze_product_category(context):
    return Output(load_mysql_table(context, "product_category_name_translation"))

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/assets/silver.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import asset, AssetIn, Output
from pyspark.sql import functions as F, DataFrame

COMPUTE_KIND = "SparkSQL"
LAYER = "silver"


def log_shape(context, df: DataFrame, name: str):
    context.log.info(f"{name} rows={df.count()} cols={len(df.columns)}")


@asset(
    ins={"bronze_customer": AssetIn(key_prefix=["bronze", "customer"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "customer"],
    group_name=LAYER,
)
def silver_customer(context, bronze_customer: DataFrame):
    df = bronze_customer.dropDuplicates(["customer_id"]).filter(F.col("customer_id").isNotNull())
    log_shape(context, df, "silver_customer")
    return Output(df)


@asset(
    ins={"bronze_seller": AssetIn(key_prefix=["bronze", "seller"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "seller"],
    group_name=LAYER,
)
def silver_seller(context, bronze_seller: DataFrame):
    df = bronze_seller.dropDuplicates(["seller_id"]).na.fill("")
    log_shape(context, df, "silver_seller")
    return Output(df)


@asset(
    ins={"bronze_product": AssetIn(key_prefix=["bronze", "product"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "product"],
    group_name=LAYER,
)
def silver_product(context, bronze_product: DataFrame):
    cols_to_cast = ["product_description_lenght", "product_length_cm", "product_height_cm", "product_width_cm"]
    df = bronze_product.dropDuplicates().na.drop()
    
    # Rename columns with typo from Olist dataset to standardized names
    if "product_description_lenght" in df.columns:
        df = df.withColumnRenamed("product_description_lenght", "product_description_length")
    if "product_name_lenght" in df.columns:
        df = df.withColumnRenamed("product_name_lenght", "product_name_length")
    
    # Cast to int after renaming
    cols_to_cast_renamed = ["product_description_length", "product_length_cm", "product_height_cm", "product_width_cm"]
    for col in cols_to_cast_renamed:
        if col in df.columns:
            df = df.withColumn(col, F.col(col).cast("int"))
    log_shape(context, df, "silver_product")
    return Output(df)


@asset(
    ins={"bronze_order": AssetIn(key_prefix=["bronze", "order"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "order"],
    group_name=LAYER,
)
def silver_order(context, bronze_order: DataFrame):
    df = bronze_order.dropDuplicates(["order_id"]).na.drop()
    if "order_purchase_timestamp" in df.columns:
        df = df.withColumn("order_purchase_timestamp", F.to_timestamp("order_purchase_timestamp"))
    log_shape(context, df, "silver_order")
    return Output(df)


@asset(
    ins={"bronze_order_item": AssetIn(key_prefix=["bronze", "orderitem"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "orderitem"],
    group_name=LAYER,
)
def silver_order_item(context, bronze_order_item: DataFrame):
    df = bronze_order_item.dropDuplicates().na.drop()
    if "price" in df.columns:
        df = df.withColumn("price", F.round(F.col("price"), 2).cast("double"))
    if "freight_value" in df.columns:
        df = df.withColumn("freight_value", F.round(F.col("freight_value"), 2).cast("double"))
    log_shape(context, df, "silver_order_item")
    return Output(df)


@asset(
    ins={"bronze_payment": AssetIn(key_prefix=["bronze", "payment"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "payment"],
    group_name=LAYER,
)
def silver_payment(context, bronze_payment: DataFrame):
    df = bronze_payment.dropDuplicates().na.drop()
    if "payment_value" in df.columns:
        df = df.withColumn("payment_value", F.round(F.col("payment_value"), 2).cast("double"))
    if "payment_installments" in df.columns:
        df = df.withColumn("payment_installments", F.col("payment_installments").cast("int"))
    log_shape(context, df, "silver_payment")
    return Output(df)


@asset(
    ins={"bronze_order_review": AssetIn(key_prefix=["bronze", "orderreview"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "orderreview"],
    group_name=LAYER,
)
def silver_order_review(context, bronze_order_review: DataFrame):
    df = bronze_order_review.dropDuplicates().na.drop()
    if "review_comment_title" in df.columns:
        df = df.drop("review_comment_title")
    log_shape(context, df, "silver_order_review")
    return Output(df)


@asset(
    ins={"bronze_product_category": AssetIn(key_prefix=["bronze", "productcategory"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "productcategory"],
    group_name=LAYER,
)
def silver_product_category(context, bronze_product_category: DataFrame):
    df = bronze_product_category.dropDuplicates().na.drop()
    log_shape(context, df, "silver_product_category")
    return Output(df)


@asset(
    ins={"bronze_order": AssetIn(key_prefix=["bronze", "order"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "date"],
    group_name=LAYER,
)
def silver_date(context, bronze_order: DataFrame):
    date_df = (
        bronze_order
        .select(F.col("order_purchase_timestamp").alias("ts"))
        .withColumn("full_date", F.to_date("ts"))
        .withColumn("year", F.year("ts"))
        .withColumn("month", F.month("ts"))
        .withColumn("day", F.dayofmonth("ts"))
        .withColumn("weekday", F.date_format("ts", "EEEE"))
        .dropDuplicates(["full_date"])
    )
    log_shape(context, date_df, "silver_date")
    return Output(date_df)


@asset(
    ins={"bronze_geolocation": AssetIn(key_prefix=["bronze", "geolocation"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "geolocation"],
    group_name=LAYER,
)
def silver_geolocation(context, bronze_geolocation: DataFrame):
    df = bronze_geolocation.dropDuplicates().na.drop()
    if {"geolocation_lat", "geolocation_lng"}.issubset(set(df.columns)):
        df = df.filter(
            (F.col("geolocation_lat") <= 5.27438888)
            & (F.col("geolocation_lng") >= -73.98283055)
            & (F.col("geolocation_lat") >= -33.75116944)
            & (F.col("geolocation_lng") <= -34.79314722)
        )
    log_shape(context, df, "silver_geolocation")
    return Output(df)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/assets/gold.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import asset, AssetIn, Output
from pyspark.sql import functions as F, DataFrame
from pyspark.sql.types import *

COMPUTE_KIND = "SparkSQL"
LAYER = "gold"

def log_shape(context, df: DataFrame, name: str):
    context.log.info(f"{name} rows={df.count()} cols={len(df.columns)}")

# --------------------
# IMPROVED DIMENSIONS
# --------------------

@asset(
    ins={"silver_customer": AssetIn(key_prefix=["silver", "customer"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimcustomer"],
    group_name=LAYER,
)
def dim_customer(context, silver_customer: DataFrame):
    """Customer dimension with enhanced attributes"""
    df = (silver_customer
          .dropDuplicates(["customer_id"])
          .withColumn("customer_unique_id", F.col("customer_unique_id"))  # Keep for tracking
          .withColumn("city_state", F.concat_ws(", ", F.col("customer_city"), F.col("customer_state")))
          .select(
              "customer_id",
              "customer_unique_id", 
              "customer_zip_code_prefix",
              "customer_city",
              "customer_state",
              "city_state"
          )
    )
    log_shape(context, df, "dim_customer")
    return Output(df)

@asset(
    ins={"silver_seller": AssetIn(key_prefix=["silver", "seller"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimseller"],
    group_name=LAYER,
)
def dim_seller(context, silver_seller: DataFrame):
    """Seller dimension with enhanced attributes"""
    df = (silver_seller
          .dropDuplicates(["seller_id"])
          .withColumn("city_state", F.concat_ws(", ", F.col("seller_city"), F.col("seller_state")))
    )
    log_shape(context, df, "dim_seller")
    return Output(df)

@asset(
    ins={"silver_product": AssetIn(key_prefix=["silver", "product"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimproduct"],
    group_name=LAYER,
)
def dim_product(context, silver_product: DataFrame):
    """Product dimension with enhanced attributes"""
    df = silver_product.dropDuplicates(["product_id"])
    log_shape(context, df, "dim_product")
    return Output(df)

@asset(
    ins={"silver_product_category": AssetIn(key_prefix=["silver", "productcategory"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimproductcategory"],
    group_name=LAYER,
)
def dim_product_category(context, silver_product_category: DataFrame):
    """Product category dimension with English translation"""
    df = (silver_product_category
          .dropDuplicates(["product_category_name"])
          .withColumn("product_category_name_english", F.col("product_category_name_english"))
    )
    log_shape(context, df, "dim_product_category")
    return Output(df)

@asset(
    ins={"silver_date": AssetIn(key_prefix=["silver", "date"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimdate"],
    group_name=LAYER,
)
def dim_date(context, silver_date: DataFrame):
    """Date dimension with enhanced time attributes"""
    df = (silver_date
          .dropDuplicates(["full_date"])
          .withColumn("year_month", F.date_format(F.col("full_date"), "yyyy-MM"))
          .withColumn("quarter", F.quarter(F.col("full_date")))
          .withColumn("week_of_year", F.weekofyear(F.col("full_date")))
    )
    log_shape(context, df, "dim_date")
    return Output(df)

@asset(
    ins={"silver_geolocation": AssetIn(key_prefix=["silver", "geolocation"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "dimgeolocation"],
    group_name=LAYER,
)
def dim_geolocation(context, silver_geolocation: DataFrame):
    """Geolocation dimension with enhanced attributes"""
    df = (silver_geolocation
          .dropDuplicates(["geolocation_zip_code_prefix"])
          .withColumn("city_state", F.concat_ws(", ", F.col("geolocation_city"), F.col("geolocation_state")))
    )
    log_shape(context, df, "dim_geolocation")
    return Output(df)

# --------------------
# IMPROVED FACTS WITH PROPER GRAIN
# --------------------

@asset(
    ins={
        "silver_order": AssetIn(key_prefix=["silver", "order"]),
        "silver_order_item": AssetIn(key_prefix=["silver", "orderitem"]),
    },
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "factorderitem"],
    group_name=LAYER,
)
def fact_order_item(context, silver_order, silver_order_item):
    """Fact table with grain: 1 row per order_id + order_item_id"""
    df = (silver_order_item
          .join(silver_order.select("order_id", "customer_id", "order_purchase_timestamp", "order_status"), 
                "order_id", "left")
          .select(
              "order_id",
              "order_item_id", 
              "product_id",
              "seller_id",
              "customer_id",
              F.col("order_purchase_timestamp").cast("date").alias("full_date"),
              "price",
              "freight_value",
              "shipping_limit_date",
              "order_status"
          )
    )
    log_shape(context, df, "fact_order_item")
    return Output(df)

@asset(
    ins={
        "silver_order": AssetIn(key_prefix=["silver", "order"]),
        "silver_payment": AssetIn(key_prefix=["silver", "payment"]),
        "fact_order_item": AssetIn(key_prefix=[LAYER, "factorderitem"]),
    },
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "factorder"],
    group_name=LAYER,
)
def fact_order(context, silver_order, silver_payment, fact_order_item):
    """Fact table with grain: 1 row per order_id"""
    
    # Aggregate from fact_order_item
    order_item_agg = (fact_order_item
                     .groupBy("order_id")
                     .agg(
                         F.count("order_item_id").alias("items_count"),
                         F.sum("price").alias("sum_price"),
                         F.sum("freight_value").alias("sum_freight")
                     )
    )
    
    # Aggregate payments
    payment_agg = (silver_payment
                  .groupBy("order_id")
                  .agg(
                      F.sum("payment_value").alias("payment_total"),
                      F.max("payment_installments").alias("payment_installments_max"),
                      F.first("payment_type").alias("primary_payment_type")
                  )
    )
    
    # Calculate delivery metrics
    df = (silver_order
          .join(order_item_agg, "order_id", "left")
          .join(payment_agg, "order_id", "left")
          .withColumn("delivered_days", 
                     F.datediff("order_delivered_customer_date", "order_purchase_timestamp"))
          .withColumn("delivered_on_time",
                     F.when(F.col("order_delivered_customer_date") <= F.col("order_estimated_delivery_date"), True)
                     .otherwise(False))
          .withColumn("is_canceled",
                     F.when(F.col("order_status") == "canceled", True)
                     .otherwise(False))
          .withColumn("full_date", F.col("order_purchase_timestamp").cast("date"))
          .select(
              "order_id",
              "customer_id",
              "full_date",
              "items_count",
              "sum_price",
              "sum_freight", 
              "payment_total",
              "payment_installments_max",
              "primary_payment_type",
              "delivered_days",
              "delivered_on_time",
              "is_canceled"
          )
    )
    log_shape(context, df, "fact_order")
    return Output(df)

@asset(
    ins={"silver_payment": AssetIn(key_prefix=["silver", "payment"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "factpayment"],
    group_name=LAYER,
)
def fact_payment(context, silver_payment: DataFrame):
    """Fact table with grain: 1 row per order_id + payment_sequential"""
    df = (silver_payment
          .select(
              "order_id",
              "payment_sequential",
              "payment_type",
              "payment_installments",
              "payment_value"
          )
    )
    log_shape(context, df, "fact_payment")
    return Output(df)

@asset(
    ins={"silver_order_review": AssetIn(key_prefix=["silver", "orderreview"])},
    io_manager_key="spark_io_manager",
    compute_kind=COMPUTE_KIND,
    key_prefix=[LAYER, "factreview"],
    group_name=LAYER,
)
def fact_review(context, silver_order_review: DataFrame):
    """Fact table with grain: 1 row per review_id"""
    df = (silver_order_review
          .select(
              "review_id",
              "order_id",
              "review_score",
              "review_creation_date",
              "review_answer_timestamp"
          )
    )
    log_shape(context, df, "fact_review")
    return Output(df)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/job/__init__.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from dagster import AssetSelection, define_asset_job

# Layer constants
BRONZE = "bronze"
SILVER = "silver"
GOLD = "gold"
PLATINUM = "platinum"

# Asset selections for each layer
bronze_selection = AssetSelection.groups(BRONZE)
silver_selection = AssetSelection.groups(SILVER)
gold_selection = AssetSelection.groups(GOLD)
platinum_selection = AssetSelection.groups(PLATINUM)

# Individual layer jobs
reload_data = define_asset_job(
    name="reload_data",
    selection=bronze_selection,  # Default to bronze layer only
)

# Full pipeline job
full_pipeline_job = define_asset_job(
    name="full_pipeline_job",
    selection=AssetSelection.all(),  # Run all layers
)



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: etl_pipeline/etl_pipeline/schedule/__init__.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from ..job import reload_data 
from dagster import ScheduleDefinition

'''

Crontab Syntax
+---------------- minute (0 - 59)
|  +------------- hour (0 - 23)
|  |  +---------- day of month (1 - 31)
|  |  |  +------- month (1 - 12)
|  |  |  |  +---- day of week (0 - 6) (Sunday is 0 or 7)
|  |  |  |  |
*  *  *  *  *  command to be executed

* means all values are acceptable

'''

reload_data_schedule = ScheduleDefinition(
    job=reload_data,
    cron_schedule="0 0 * * *",  # every day at 00:00
)







# Path: etl_pipeline/etl_pipeline/schedule/__init__.py

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: docker_image/spark/conf/spark-defaults.conf
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#####################################
# ğŸš€ Spark Defaults Configuration   #
#####################################
# JARs (Ã©p Spark load Delta + MySQL + S3)
spark.jars=/opt/jars/mysql-connector-j-8.0.33.jar,/opt/jars/delta-core_2.12-2.3.0.jar,/opt/jars/delta-storage-2.3.0.jar,/opt/jars/hadoop-aws-3.3.2.jar,/opt/jars/aws-java-sdk-bundle-1.11.1026.jar

# Delta Lake
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# S3 / MinIO configs
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.connection.establish.timeout=5000
spark.hadoop.fs.s3a.connection.request.timeout=10000
spark.hadoop.fs.s3a.connection.maximum=50
spark.hadoop.fs.s3a.multipart.size=104857600
spark.hadoop.fs.s3a.multipart.threshold=104857600

# Warehouse path
spark.sql.warehouse.dir=s3a://lakehouse/

# Memory / Parallelism
spark.driver.memory=4g
spark.executor.memory=4g
spark.executor.cores=2
spark.default.parallelism=4

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: docker_image/hive-metastore/metastore-site.xml
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://de_mysql:3306/metastore?createDatabaseIfNotExist=true&amp;useSSL=false&amp;allowPublicKeyRetrieval=true</value>  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.cj.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
  </property>
  <property>
    <name>datanucleus.autoCreateSchema</name>
    <value>true</value>
  </property>
  <property>
    <name>datanucleus.fixedDatastore</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.event.db.notification.api.auth</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>s3a://lakehouse/</value>
    <description>Location of default database for the warehouse</description>
  </property>
  <property>
    <name>fs.s3a.access.key</name>
    <value>minio</value>
  </property>
  <property>
    <name>fs.s3a.secret.key</name>
    <value>minio123</value>
  </property>
  <property>
    <name>fs.s3a.endpoint</name>
    <value>http://minio:9000</value>
  </property>
  <property>
    <name>fs.s3a.path.style.access</name>
    <value>true</value>
  </property>
</configuration>


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: trino/etc/catalog/lakehouse.properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

connector.name=delta_lake

# MinIO credentials
hive.s3.aws-access-key=minio
hive.s3.aws-secret-key=minio123
hive.s3.endpoint=http://minio:9000
hive.s3.path-style-access=true
hive.s3.region=us-east-1
hive.s3.ssl.enabled=false

# Hive Metastore connection
hive.metastore.uri=thrift://hive-metastore:9083

# Delta Lake configuration
delta.register-table-procedure.enabled=true

# Additional S3 configuration for better MinIO compatibility  
# hive.s3.socket-timeout=50000
# hive.s3.connection-timeout=50000
# hive.s3.max-error-retries=10

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: trino/etc/catalog/hive.properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

connector.name=hive

# MinIO credentials
hive.s3.aws-access-key=minio
hive.s3.aws-secret-key=minio123
hive.s3.endpoint=http://minio:9000
hive.s3.path-style-access=true
hive.s3.region=us-east-1

# Hive Metastore connection
hive.metastore.uri=thrift://hive-metastore:9083

# S3 configuration
hive.s3.ssl.enabled=false


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: load_dataset_into_mysql/00_init_dagster.sql
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- ============================================
-- Initialize Dagster Database and User
-- ============================================

-- Create Dagster user if not exists
CREATE USER IF NOT EXISTS 'dagster'@'%' IDENTIFIED BY 'dagster123';

-- Create Dagster database if not exists
CREATE DATABASE IF NOT EXISTS dagster;

-- Grant all privileges to dagster user
GRANT ALL PRIVILEGES ON dagster.* TO 'dagster'@'%';

-- Also create Hive user and database for Hive Metastore
CREATE USER IF NOT EXISTS 'hive'@'%' IDENTIFIED BY 'hive';
CREATE DATABASE IF NOT EXISTS metastore;
GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'%';

-- Flush privileges
FLUSH PRIVILEGES;



â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“„ FILE: load_dataset_into_mysql/01_olist.sql
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SET FOREIGN_KEY_CHECKS = 0;

DROP TABLE IF EXISTS order_items;
DROP TABLE IF EXISTS payments;
DROP TABLE IF EXISTS products;
DROP TABLE IF EXISTS orders;
DROP TABLE IF EXISTS customers;
DROP TABLE IF EXISTS sellers;
DROP TABLE IF EXISTS geolocation;
DROP TABLE IF EXISTS order_reviews;
DROP TABLE IF EXISTS product_category_name_translation;

SET FOREIGN_KEY_CHECKS = 1;
-- Táº¡o cÆ¡ sá»Ÿ dá»¯ liá»‡u náº¿u chÆ°a tá»“n táº¡i
CREATE DATABASE IF NOT EXISTS brazillian_ecommerce;
USE brazillian_ecommerce;

DROP TABLE IF EXISTS customers;
DROP TABLE IF EXISTS geolocation;
DROP TABLE IF EXISTS order_items;
DROP TABLE IF EXISTS order_payments;
DROP TABLE IF EXISTS order_reviews;
DROP TABLE IF EXISTS orders;
DROP TABLE IF EXISTS products;
DROP TABLE IF EXISTS sellers;
DROP TABLE IF EXISTS product_category_name_translation;

CREATE TABLE customers (
  customer_id VARCHAR(50) NOT NULL,
  customer_unique_id VARCHAR(50) NOT NULL,
  customer_zip_code_prefix INT,
  customer_city VARCHAR(50),
  customer_state VARCHAR(50),
  PRIMARY KEY (customer_id)
);

CREATE TABLE geolocation (
  geolocation_zip_code_prefix INT NOT NULL,
  geolocation_lat FLOAT,
  geolocation_lng FLOAT,
  geolocation_city VARCHAR(50),
  geolocation_state VARCHAR(50),
  PRIMARY KEY (geolocation_zip_code_prefix)
);

CREATE TABLE order_items (
  order_id VARCHAR(50) NOT NULL,
  order_item_id INT NOT NULL,
  product_id VARCHAR(50),
  seller_id VARCHAR(50),
  shipping_limit_date DATETIME,
  price FLOAT,
  freight_value FLOAT,
  PRIMARY KEY (order_id, order_item_id)
);

CREATE TABLE order_payments (
  order_id VARCHAR(50) NOT NULL,
  payment_sequential INT NOT NULL,
  payment_type VARCHAR(50),
  payment_installments INT,
  payment_value FLOAT,
  PRIMARY KEY (order_id, payment_sequential)
);

CREATE TABLE order_reviews (
  review_id VARCHAR(50) NOT NULL,
  order_id VARCHAR(50),
  review_score INT,
  review_comment_title TEXT,
  review_comment_message TEXT,
  review_creation_date DATETIME,
  review_answer_timestamp DATETIME,
  PRIMARY KEY (review_id)
);

CREATE TABLE orders (
  order_id VARCHAR(50) NOT NULL,
  customer_id VARCHAR(50),
  order_status VARCHAR(50),
  order_purchase_timestamp DATETIME,
  order_approved_at DATETIME,
  order_delivered_carrier_date DATETIME,
  order_delivered_customer_date DATETIME,
  order_estimated_delivery_date DATETIME,
  PRIMARY KEY (order_id)
);

CREATE TABLE products (
  product_id VARCHAR(50) NOT NULL,
  product_category_name VARCHAR(100),
  product_name_lenght INT,
  product_description_lenght INT,
  product_photos_qty INT,
  product_weight_g INT,
  product_length_cm INT,
  product_height_cm INT,
  product_width_cm INT,
  PRIMARY KEY (product_id)
);

CREATE TABLE sellers (
  seller_id VARCHAR(50) NOT NULL,
  seller_zip_code_prefix INT,
  seller_city VARCHAR(50),
  seller_state VARCHAR(50),
  PRIMARY KEY (seller_id)
);

CREATE TABLE product_category_name_translation (
  product_category_name VARCHAR(100) NOT NULL,
  product_category_name_english VARCHAR(100),
  PRIMARY KEY (product_category_name)
);

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# END OF DUMP
