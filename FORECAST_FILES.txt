â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FORECAST PIPELINE - Táº¤T Cáº¢ CÃC FILE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Generated: $(date)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/ml/__init__.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
ML module for Demand Forecasting
"""
from .feature_build import build_features
from .train_models import train_lightgbm, train_prophet_aggregate
from .batch_predict import batch_predict
from .backtest import rolling_backtest

__all__ = [
    "build_features",
    "train_lightgbm",
    "train_prophet_aggregate",
    "batch_predict",
    "rolling_backtest",
]



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/ml/feature_build.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Feature Engineering for Demand Forecasting
Builds time-series features from gold.factorderitem + gold.factorder
"""
import os
from pyspark.sql import SparkSession, Window
from pyspark.sql import functions as F


def build_features(
    spark_master: str = None,
    delta_root: str = "s3a://lakehouse",
    output_table: str = "silver.forecast_features",
    start_date: str = None,
    end_date: str = None,
):
    """
    Build forecast features from gold.factorderitem + gold.factorder
    
    Args:
        spark_master: Spark master URL (default: from env SPARK_MASTER_URL)
        delta_root: Root path for Delta tables (default: s3a://lakehouse)
        output_table: Output table name (default: silver.forecast_features)
        start_date: Filter start date (format: YYYY-MM-DD)
        end_date: Filter end date (format: YYYY-MM-DD)
    
    Output schema:
        - date: DATE
        - product_id: STRING
        - region_id: STRING (customer_state for Olist)
        - y: DOUBLE (target: revenue)
        - quantity: INT (order items count)
        - lag_1, lag_7, lag_28: DOUBLE (lagged features)
        - roll7, roll28: DOUBLE (rolling averages)
        - dow: INT (day of week)
        - month: INT
        - is_weekend: INT
        - price: DOUBLE
        - payment_type: STRING
    """
    
    # Initialize Spark (using pre-downloaded JARs like SparkIOManager)
    spark_master = spark_master or os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077")
    jars_dir = os.getenv("JARS_DIR", "/opt/jars")
    
    # Use same JARs as SparkIOManager (already downloaded)
    spark_jars = ",".join([
        f"file://{jars_dir}/delta-core_2.12-2.3.0.jar",
        f"file://{jars_dir}/delta-storage-2.3.0.jar",
        f"file://{jars_dir}/hadoop-aws-3.3.2.jar",
        f"file://{jars_dir}/aws-java-sdk-bundle-1.11.1026.jar",
    ])
    
    builder = (
        SparkSession.builder
        .appName("build_forecast_features")
        .master(spark_master)
        .config("spark.jars", spark_jars)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000"))
        .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "minio"))
        .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "minio123"))
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
    )
    
    spark = builder.getOrCreate()
    
    try:
        # Read gold tables (with underscores in table names)
        fact_order_item_path = f"{delta_root}/gold/fact_order_item"
        fact_order_path = f"{delta_root}/gold/fact_order"
        dim_customer_path = f"{delta_root}/gold/dim_customer"
        
        fact_order_item = spark.read.format("delta").load(fact_order_item_path)
        fact_order = spark.read.format("delta").load(fact_order_path)
        dim_customer = spark.read.format("delta").load(dim_customer_path)
        
        # Join to get complete picture (order_item level with payment & customer info)
        complete_data = (
            fact_order_item
            .join(fact_order.select("order_id", "primary_payment_type"), "order_id", "left")
            .join(dim_customer.select("customer_id", "customer_state"), "customer_id", "left")
        )
        
        # Aggregate to daily level (date Ã— product Ã— customer_state)
        # Target: revenue (price + freight)
        daily_sales = (
            complete_data
            .groupBy(
                F.col("full_date").alias("date"),
                "product_id",
                "customer_state"
            )
            .agg(
                F.sum(F.col("price") + F.col("freight_value")).alias("y"),  # target: revenue
                F.count("order_item_id").alias("quantity"),  # items sold
                F.avg("price").alias("avg_price"),
                F.first("primary_payment_type").alias("primary_payment_type")
            )
        )
        
        # Filter by date range if provided
        if start_date:
            daily_sales = daily_sales.filter(F.col("date") >= F.lit(start_date))
        if end_date:
            daily_sales = daily_sales.filter(F.col("date") <= F.lit(end_date))
        
        # Window for time series features per series
        w = Window.partitionBy("product_id", "customer_state").orderBy(
            F.col("date").cast("timestamp")
        )
        
        # Create features
        df = (
            daily_sales
            .withColumn("y", F.col("y").cast("double"))
            .withColumn("lag_1", F.lag("y", 1).over(w))
            .withColumn("lag_7", F.lag("y", 7).over(w))
            .withColumn("lag_28", F.lag("y", 28).over(w))
            .withColumn("roll7", F.avg("y").over(w.rowsBetween(-6, 0)))
            .withColumn("roll28", F.avg("y").over(w.rowsBetween(-27, 0)))
            .withColumn("dow", F.dayofweek("date"))
            .withColumn("month", F.month("date"))
            .withColumn("is_weekend", F.col("dow").isin([1, 7]).cast("int"))
            .withColumnRenamed("customer_state", "region_id")
            .withColumnRenamed("avg_price", "price")
            .withColumnRenamed("primary_payment_type", "payment_type")
        )
        
        # Select final columns
        output_cols = [
            "date", "product_id", "region_id", "y", "quantity",
            "lag_1", "lag_7", "lag_28", "roll7", "roll28",
            "dow", "month", "is_weekend", "price", "payment_type"
        ]
        
        df_final = df.select(*output_cols)
        
        # Write to Delta
        output_path = f"{delta_root}/{output_table.replace('.', '/')}"
        print(f"Writing features to {output_path}...")
        (
            df_final
            .write
            .format("delta")
            .mode("overwrite")
            .option("overwriteSchema", "true")
            .save(output_path)
        )
        
        row_count = df_final.count()
        print(f"âœ… Features built successfully: {row_count} rows written to {output_table}")
        
        return output_table
        
    finally:
        spark.stop()


if __name__ == "__main__":
    # For standalone testing
    build_features()



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/ml/train_models.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Model Training for Demand Forecasting
Supports LightGBM (global) and Prophet (baseline aggregate)
"""
import os
import warnings
import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error

warnings.filterwarnings("ignore")

# Try import ML libraries
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  LightGBM not available. Install: pip install lightgbm")

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("âš ï¸  Prophet not available. Install: pip install prophet")


def smape(y_true, y_pred):
    """Symmetric Mean Absolute Percentage Error"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denom = np.where(denom == 0, 1, denom)  # avoid division by zero
    diff = np.abs(y_true - y_pred) / denom
    return np.mean(diff) * 100


def load_features_from_delta(
    table_name: str = "silver.forecast_features",
    delta_root: str = "s3a://lakehouse",
    sample_frac: float = 1.0,
):
    """
    Load features from Delta table using PySpark and convert to Pandas
    
    Args:
        table_name: Delta table name
        delta_root: Root path for Delta tables
        sample_frac: Sample fraction (0-1) for faster dev iteration
    
    Returns:
        pandas DataFrame
    """
    from pyspark.sql import SparkSession
    
    jars_dir = os.getenv("JARS_DIR", "/opt/jars")
    spark_jars = ",".join([
        f"file://{jars_dir}/delta-core_2.12-2.3.0.jar",
        f"file://{jars_dir}/delta-storage-2.3.0.jar",
        f"file://{jars_dir}/hadoop-aws-3.3.2.jar",
        f"file://{jars_dir}/aws-java-sdk-bundle-1.11.1026.jar",
    ])
    
    builder = (
        SparkSession.builder
        .appName("load_features")
        .master(os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077"))
        .config("spark.jars", spark_jars)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000"))
        .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "minio"))
        .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "minio123"))
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
    )
    
    spark = builder.getOrCreate()
    
    try:
        table_path = f"{delta_root}/{table_name.replace('.', '/')}"
        df_spark = spark.read.format("delta").load(table_path)
        
        if sample_frac < 1.0:
            df_spark = df_spark.sample(fraction=sample_frac, seed=42)
        
        df = df_spark.toPandas()
        print(f"âœ… Loaded {len(df):,} rows from {table_name}")
        return df
    finally:
        spark.stop()


def train_lightgbm(
    features_df: pd.DataFrame = None,
    features_table: str = "silver.forecast_features",
    mlflow_experiment: str = "demand_forecast",
    n_splits: int = 5,
    target_transform: str = "log1p",  # "log1p" or "none"
):
    """
    Train LightGBM global model for demand forecasting
    
    Args:
        features_df: Pre-loaded features DataFrame (if None, load from table)
        features_table: Delta table name to load features from
        mlflow_experiment: MLflow experiment name
        n_splits: Number of time series CV splits
        target_transform: "log1p" or "none"
    
    Returns:
        run_id: MLflow run ID
    """
    if not LIGHTGBM_AVAILABLE:
        raise ImportError("LightGBM not installed. Run: pip install lightgbm")
    
    # Setup MLflow
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "mysql+pymysql://mlflow:mlflow@de_mysql:3306/mlflowdb"))
    mlflow.set_experiment(mlflow_experiment)
    
    # Load features
    if features_df is None:
        features_df = load_features_from_delta(features_table)
    
    # Prepare data
    df = features_df.copy()
    
    # Keep necessary columns
    feature_cols = [
        "lag_1", "lag_7", "lag_28", "roll7", "roll28",
        "dow", "month", "is_weekend", "price"
    ]
    
    # Drop rows with missing lags
    df = df.dropna(subset=["y"] + feature_cols)
    df = df.sort_values(["product_id", "region_id", "date"])
    
    # Target transformation
    if target_transform == "log1p":
        df["y_transformed"] = np.log1p(df["y"])
    else:
        df["y_transformed"] = df["y"]
    
    X = df[feature_cols]
    y = df["y_transformed"]
    
    # Time series split
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    # Model parameters
    params = {
        "objective": "regression",
        "metric": "rmse",
        "learning_rate": 0.05,
        "num_leaves": 128,
        "max_depth": 10,
        "min_data_in_leaf": 100,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "n_estimators": 500,
        "verbosity": -1,
    }
    
    # Training with CV
    with mlflow.start_run(run_name=f"lgbm_global_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}") as run:
        mlflow.log_params(params)
        mlflow.log_param("target_transform", target_transform)
        mlflow.log_param("n_features", len(feature_cols))
        mlflow.log_param("n_samples", len(df))
        
        rmses, smapes = [], []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            print(f"\nğŸ“Š Fold {fold + 1}/{n_splits}")
            
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # Train
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(50, verbose=False)]
            )
            
            # Predict
            y_pred_transformed = model.predict(X_val)
            
            # Inverse transform
            if target_transform == "log1p":
                y_pred = np.expm1(y_pred_transformed)
                y_val_orig = np.expm1(y_val)
            else:
                y_pred = y_pred_transformed
                y_val_orig = y_val
            
            # Ensure non-negative
            y_pred = np.maximum(0, y_pred)
            
            # Metrics
            rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred))
            s = smape(y_val_orig, y_pred)
            
            rmses.append(rmse)
            smapes.append(s)
            
            print(f"   RMSE: {rmse:.2f}, sMAPE: {s:.2f}%")
            
            mlflow.log_metric(f"fold_{fold}_rmse", rmse)
            mlflow.log_metric(f"fold_{fold}_smape", s)
        
        # Log average metrics
        avg_rmse = float(np.mean(rmses))
        avg_smape = float(np.mean(smapes))
        
        mlflow.log_metric("cv_rmse_mean", avg_rmse)
        mlflow.log_metric("cv_smape_mean", avg_smape)
        
        print(f"\nâœ… CV Results: RMSE={avg_rmse:.2f}, sMAPE={avg_smape:.2f}%")
        
        # Train final model on all data
        print("\nğŸ”„ Training final model on all data...")
        final_model = lgb.LGBMRegressor(**params)
        final_model.fit(X, y)
        
        # Log model
        mlflow.sklearn.log_model(final_model, artifact_path="model")
        
        # Log feature importance
        importance = pd.DataFrame({
            "feature": feature_cols,
            "importance": final_model.feature_importances_
        }).sort_values("importance", ascending=False)
        
        mlflow.log_text(importance.to_string(), "feature_importance.txt")
        
        run_id = run.info.run_id
        print(f"âœ… Model logged to MLflow: run_id={run_id}")
        
        return run_id


def train_prophet_aggregate(
    features_df: pd.DataFrame = None,
    features_table: str = "silver.forecast_features",
    horizon_days: int = 28,
    mlflow_experiment: str = "demand_forecast",
):
    """
    Train Prophet model on aggregated data (baseline)
    
    Args:
        features_df: Pre-loaded features DataFrame
        features_table: Delta table name
        horizon_days: Forecast horizon
        mlflow_experiment: MLflow experiment name
    
    Returns:
        run_id: MLflow run ID
    """
    if not PROPHET_AVAILABLE:
        raise ImportError("Prophet not installed. Run: pip install prophet")
    
    # Setup MLflow
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "mysql+pymysql://mlflow:mlflow@de_mysql:3306/mlflowdb"))
    mlflow.set_experiment(mlflow_experiment)
    
    # Load features
    if features_df is None:
        features_df = load_features_from_delta(features_table)
    
    # Aggregate to daily total
    agg = (
        features_df
        .groupby("date", as_index=False)["y"]
        .sum()
        .rename(columns={"date": "ds", "y": "y"})
    )
    
    with mlflow.start_run(run_name=f"prophet_baseline_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}") as run:
        # Train Prophet
        model = Prophet(
            weekly_seasonality=True,
            yearly_seasonality=True,
            daily_seasonality=False,
        )
        model.fit(agg)
        
        # Make forecast
        future = model.make_future_dataframe(periods=horizon_days)
        forecast = model.predict(future)
        
        # Log parameters
        mlflow.log_param("model_type", "prophet_aggregate")
        mlflow.log_param("horizon_days", horizon_days)
        mlflow.log_param("n_train_days", len(agg))
        
        # Log model
        mlflow.sklearn.log_model(model, artifact_path="model")
        
        run_id = run.info.run_id
        print(f"âœ… Prophet model logged: run_id={run_id}")
        
        return run_id


if __name__ == "__main__":
    # For standalone testing
    print("ğŸš€ Training LightGBM model...")
    run_id = train_lightgbm()
    print(f"\nâœ… Completed! Run ID: {run_id}")



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/ml/batch_predict.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Batch Prediction for Demand Forecasting
Loads trained model from MLflow and generates forecasts
Uses recursive roll-forward for multi-horizon predictions
"""
import os
import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


def recursive_forecast_single_series(model, feat_row, feature_cols, horizon_days=28, target_transform="log1p"):
    """
    Generate multi-horizon forecast for a single series using recursive roll-forward
    
    Args:
        model: Trained model
        feat_row: pandas Series with features for last known date
        feature_cols: List of feature column names
        horizon_days: Number of days to forecast
        target_transform: "log1p" or "none"
    
    Returns:
        List of (horizon, yhat, yhat_lo, yhat_hi) tuples
    """
    results = []
    
    # Initialize current features
    cur_features = feat_row.copy()
    
    # Store last values for rolling updates
    last_7_values = []
    last_28_values = []
    
    # Initialize with existing lags if available
    if pd.notna(cur_features.get('lag_1')):
        last_7_values.append(cur_features['lag_1'])
    if pd.notna(cur_features.get('lag_7')) and len(last_7_values) < 7:
        for _ in range(min(6, 7 - len(last_7_values))):
            last_7_values.insert(0, cur_features.get('lag_7', cur_features.get('y', 0)))
    
    for h in range(1, horizon_days + 1):
        # 1. Extract features for prediction
        X = cur_features[feature_cols].values.reshape(1, -1)
        
        # 2. Predict
        y_pred_transformed = model.predict(X)[0]
        
        # 3. Inverse transform
        if target_transform == "log1p":
            y_pred = np.expm1(y_pred_transformed)
        else:
            y_pred = y_pred_transformed
        
        # Ensure non-negative
        y_pred = max(0, y_pred)
        
        # 4. Simple prediction intervals (Â±15%)
        y_pred_lo = max(0, y_pred * 0.85)
        y_pred_hi = y_pred * 1.15
        
        results.append((h, y_pred, y_pred_lo, y_pred_hi))
        
        # 5. Update features for next horizon
        # Update last_7 and last_28 lists
        last_7_values.append(y_pred)
        if len(last_7_values) > 7:
            last_7_values.pop(0)
        
        last_28_values.append(y_pred)
        if len(last_28_values) > 28:
            last_28_values.pop(0)
        
        # Update lag features
        cur_features['lag_1'] = y_pred
        if len(last_7_values) >= 7:
            cur_features['lag_7'] = last_7_values[0]
        if len(last_28_values) >= 28:
            cur_features['lag_28'] = last_28_values[0]
        
        # Update rolling averages
        cur_features['roll7'] = np.mean(last_7_values)
        if len(last_28_values) > 0:
            cur_features['roll28'] = np.mean(last_28_values)
        
        # Update calendar features (advance by 1 day)
        # dow: 1=Sunday, 7=Saturday
        cur_features['dow'] = (cur_features['dow'] % 7) + 1
        cur_features['is_weekend'] = 1 if cur_features['dow'] in [1, 7] else 0
        # month: stays same or increments (simplified, doesn't handle month boundaries)
        # For production, use proper date arithmetic
    
    return results


def batch_predict(
    run_id: str,
    features_table: str = "silver.forecast_features",
    output_table: str = "platinum.demand_forecast",
    delta_root: str = "s3a://lakehouse",
    horizon_days: int = 28,
    target_transform: str = "log1p",
):
    """
    Generate batch predictions and write to platinum layer
    Uses recursive roll-forward for improved multi-horizon forecasts
    
    Args:
        run_id: MLflow run ID of the trained model
        features_table: Input features table
        output_table: Output forecast table (platinum.demand_forecast)
        delta_root: Root path for Delta tables
        horizon_days: Number of days to forecast
        target_transform: "log1p" or "none" (must match training)
    
    Output schema:
        - forecast_date: DATE (date being forecasted)
        - product_id: STRING
        - region_id: STRING
        - horizon: INT (1..28)
        - yhat: DOUBLE (point forecast)
        - yhat_lo: DOUBLE (lower bound)
        - yhat_hi: DOUBLE (upper bound)
        - model_name: STRING
        - model_version: STRING
        - run_id: STRING
        - generated_at: TIMESTAMP
    """
    
    # Setup MLflow
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "mysql+pymysql://mlflow:mlflow@de_mysql:3306/mlflowdb"))
    
    # Load model
    print(f"ğŸ“¦ Loading model from run_id: {run_id}")
    model_uri = f"runs:/{run_id}/model"
    model = mlflow.sklearn.load_model(model_uri)
    
    # Initialize Spark
    jars_dir = os.getenv("JARS_DIR", "/opt/jars")
    spark_jars = ",".join([
        f"file://{jars_dir}/delta-core_2.12-2.3.0.jar",
        f"file://{jars_dir}/delta-storage-2.3.0.jar",
        f"file://{jars_dir}/hadoop-aws-3.3.2.jar",
        f"file://{jars_dir}/aws-java-sdk-bundle-1.11.1026.jar",
    ])
    
    builder = (
        SparkSession.builder
        .appName("batch_predict")
        .master(os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077"))
        .config("spark.jars", spark_jars)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000"))
        .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "minio"))
        .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "minio123"))
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
    )
    
    spark = builder.getOrCreate()
    
    try:
        # Load features
        table_path = f"{delta_root}/{features_table.replace('.', '/')}"
        df_spark = spark.read.format("delta").load(table_path)
        
        # Convert to pandas for prediction
        # Take most recent features per series
        df_pandas = (
            df_spark
            .groupBy("product_id", "region_id")
            .agg(F.max("date").alias("last_date"))
            .join(df_spark, ["product_id", "region_id"])
            .filter(F.col("date") == F.col("last_date"))
            .toPandas()
        )
        
        print(f"ğŸ“Š Generating forecasts for {len(df_pandas)} series...")
        
        # Feature columns (must match training)
        feature_cols = [
            "lag_1", "lag_7", "lag_28", "roll7", "roll28",
            "dow", "month", "is_weekend", "price"
        ]
        
        # Drop rows with missing features
        df_pandas = df_pandas.dropna(subset=feature_cols)
        
        if len(df_pandas) == 0:
            raise ValueError("No valid features found for prediction!")
        
        print(f"ğŸ“ˆ Using recursive roll-forward for {len(df_pandas)} series Ã— {horizon_days} horizons...")
        
        # Generate forecasts using recursive approach
        all_forecasts = []
        last_date = df_pandas["date"].max()
        
        for idx, row in df_pandas.iterrows():
            # Get recursive forecasts for this series
            series_forecasts = recursive_forecast_single_series(
                model, row, feature_cols, horizon_days, target_transform
            )
            
            # Convert to dataframe rows
            for h, yhat, yhat_lo, yhat_hi in series_forecasts:
                forecast_date = last_date + timedelta(days=h)
                all_forecasts.append({
                    "forecast_date": forecast_date,
                    "product_id": row["product_id"],
                    "region_id": row["region_id"],
                    "horizon": h,
                    "yhat": yhat,
                    "yhat_lo": yhat_lo,
                    "yhat_hi": yhat_hi,
                    "model_name": "lightgbm_global_recursive",
                    "model_version": "v1",
                    "run_id": run_id,
                    "generated_at": pd.Timestamp.utcnow(),
                })
            
            # Progress indicator
            if (idx + 1) % 100 == 0:
                print(f"   Processed {idx + 1}/{len(df_pandas)} series...")
        
        # Create final forecast dataframe
        final_forecast = pd.DataFrame(all_forecasts)
        
        print(f"âœ… Generated {len(final_forecast):,} forecast rows")
        
        # Convert to Spark DataFrame and write to Delta
        sdf = spark.createDataFrame(final_forecast)
        
        output_path = f"{delta_root}/{output_table.replace('.', '/')}"
        
        print(f"ğŸ’¾ Writing forecasts to {output_path}...")
        (
            sdf
            .write
            .format("delta")
            .mode("append")
            .save(output_path)
        )
        
        print(f"âœ… Forecasts written successfully to {output_table}")
        
        return output_table
        
    finally:
        spark.stop()


if __name__ == "__main__":
    # For standalone testing
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python batch_predict.py <run_id>")
        sys.exit(1)
    
    run_id = sys.argv[1]
    batch_predict(run_id)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/ml/backtest.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Backtesting for Demand Forecasting
Performs rolling-origin validation
"""
import os
import numpy as np
import pandas as pd
import mlflow
from datetime import datetime, timedelta
from typing import List, Tuple


def smape(y_true, y_pred):
    """Symmetric Mean Absolute Percentage Error"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denom = np.where(denom == 0, 1, denom)
    diff = np.abs(y_true - y_pred) / denom
    return np.mean(diff) * 100


def mase(y_true, y_pred, y_train):
    """Mean Absolute Scaled Error"""
    mae = np.mean(np.abs(y_true - y_pred))
    mae_naive = np.mean(np.abs(np.diff(y_train)))
    return mae / mae_naive if mae_naive != 0 else np.inf


def rolling_backtest(
    features_df: pd.DataFrame,
    train_func,
    predict_func,
    horizons: List[int] = [7, 14, 28],
    n_splits: int = 4,
    min_train_days: int = 90,
    mlflow_experiment: str = "demand_forecast_backtest",
):
    """
    Perform rolling-origin backtesting
    
    Args:
        features_df: Features DataFrame with columns [date, product_id, region_id, y, ...]
        train_func: Training function(train_df) -> model
        predict_func: Prediction function(model, features, horizon) -> predictions
        horizons: List of forecast horizons to evaluate
        n_splits: Number of time splits
        min_train_days: Minimum training window size
        mlflow_experiment: MLflow experiment name
    
    Returns:
        results_df: DataFrame with backtest results
    """
    
    # Setup MLflow
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "mysql+pymysql://mlflow:mlflow@de_mysql:3306/mlflowdb"))
    mlflow.set_experiment(mlflow_experiment)
    
    # Sort by date
    df = features_df.sort_values("date")
    
    # Determine split dates
    date_range = (df["date"].max() - df["date"].min()).days
    test_window = date_range // (n_splits + 1)
    
    split_dates = []
    for i in range(n_splits):
        cutoff = df["date"].min() + timedelta(days=min_train_days + i * test_window)
        split_dates.append(cutoff)
    
    print(f"ğŸ“Š Backtesting with {n_splits} splits, horizons: {horizons}")
    print(f"   Split dates: {[d.strftime('%Y-%m-%d') for d in split_dates]}")
    
    results = []
    
    with mlflow.start_run(run_name=f"backtest_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"):
        mlflow.log_param("n_splits", n_splits)
        mlflow.log_param("horizons", str(horizons))
        mlflow.log_param("min_train_days", min_train_days)
        
        for fold, cutoff_date in enumerate(split_dates):
            print(f"\nğŸ”„ Fold {fold + 1}/{n_splits} - cutoff: {cutoff_date.strftime('%Y-%m-%d')}")
            
            # Split train/test
            train_df = df[df["date"] <= cutoff_date]
            
            if len(train_df) < min_train_days:
                print(f"   âš ï¸  Skipping - insufficient training data")
                continue
            
            # Train model
            print(f"   ğŸ“š Training on {len(train_df)} samples...")
            model = train_func(train_df)
            
            # Evaluate on each horizon
            for horizon in horizons:
                test_start = cutoff_date + timedelta(days=1)
                test_end = cutoff_date + timedelta(days=horizon)
                
                test_df = df[(df["date"] >= test_start) & (df["date"] <= test_end)]
                
                if len(test_df) == 0:
                    continue
                
                # Predict
                predictions = predict_func(model, train_df.tail(1), horizon)
                
                # Calculate metrics
                y_true = test_df["y"].values
                y_pred = predictions[:len(y_true)]  # match length
                
                if len(y_pred) != len(y_true):
                    print(f"   âš ï¸  Length mismatch: pred={len(y_pred)}, true={len(y_true)}")
                    continue
                
                smape_score = smape(y_true, y_pred)
                rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
                mae = np.mean(np.abs(y_true - y_pred))
                
                result = {
                    "fold": fold,
                    "cutoff_date": cutoff_date,
                    "horizon": horizon,
                    "smape": smape_score,
                    "rmse": rmse,
                    "mae": mae,
                    "n_train": len(train_df),
                    "n_test": len(test_df),
                }
                
                results.append(result)
                
                print(f"   H{horizon:2d}: sMAPE={smape_score:5.2f}%, RMSE={rmse:7.2f}, MAE={mae:7.2f}")
                
                # Log to MLflow
                mlflow.log_metric(f"fold_{fold}_h{horizon}_smape", smape_score)
                mlflow.log_metric(f"fold_{fold}_h{horizon}_rmse", rmse)
        
        # Aggregate results
        results_df = pd.DataFrame(results)
        
        if len(results_df) > 0:
            summary = results_df.groupby("horizon").agg({
                "smape": ["mean", "std"],
                "rmse": ["mean", "std"],
                "mae": ["mean", "std"],
            }).round(2)
            
            print("\nğŸ“ˆ Backtest Summary:")
            print(summary)
            
            # Log summary metrics
            for horizon in horizons:
                horizon_data = results_df[results_df["horizon"] == horizon]
                if len(horizon_data) > 0:
                    mlflow.log_metric(f"h{horizon}_smape_mean", horizon_data["smape"].mean())
                    mlflow.log_metric(f"h{horizon}_rmse_mean", horizon_data["rmse"].mean())
            
            # Save results
            mlflow.log_text(results_df.to_csv(index=False), "backtest_results.csv")
        else:
            print("âš ï¸  No backtest results generated")
        
        return results_df


if __name__ == "__main__":
    print("ğŸ’¡ Backtest module - import and use rolling_backtest() function")



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/forecast/__init__.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Dagster ops, jobs, and schedules for Demand Forecasting
"""
from .forecast_ops import (
    op_build_features,
    op_train_model,
    op_batch_predict,
    op_monitor_forecast,
)

from .forecast_jobs import (
    forecast_job,
    daily_forecast_schedule,
)

__all__ = [
    "op_build_features",
    "op_train_model",
    "op_batch_predict",
    "op_monitor_forecast",
    "forecast_job",
    "daily_forecast_schedule",
]



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/forecast/forecast_ops.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Dagster ops for Demand Forecasting Pipeline
"""
from dagster import op, Out, In, get_dagster_logger


@op(out=Out(str))
def op_build_features():
    """Build forecast features from gold.factorderitem + gold.factorder"""
    logger = get_dagster_logger()
    logger.info("ğŸ”¨ Building forecast features...")
    
    from etl_pipeline.ml.feature_build import build_features
    
    table_name = build_features(
        delta_root="s3a://lakehouse",
        output_table="silver.forecast_features",
    )
    
    logger.info(f"âœ… Features built: {table_name}")
    return table_name


@op(ins={"features_table": In(str)}, out=Out(str))
def op_train_model(features_table: str) -> str:
    """Train LightGBM model"""
    logger = get_dagster_logger()
    logger.info("ğŸ¤– Training forecasting model...")
    
    from etl_pipeline.ml.train_models import train_lightgbm
    
    run_id = train_lightgbm(
        features_table=features_table,
        mlflow_experiment="demand_forecast",
        n_splits=5,
        target_transform="log1p",
    )
    
    logger.info(f"âœ… Model trained: run_id={run_id}")
    return run_id


@op(ins={"run_id": In(str), "features_table": In(str)}, out=Out(str))
def op_batch_predict(run_id: str, features_table: str) -> str:
    """Generate batch predictions"""
    logger = get_dagster_logger()
    logger.info(f"ğŸ”® Generating forecasts with model: {run_id}")
    
    from etl_pipeline.ml.batch_predict import batch_predict
    
    output_table = batch_predict(
        run_id=run_id,
        features_table=features_table,
        output_table="platinum.demand_forecast",
        horizon_days=28,
        target_transform="log1p",
    )
    
    logger.info(f"âœ… Forecasts generated: {output_table}")
    return output_table


@op(ins={"forecast_table": In(str)})
def op_monitor_forecast(forecast_table: str):
    """
    Monitor forecast accuracy by comparing actuals vs forecasts (horizon=1)
    Writes results to platinum.forecast_monitoring
    """
    logger = get_dagster_logger()
    logger.info(f"ğŸ“Š Monitoring skipped (no data for yesterday in 2018 dataset)")
    return
    
    import os
    from pyspark.sql import SparkSession, functions as F
    from datetime import date, timedelta
    
    # Initialize Spark
    jars_dir = os.getenv("JARS_DIR", "/opt/jars")
    spark_jars = ",".join([
        f"file://{jars_dir}/delta-core_2.12-2.3.0.jar",
        f"file://{jars_dir}/delta-storage-2.3.0.jar",
        f"file://{jars_dir}/hadoop-aws-3.3.2.jar",
        f"file://{jars_dir}/aws-java-sdk-bundle-1.11.1026.jar",
    ])
    
    builder = (
        SparkSession.builder
        .appName("monitor_forecast")
        .master(os.getenv("SPARK_MASTER_URL", "spark://spark-master:7077"))
        .config("spark.jars", spark_jars)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .config("spark.hadoop.fs.s3a.endpoint", os.getenv("MINIO_ENDPOINT", "http://minio:9000"))
        .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ACCESS_KEY", "minio"))
        .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_SECRET_KEY", "minio123"))
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        .config("hive.metastore.uris", "thrift://hive-metastore:9083")
        .config("spark.sql.catalogImplementation", "hive")
        .enableHiveSupport()
    )
    
    spark = builder.getOrCreate()
    
    try:
        # Monitor yesterday's forecast vs actual
        yesterday = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')
        logger.info(f"Monitoring date: {yesterday}")
        
        # 1. Get actuals from gold.fact_order_item (with underscores)
        actuals_with_state = spark.sql(f"""
            SELECT 
                CAST(foi.full_date AS DATE) AS date,
                foi.product_id,
                dc.customer_state AS region_id,
                SUM(foi.price + foi.freight_value) AS y_actual,
                COUNT(foi.order_item_id) AS quantity_actual
            FROM gold.fact_order_item foi
            LEFT JOIN gold.dim_customer dc ON foi.customer_id = dc.customer_id
            WHERE CAST(foi.full_date AS DATE) = DATE '{yesterday}'
            GROUP BY 1, 2, 3
        """)
        
        # 2. Get forecasts (horizon=1 for yesterday)
        forecasts = spark.sql(f"""
            SELECT 
                CAST(forecast_date AS DATE) AS date,
                product_id,
                region_id,
                horizon,
                yhat,
                yhat_lo,
                yhat_hi,
                model_name,
                run_id
            FROM platinum.demand_forecast
            WHERE CAST(forecast_date AS DATE) = DATE '{yesterday}'
                AND horizon = 1
        """)
        
        # 3. Join and calculate errors
        monitoring = (
            actuals_with_state.alias("a")
            .join(forecasts.alias("f"), 
                  (F.col("a.date") == F.col("f.date")) & 
                  (F.col("a.product_id") == F.col("f.product_id")) &
                  (F.col("a.region_id") == F.col("f.region_id")),
                  "inner")
            .select(
                F.col("a.date").alias("date"),
                F.col("a.product_id").alias("product_id"),
                F.col("a.region_id").alias("region_id"),
                F.col("f.horizon").alias("horizon"),
                F.col("a.y_actual").alias("y_actual"),
                F.col("f.yhat").alias("yhat"),
                F.col("f.yhat_lo").alias("yhat_lo"),
                F.col("f.yhat_hi").alias("yhat_hi"),
                F.col("f.model_name").alias("model_name"),
                F.col("f.run_id").alias("run_id")
            )
            .withColumn("abs_error", F.abs(F.col("y_actual") - F.col("yhat")))
            .withColumn("pct_error",
                       F.when(F.col("y_actual") == 0, None)
                        .otherwise(F.abs((F.col("y_actual") - F.col("yhat")) / F.col("y_actual")) * 100))
            .withColumn("smape",
                       F.when((F.col("y_actual") + F.col("yhat")) == 0, None)
                        .otherwise(200 * F.abs(F.col("y_actual") - F.col("yhat")) / 
                                 (F.col("y_actual") + F.col("yhat"))))
        )
        
        # 4. Write to monitoring table
        row_count = monitoring.count()
        
        if row_count > 0:
            (
                monitoring
                .write
                .format("delta")
                .mode("append")
                .saveAsTable("lakehouse.platinum.forecast_monitoring")
            )
            
            # Calculate summary metrics
            summary = monitoring.agg(
                F.avg("smape").alias("avg_smape"),
                F.avg("abs_error").alias("avg_abs_error"),
                F.count("*").alias("n_series")
            ).collect()[0]
            
            logger.info(f"âœ… Monitoring completed: {row_count} rows")
            logger.info(f"   Avg sMAPE: {summary['avg_smape']:.2f}%")
            logger.info(f"   Avg Abs Error: {summary['avg_abs_error']:.2f}")
            logger.info(f"   Series monitored: {summary['n_series']}")
        else:
            logger.warning(f"âš ï¸  No matching forecast-actual pairs found for {yesterday}")
    
    finally:
        spark.stop()



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/etl_pipeline/forecast/forecast_jobs.py
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
Dagster jobs and schedules for Demand Forecasting
"""
from dagster import job, schedule, ScheduleDefinition
from .forecast_ops import (
    op_build_features,
    op_train_model,
    op_batch_predict,
    op_monitor_forecast,
)


@job(
    name="forecast_job",
    description="Daily demand forecasting pipeline: features â†’ train â†’ predict â†’ monitor",
)
def forecast_job():
    """
    Complete forecasting pipeline:
    1. Build features from gold.factorderitem + gold.factorder
    2. Train LightGBM model
    3. Generate 28-day forecasts
    4. Monitor accuracy
    """
    features_table = op_build_features()
    run_id = op_train_model(features_table)
    forecast_table = op_batch_predict(run_id, features_table)
    op_monitor_forecast(forecast_table)


@schedule(
    cron_schedule="0 3 * * *",  # Every day at 3:00 AM
    job=forecast_job,
    execution_timezone="Asia/Ho_Chi_Minh",
)
def daily_forecast_schedule(_context):
    """Run forecast pipeline daily at 3:00 AM Vietnam time"""
    return {}



â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: etl_pipeline/requirements.txt (ML/Forecast dependencies)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

numpy==1.24.3
pandas==1.5.3
mlflow==2.8.1
lightgbm==4.1.0
scikit-learn==1.3.2
prophet==1.1.5


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FILE: docker-compose.yaml (etl_pipeline environment - Forecast relevant)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  etl_pipeline:
    build:
      context: ./etl_pipeline
      dockerfile: Dockerfile
    container_name: etl_pipeline
    ports:
      - "4000:4000"
    volumes:
      - ./jars:/opt/jars
      - ./dagster_home:/opt/dagster/dagster_home
    environment:
      - JARS_DIR=/opt/jars
      - SPARK_DIST_CLASSPATH=/opt/jars/*
      - DAGSTER_HOME=/opt/dagster/dagster_home
    env_file:
      - .env
    depends_on:
      - spark-master
      - minio
      - de_mysql


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CONFIGURATION SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“¦ DATA FLOW:

1. Input: gold.fact_order_item + gold.fact_order + gold.dim_customer
   â†“
2. Feature Engineering: silver.forecast_features
   - Lagged features: lag_1, lag_7, lag_28
   - Rolling features: roll7, roll28
   - Calendar features: dow, month, is_weekend
   - Product/region features
   â†“
3. Model Training: LightGBM (via MLflow)
   - Cross-validation: 5 folds
   - Target: log1p transformed revenue
   - Metrics: RMSE, MAE, sMAPE, R2
   â†“
4. Batch Prediction: platinum.demand_forecast
   - Recursive roll-forward for 28 horizons
   - Confidence intervals: yhat_lo, yhat_hi
   â†“
5. Monitoring: platinum.forecast_monitoring (skipped - no 2025 data)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš™ï¸ ENVIRONMENT VARIABLES:

MLFLOW_TRACKING_URI=mysql+pymysql://mlflow:mlflow@de_mysql:3306/mlflowdb
MLFLOW_S3_ENDPOINT_URL=http://minio:9000
AWS_ACCESS_KEY_ID=minio
AWS_SECRET_ACCESS_KEY=minio123
JARS_DIR=/opt/jars
SPARK_MASTER_URL=spark://spark-master:7077
DAGSTER_HOME=/opt/dagster/dagster_home

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ SPARK JARS REQUIRED:

- delta-core_2.12-2.3.0.jar
- delta-storage-2.3.0.jar
- hadoop-aws-3.3.2.jar
- aws-java-sdk-bundle-1.11.1026.jar

Mounted at: /opt/jars in all Spark containers

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ DAGSTER JOB:

Job Name: forecast_job

Ops (4):
1. op_build_features â†’ Build features from gold layer
2. op_train_model â†’ Train LightGBM with MLflow
3. op_batch_predict â†’ Generate 28-day forecasts
4. op_monitor_forecast â†’ Monitor accuracy (currently skipped)

Schedule: daily_forecast_schedule
- Cron: 0 3 * * * (3:00 AM daily)
- Timezone: Asia/Ho_Chi_Minh

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ USAGE:

# Run full pipeline
docker exec etl_pipeline python -c "
from dagster import DagsterInstance
from etl_pipeline import defs
instance = DagsterInstance.get()
job = defs.get_job_def('forecast_job')
result = job.execute_in_process(instance=instance)
print(f'Success: {result.success}')
"

# Or via Dagster UI
http://localhost:3001 â†’ Jobs â†’ forecast_job â†’ Launch Run

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… OUTPUT TABLES:

- silver.forecast_features (s3a://lakehouse/silver/forecast_features)
- platinum.demand_forecast (s3a://lakehouse/platinum/demand_forecast)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
