2025-10-23 16:45:16 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - LOGS_CAPTURED - Started capturing logs in process (pid: 1995).
2025-10-23 16:45:16 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - bronze__product__bronze_product - STEP_START - Started execution of step "bronze__product__bronze_product".
/usr/local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
Warning: Ignoring non-Spark config property: hive.metastore.uris
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                          (0 + 0) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                2025-10-23 16:46:01 +0000 - dagster - INFO - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - bronze__product__bronze_product - bronze_products rows=32951 cols=9
2025-10-23 16:46:01 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - bronze__product__bronze_product - STEP_OUTPUT - Yielded output "result" of type "Any". (Type check passed).
2025-10-23 16:46:01 +0000 - dagster - INFO - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - bronze__product__bronze_product - (SparkIOManager) Saving table: bronze.bronze_product -> s3a://lakehouse/bronze/bronze_product
[Stage 6:>                                                          (0 + 1) / 1]                                                                                [Stage 10:>                                                        (0 + 8) / 50][Stage 10:=========>                                               (8 + 8) / 50][Stage 10:=================>                                      (16 + 8) / 50][Stage 10:=======================>                                (21 + 8) / 50][Stage 10:==================================>                     (31 + 8) / 50][Stage 10:=============================================>          (41 + 8) / 50]                                                                                2025-10-23 16:46:13 +0000 - dagster - INFO - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - bronze__product__bronze_product - Successfully wrote 32951 rows to s3a://lakehouse/bronze/bronze_product
2025-10-23 16:46:13 +0000 - dagster - WARNING - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - bronze__product__bronze_product - Failed to register in Hive Metastore: An error occurred while calling o62.sql.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:785)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:722)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:660)
	at org.apache.spark.sql.delta.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:101)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:165)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:86)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:271)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:57)
	at org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:263)
	at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)

[Stage 20:========================>                               (22 + 9) / 50][Stage 20:=======================================>                (35 + 8) / 50]                                                                                2025-10-23 16:46:16 +0000 - dagster - INFO - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - bronze__product__bronze_product - Used fallback registration method for bronze.bronze_product
2025-10-23 16:46:16 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - bronze__product__bronze_product - ASSET_MATERIALIZATION - Materialized value bronze product bronze_product.
2025-10-23 16:46:16 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - bronze__product__bronze_product - HANDLED_OUTPUT - Handled output "result" using IO manager "spark_io_manager"
2025-10-23 16:46:16 +0000 - dagster - DEBUG - __ASSET_JOB - cec692b6-751b-42d9-9ca6-5fd9decbbca5 - 1995 - bronze__product__bronze_product - STEP_SUCCESS - Finished execution of step "bronze__product__bronze_product" in 1m0s.
